{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from cade.metrics.comparative import moving_lncs2\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "from tabulate import tabulate\n",
    "from config import CURRENT_EXP_DIR, config, get_logger, log_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load language models and groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(lang: str):\n",
    "    model1 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus1.model\"\n",
    "    )\n",
    "    model2 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus2.model\"\n",
    "    )\n",
    "    return model1, model2\n",
    "\n",
    "def get_gt(lang: str, binary=True):\n",
    "    binary_truth = numpy.loadtxt(\n",
    "        \"./data/\"\n",
    "        + lang\n",
    "        + \"/semeval2020_ulscd_\"\n",
    "        + lang[:3]\n",
    "        + \"/truth/\" + (\"binary\" if binary else \"graded\") + \".txt\",\n",
    "        dtype=str,\n",
    "        delimiter=\"\\t\",\n",
    "    )\n",
    "    return binary_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English (Hyper on ACC: thr=0.7329, t=0.6107, NN=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word                Truth    Prediction\n----------------  -------  ------------\nattack_nn               1             0\nbag_nn                  0             0\nball_nn                 0             0\nbit_nn                  1             1\nchairman_nn             0             0\ncircle_vb               1             0\ncontemplation_nn        0             0\ndonkey_nn               0             0\nedge_nn                 1             0\nface_nn                 0             0\nfiction_nn              0             0\ngas_nn                  0             0\ngraft_nn                1             1\nhead_nn                 1             0\nland_nn                 1             0\nlane_nn                 0             0\nlass_nn                 1             1\nmultitude_nn            0             1\nounce_nn                0             0\npart_nn                 0             0\npin_vb                  0             0\nplane_nn                1             1\nplayer_nn               1             1\nprop_nn                 1             1\nquilt_nn                0             0\nrag_nn                  1             0\nrecord_nn               1             1\nrelationship_nn         0             0\nrisk_nn                 0             0\nsavage_nn               0             1\nstab_nn                 1             1\nstroke_vb               0             0\nthump_nn                1             0\ntip_vb                  1             0\ntree_nn                 0             0\ntwist_nn                0             1\nword_nn                 0             0\nCLassification score for english\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.69      0.86      0.77        21\nclass 1 (change)       0.73      0.50      0.59        16\n\n        accuracy                           0.70        37\n       macro avg       0.71      0.68      0.68        37\n    weighted avg       0.71      0.70      0.69        37\n\nWord                  Truth       Rank\n----------------  ---------  ---------\nattack_nn         0.14397    0.208083\nbag_nn            0.100364   0.169015\nball_nn           0.409367   0.233683\nbit_nn            0.306577   0.273854\nchairman_nn       0          0.248951\ncircle_vb         0.171087   0.201753\ncontemplation_nn  0.0708387  0.19774\ndonkey_nn         0.160104   0.231598\nedge_nn           0.260966   0.15058\nface_nn           0.137791   0.0939099\nfiction_nn        0.0207233  0.218485\ngas_nn            0.15957    0.209614\ngraft_nn          0.553976   0.505534\nhead_nn           0.295256   0.113097\nland_nn           0.223448   0.124811\nlane_nn           0.10372    0.233373\nlass_nn           0.21259    0.340645\nmultitude_nn      0.100364   0.370482\nounce_nn          0.284899   0.155154\npart_nn           0.161271   0.233037\npin_vb            0.207212   0.233937\nplane_nn          0.882348   0.634514\nplayer_nn         0.273667   0.332777\nprop_nn           0.62476    0.467811\nquilt_nn          0.123145   0.215391\nrag_nn            0.276515   0.187123\nrecord_nn         0.42735    0.490607\nrelationship_nn   0.0562178  0.200358\nrisk_nn           0          0.256572\nsavage_nn         0.0968689  0.339028\nstab_nn           0.40059    0.327015\nstroke_vb         0.176231   0.183563\nthump_nn          0.142992   0.227264\ntip_vb            0.678899   0.260849\ntree_nn           0.0708387  0.0922242\ntwist_nn          0.398493   0.342229\nword_nn           0.179307   0.130476\nCLassification score for english\nSpearman score for english: 0.3561562685997265\n"
    }
   ],
   "source": [
    "lang = \"english\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if moving_lncs2(word, model1, model2, 36, 0.6107) >= 0.7329\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - moving_lncs2(word, model1, model2, 36, 0.6107)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German (Hyper on ACC: thr=0.5, t=7930, NN=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word                  Truth    Prediction\n------------------  -------  ------------\nabbauen                   1             1\nabdecken                  1             0\nabgebrüht                 0             1\nAbgesang                  1             1\nAckergerät                0             0\nArmenhaus                 0             1\nartikulieren              1             1\naufrechterhalten          0             1\nAusnahmegesetz            0             0\nausspannen                1             1\nbeimischen                0             0\nDynamik                   1             1\nEinreichung               0             0\nEintagsfliege             0             1\nEngpaß                    1             1\nEntscheidung              0             0\nFestspiel                 0             0\nFrechheit                 0             0\nFuß                       0             0\nGesichtsausdruck          0             0\nKnotenpunkt               1             1\nKubikmeter                0             0\nLyzeum                    0             0\nManschette                1             0\nMißklang                  1             0\nMulatte                   0             0\nNaturschönheit            0             0\nOhrwurm                   1             1\nPachtzins                 0             0\npacken                    1             0\nRezeption                 1             1\nSchmiere                  1             0\nSeminar                   0             0\nSensation                 1             1\nSpielball                 0             0\nTier                      0             0\nTitel                     0             1\nTragfähigkeit             0             0\nTruppenteil               0             0\nüberspannen               1             0\nUnentschlossenheit        0             0\nverbauen                  1             1\nvergönnen                 0             0\nvoranstellen              0             0\nvorliegen                 0             0\nvorweisen                 0             0\nweitgreifend              0             0\nzersetzen                 0             0\nCLassification score for german\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.81      0.84      0.83        31\nclass 1 (change)       0.69      0.65      0.67        17\n\n        accuracy                           0.77        48\n       macro avg       0.75      0.74      0.75        48\n    weighted avg       0.77      0.77      0.77        48\n\nWord                    Truth      Rank\n------------------  ---------  --------\nabbauen             0.740115   0.668496\nabdecken            0.606884   0.437669\nabgebrüht           0.832645   0.981729\nAbgesang            0.578548   0.597758\nAckergerät          0          0.441013\nArmenhaus           0.51967    0.545538\nartikulieren        0.615743   0.917052\naufrechterhalten    0.0361091  0.561408\nAusnahmegesetz      0.0931384  0.41077\nausspannen          0.70669    0.531743\nbeimischen          0.307359   0.285912\nDynamik             0.578845   0.648977\nEinreichung         0          0.320492\nEintagsfliege       0.66006    0.915776\nEngpaß              0.819957   0.869187\nEntscheidung        0.141681   0.273794\nFestspiel           0.100364   0.438095\nFrechheit           0.0708387  0.282315\nFuß                 0.564633   0.490477\nGesichtsausdruck    0.0773181  0.289516\nKnotenpunkt         0.647627   0.52841\nKubikmeter          0          0.242053\nLyzeum              0.126381   0.358684\nManschette          0.355802   0.293669\nMißklang            0.379723   0.447301\nMulatte             0          0.479716\nNaturschönheit      0.0715606  0.468078\nOhrwurm             0.832451   0.731565\nPachtzins           0          0.273779\npacken              0.462253   0.311238\nRezeption           0.464989   0.613711\nSchmiere            0.437671   0.454006\nSeminar             0.0644862  0.475705\nSensation           0.406144   0.60467\nSpielball           0.10329    0.365463\nTier                0.0734664  0.316692\nTitel               0.393045   0.523495\nTragfähigkeit       0.114694   0.340359\nTruppenteil         0          0.478664\nüberspannen         0.252066   0.460862\nUnentschlossenheit  0          0.23935\nverbauen            0.578125   0.513182\nvergönnen           0.0711969  0.38024\nvoranstellen        0.124192   0.493326\nvorliegen           0.190266   0.362345\nvorweisen           0.126837   0.36145\nweitgreifend        0          0.427409\nzersetzen           0.50588    0.40412\nCLassification score for german\nSpearman score for german: 0.642314809020256\n"
    }
   ],
   "source": [
    "lang = \"german\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if moving_lncs2(word, model1, model2, 18, 0.7930) >= 0.5\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - moving_lncs2(word, model1, model2, 18, 0.7930)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latin (Hyper on ACC: thr=0.7820, t=7061, NN=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word           Truth    Prediction\n-----------  -------  ------------\nacerbus            0             0\nadsumo             1             1\nancilla            0             1\nbeatus             1             1\ncivitas            1             1\ncohors             1             1\nconsilium          0             0\nconsul             1             1\ncredo              1             1\ndolus              1             1\ndubius             1             1\ndux                1             1\nfidelis            0             1\nhonor              0             0\nhostis             0             0\nhumanitas          1             1\nimperator          1             1\nitero              0             1\njus                1             1\nlicet              1             1\nnecessarius        0             1\nnepos              1             1\nnobilitas          0             0\noportet            0             1\npoena              0             0\npontifex           1             1\npotestas           1             1\nregnum             1             1\nsacramentum        1             1\nsalus              0             1\nsanctus            1             1\nsapientia          0             1\nscriptura          1             1\nsenatus            1             0\nsensus             1             1\nsimplex            0             1\ntemplum            1             0\ntitulus            1             1\nvirtus             1             0\nvoluntas           1             1\nCLassification score for latin\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.67      0.43      0.52        14\nclass 1 (change)       0.74      0.88      0.81        26\n\n        accuracy                           0.73        40\n       macro avg       0.70      0.66      0.66        40\n    weighted avg       0.72      0.72      0.71        40\n\nWord             Truth      Rank\n-----------  ---------  --------\nacerbus      0.169367   0.207035\nadsumo       0.342616   0.546884\nancilla      0          0.355389\nbeatus       0.816392   0.554085\ncivitas      0.322392   0.513859\ncohors       0.28083    0.222891\nconsilium    0.102932   0.193067\nconsul       0.129886   0.259591\ncredo        0.370992   0.270383\ndolus        0.176682   0.285197\ndubius       0.337623   0.291272\ndux          0.289054   0.256449\nfidelis      0.170439   0.561424\nhonor        0.290373   0.202295\nhostis       0          0.161053\nhumanitas    0.455671   0.328254\nimperator    0.846816   0.450168\nitero        0.039678   0.487316\njus          0.350099   0.258068\nlicet        0.506818   0.341681\nnecessarius  0.0951902  0.264311\nnepos        0.364883   0.265674\nnobilitas    0.181606   0.216037\noportet      0.102492   0.250564\npoena        0.230906   0.176204\npontifex     0.9056     0.574545\npotestas     0.548475   0.236949\nregnum       0.355575   0.233391\nsacramentum  0.68804    0.644079\nsalus        0.469503   0.317807\nsanctus      0.425203   0.591967\nsapientia    0.234681   0.278552\nscriptura    0.516652   0.507926\nsenatus      0.264773   0.210229\nsensus       0.39351    0.243457\nsimplex      0.0095403  0.350162\ntemplum      0.370184   0.156515\ntitulus      0.619797   0.402792\nvirtus       0.39711    0.16155\nvoluntas     0.144737   0.321864\nCLassification score for latin\nSpearman score for latin: 0.3415732448994327\n"
    }
   ],
   "source": [
    "lang = \"latin\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if moving_lncs2(word, model1, model2, 43, 0.7061) >= 0.7820\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - moving_lncs2(word, model1, model2, 43, 0.7061)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swedish (Hyper on ACC: thr=0.5539, t=0.2343, NN=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word            Truth    Prediction\n------------  -------  ------------\naktiv               0             0\nannandag            0             0\nantyda              0             1\nbearbeta            0             0\nbedömande           0             0\nberedning           0             0\nblockera            0             0\nbolagsstämma        0             0\nbröllop             0             0\nby                  0             0\ncentral             0             1\nfärg                0             0\nförhandling         0             0\ngagn                0             0\ngranskare           1             0\nkemisk              0             0\nkokärt              0             0\nkonduktör           1             1\nkrita               1             0\nledning             1             0\nmedium              1             1\nmotiv               1             0\nnotis               0             0\nstudie              0             0\nundertrycka         0             0\nuppfattning         1             0\nuppfostran          0             0\nuppläggning         1             1\nuträtta             0             0\nvaktmästare         0             1\nvegetation          0             0\nCLassification score for swedish\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.80      0.87      0.83        23\nclass 1 (change)       0.50      0.38      0.43         8\n\n        accuracy                           0.74        31\n       macro avg       0.65      0.62      0.63        31\n    weighted avg       0.72      0.74      0.73        31\n\nWord                Truth      Rank\n------------  -----------  --------\naktiv         0.0870752    0.374868\nannandag      0.0708387    0.25088\nantyda        0.143225     0.848428\nbearbeta      0.243557     0.282764\nbedömande     0.000443385  0.199576\nberedning     0.26371      0.384576\nblockera      0.15957      0.232868\nbolagsstämma  0            0.37938\nbröllop       0.0708387    0.210151\nby            0.0746856    0.25969\ncentral       0.122427     0.603701\nfärg          0.163421     0.343846\nförhandling   0.00221775   0.286177\ngagn          0.0711969    0.165051\ngranskare     0.319612     0.407577\nkemisk        0.100908     0.368094\nkokärt        0            0.229245\nkonduktör     0.247635     0.571537\nkrita         0.442764     0.232005\nledning       0.337868     0.211425\nmedium        0.603554     0.528778\nmotiv         0.353028     0.248296\nnotis         0.212213     0.29076\nstudie        0.0704426    0.249318\nundertrycka   0.166747     0.245808\nuppfattning   0.195435     0.273597\nuppfostran    0            0.331533\nuppläggning   0.29304      0.472793\nuträtta       0            0.3908\nvaktmästare   0            0.481701\nvegetation    0            0.188808\nCLassification score for swedish\nSpearman score for swedish: 0.1766503016310028\n"
    }
   ],
   "source": [
    "lang = \"swedish\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if moving_lncs2(word, model1, model2, 10, 0.2343) >= 0.5539\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - moving_lncs2(word, model1, model2, 10, 0.2343)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}