{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from cade.metrics.comparative import lncs2\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "from tabulate import tabulate\n",
    "from config import CURRENT_EXP_DIR, config, get_logger, log_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load language models and groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(lang: str):\n",
    "    model1 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus1.model\"\n",
    "    )\n",
    "    model2 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus2.model\"\n",
    "    )\n",
    "    return model1, model2\n",
    "\n",
    "def get_gt(lang: str, binary=True):\n",
    "    binary_truth = numpy.loadtxt(\n",
    "        \"./data/\"\n",
    "        + lang\n",
    "        + \"/semeval2020_ulscd_\"\n",
    "        + lang[:3]\n",
    "        + \"/truth/\" + (\"binary\" if binary else \"graded\") + \".txt\",\n",
    "        dtype=str,\n",
    "        delimiter=\"\\t\",\n",
    "    )\n",
    "    return binary_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English (Hyper on ACC: thr=0.8, sim=LNCS2, NN=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word                Truth    Prediction\n----------------  -------  ------------\nattack_nn               1             0\nbag_nn                  0             0\nball_nn                 0             0\nbit_nn                  1             0\nchairman_nn             0             0\ncircle_vb               1             0\ncontemplation_nn        0             0\ndonkey_nn               0             0\nedge_nn                 1             0\nface_nn                 0             0\nfiction_nn              0             0\ngas_nn                  0             0\ngraft_nn                1             1\nhead_nn                 1             0\nland_nn                 1             0\nlane_nn                 0             0\nlass_nn                 1             0\nmultitude_nn            0             0\nounce_nn                0             0\npart_nn                 0             0\npin_vb                  0             0\nplane_nn                1             1\nplayer_nn               1             1\nprop_nn                 1             1\nquilt_nn                0             0\nrag_nn                  1             0\nrecord_nn               1             1\nrelationship_nn         0             0\nrisk_nn                 0             0\nsavage_nn               0             0\nstab_nn                 1             0\nstroke_vb               0             0\nthump_nn                1             0\ntip_vb                  1             0\ntree_nn                 0             0\ntwist_nn                0             0\nword_nn                 0             0\nCLassification score for english\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.66      1.00      0.79        21\nclass 1 (change)       1.00      0.31      0.48        16\n\n        accuracy                           0.70        37\n       macro avg       0.83      0.66      0.63        37\n    weighted avg       0.80      0.70      0.66        37\n\nWord                  Truth       Rank\n----------------  ---------  ---------\nattack_nn         0.14397    0.122633\nbag_nn            0.100364   0.0808394\nball_nn           0.409367   0.0999771\nbit_nn            0.306577   0.0912524\nchairman_nn       0          0.0788344\ncircle_vb         0.171087   0.0967214\ncontemplation_nn  0.0708387  0.0665827\ndonkey_nn         0.160104   0.0416854\nedge_nn           0.260966   0.0342517\nface_nn           0.137791   0.0752994\nfiction_nn        0.0207233  0.114111\ngas_nn            0.15957    0.109651\ngraft_nn          0.553976   0.487316\nhead_nn           0.295256   0.0210035\nland_nn           0.223448   0.157877\nlane_nn           0.10372    0.112759\nlass_nn           0.21259    0.152281\nmultitude_nn      0.100364   0.175686\nounce_nn          0.284899   0.121713\npart_nn           0.161271   0.193652\npin_vb            0.207212   0.0374466\nplane_nn          0.882348   0.533966\nplayer_nn         0.273667   0.268426\nprop_nn           0.62476    0.219481\nquilt_nn          0.123145   0.0928617\nrag_nn            0.276515   0.0370809\nrecord_nn         0.42735    0.458929\nrelationship_nn   0.0562178  0.0969648\nrisk_nn           0          0.186698\nsavage_nn         0.0968689  0.113369\nstab_nn           0.40059    0.163361\nstroke_vb         0.176231   0.0781804\nthump_nn          0.142992   0.108026\ntip_vb            0.678899   0.044822\ntree_nn           0.0708387  0.0418413\ntwist_nn          0.398493   0.163846\nword_nn           0.179307   0.0390948\nCLassification score for english\nSpearman score for english: 0.21838876390169645\n"
    }
   ],
   "source": [
    "lang = \"english\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if lncs2(word, model1, model2, 12) >= 0.8\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - lncs2(word, model1, model2, 12)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German (Hyper on ACC: thr=0.5, sim=Cosine, NN=/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word                  Truth    Prediction\n------------------  -------  ------------\nabbauen                   1             1\nabdecken                  1             0\nabgebrüht                 0             1\nAbgesang                  1             1\nAckergerät                0             0\nArmenhaus                 0             1\nartikulieren              1             1\naufrechterhalten          0             1\nAusnahmegesetz            0             0\nausspannen                1             1\nbeimischen                0             0\nDynamik                   1             1\nEinreichung               0             0\nEintagsfliege             0             1\nEngpaß                    1             1\nEntscheidung              0             0\nFestspiel                 0             0\nFrechheit                 0             0\nFuß                       0             1\nGesichtsausdruck          0             0\nKnotenpunkt               1             1\nKubikmeter                0             0\nLyzeum                    0             0\nManschette                1             0\nMißklang                  1             1\nMulatte                   0             1\nNaturschönheit            0             1\nOhrwurm                   1             1\nPachtzins                 0             0\npacken                    1             0\nRezeption                 1             1\nSchmiere                  1             0\nSeminar                   0             0\nSensation                 1             1\nSpielball                 0             0\nTier                      0             0\nTitel                     0             1\nTragfähigkeit             0             0\nTruppenteil               0             0\nüberspannen               1             0\nUnentschlossenheit        0             0\nverbauen                  1             1\nvergönnen                 0             0\nvoranstellen              0             1\nvorliegen                 0             0\nvorweisen                 0             0\nweitgreifend              0             0\nzersetzen                 0             0\nCLassification score for german\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.81      0.71      0.76        31\nclass 1 (change)       0.57      0.71      0.63        17\n\n        accuracy                           0.71        48\n       macro avg       0.69      0.71      0.70        48\n    weighted avg       0.73      0.71      0.71        48\n\nWord                    Truth      Rank\n------------------  ---------  --------\nabbauen             0.740115   0.702639\nabdecken            0.606884   0.494601\nabgebrüht           0.832645   1.02176\nAbgesang            0.578548   0.633297\nAckergerät          0          0.499709\nArmenhaus           0.51967    0.561777\nartikulieren        0.615743   0.876701\naufrechterhalten    0.0361091  0.622251\nAusnahmegesetz      0.0931384  0.43972\nausspannen          0.70669    0.573802\nbeimischen          0.307359   0.304375\nDynamik             0.578845   0.690278\nEinreichung         0          0.36764\nEintagsfliege       0.66006    0.945078\nEngpaß              0.819957   0.928241\nEntscheidung        0.141681   0.295416\nFestspiel           0.100364   0.440459\nFrechheit           0.0708387  0.337347\nFuß                 0.564633   0.539137\nGesichtsausdruck    0.0773181  0.34691\nKnotenpunkt         0.647627   0.54406\nKubikmeter          0          0.244332\nLyzeum              0.126381   0.403714\nManschette          0.355802   0.343571\nMißklang            0.379723   0.505234\nMulatte             0          0.570551\nNaturschönheit      0.0715606  0.525613\nOhrwurm             0.832451   0.774835\nPachtzins           0          0.308343\npacken              0.462253   0.383981\nRezeption           0.464989   0.656321\nSchmiere            0.437671   0.492704\nSeminar             0.0644862  0.490499\nSensation           0.406144   0.659745\nSpielball           0.10329    0.383569\nTier                0.0734664  0.386857\nTitel               0.393045   0.592178\nTragfähigkeit       0.114694   0.368452\nTruppenteil         0          0.493093\nüberspannen         0.252066   0.496525\nUnentschlossenheit  0          0.278884\nverbauen            0.578125   0.555673\nvergönnen           0.0711969  0.39825\nvoranstellen        0.124192   0.531269\nvorliegen           0.190266   0.402427\nvorweisen           0.126837   0.412472\nweitgreifend        0          0.472556\nzersetzen           0.50588    0.432466\nCLassification score for german\nSpearman score for german: 0.6435660363097846\n"
    }
   ],
   "source": [
    "lang = \"german\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if 1 - cosine(model1[word], model2[word]) >= 0.5\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = cosine(model1[word], model2[word])\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latin (Hyper on ACC: thr=0.8, sim=Cosine, NN=/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word           Truth    Prediction\n-----------  -------  ------------\nacerbus            0             1\nadsumo             1             1\nancilla            0             1\nbeatus             1             1\ncivitas            1             1\ncohors             1             1\nconsilium          0             0\nconsul             1             1\ncredo              1             1\ndolus              1             1\ndubius             1             1\ndux                1             1\nfidelis            0             1\nhonor              0             0\nhostis             0             0\nhumanitas          1             1\nimperator          1             1\nitero              0             1\njus                1             1\nlicet              1             1\nnecessarius        0             1\nnepos              1             1\nnobilitas          0             1\noportet            0             1\npoena              0             0\npontifex           1             1\npotestas           1             1\nregnum             1             1\nsacramentum        1             1\nsalus              0             1\nsanctus            1             1\nsapientia          0             1\nscriptura          1             1\nsenatus            1             1\nsensus             1             1\nsimplex            0             1\ntemplum            1             0\ntitulus            1             1\nvirtus             1             0\nvoluntas           1             1\nCLassification score for latin\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.67      0.29      0.40        14\nclass 1 (change)       0.71      0.92      0.80        26\n\n        accuracy                           0.70        40\n       macro avg       0.69      0.60      0.60        40\n    weighted avg       0.69      0.70      0.66        40\n\nWord             Truth      Rank\n-----------  ---------  --------\nacerbus      0.169367   0.2607\nadsumo       0.342616   0.60656\nancilla      0          0.378654\nbeatus       0.816392   0.515063\ncivitas      0.322392   0.510087\ncohors       0.28083    0.258232\nconsilium    0.102932   0.184222\nconsul       0.129886   0.313464\ncredo        0.370992   0.297098\ndolus        0.176682   0.326791\ndubius       0.337623   0.336072\ndux          0.289054   0.242273\nfidelis      0.170439   0.527022\nhonor        0.290373   0.199342\nhostis       0          0.178748\nhumanitas    0.455671   0.40189\nimperator    0.846816   0.392853\nitero        0.039678   0.54907\njus          0.350099   0.23202\nlicet        0.506818   0.403473\nnecessarius  0.0951902  0.295421\nnepos        0.364883   0.322389\nnobilitas    0.181606   0.213728\noportet      0.102492   0.276004\npoena        0.230906   0.166222\npontifex     0.9056     0.632757\npotestas     0.548475   0.21752\nregnum       0.355575   0.221556\nsacramentum  0.68804    0.654\nsalus        0.469503   0.31473\nsanctus      0.425203   0.625461\nsapientia    0.234681   0.30917\nscriptura    0.516652   0.642499\nsenatus      0.264773   0.227833\nsensus       0.39351    0.247708\nsimplex      0.0095403  0.378102\ntemplum      0.370184   0.181873\ntitulus      0.619797   0.445423\nvirtus       0.39711    0.196581\nvoluntas     0.144737   0.330577\nCLassification score for latin\nSpearman score for latin: 0.32693841210505986\n"
    }
   ],
   "source": [
    "lang = \"latin\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if 1 - cosine(model1[word], model2[word]) >= 0.8\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = cosine(model1[word], model2[word])\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swedish (Hyper on ACC: thr=0.5, sim=LNCS2, NN=1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word            Truth    Prediction\n------------  -------  ------------\naktiv               0             0\nannandag            0             0\nantyda              0             1\nbearbeta            0             0\nbedömande           0             0\nberedning           0             0\nblockera            0             0\nbolagsstämma        0             0\nbröllop             0             0\nby                  0             0\ncentral             0             0\nfärg                0             0\nförhandling         0             0\ngagn                0             0\ngranskare           1             1\nkemisk              0             0\nkokärt              0             0\nkonduktör           1             1\nkrita               1             0\nledning             1             0\nmedium              1             1\nmotiv               1             0\nnotis               0             0\nstudie              0             0\nundertrycka         0             0\nuppfattning         1             0\nuppfostran          0             0\nuppläggning         1             0\nuträtta             0             0\nvaktmästare         0             1\nvegetation          0             0\nCLassification score for swedish\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.81      0.91      0.86        23\nclass 1 (change)       0.60      0.38      0.46         8\n\n        accuracy                           0.77        31\n       macro avg       0.70      0.64      0.66        31\n    weighted avg       0.75      0.77      0.76        31\n\nWord                Truth        Rank\n------------  -----------  ----------\naktiv         0.0870752    0.288123\nannandag      0.0708387    0.110008\nantyda        0.143225     0.824448\nbearbeta      0.243557     0.144819\nbedömande     0.000443385  0.0402077\nberedning     0.26371      0.223672\nblockera      0.15957      0.00852626\nbolagsstämma  0            0.205453\nbröllop       0.0708387    0.133821\nby            0.0746856    0.215911\ncentral       0.122427     0.415133\nfärg          0.163421     0.30982\nförhandling   0.00221775   0.07374\ngagn          0.0711969    0.0120458\ngranskare     0.319612     0.501732\nkemisk        0.100908     0.165807\nkokärt        0            0.215621\nkonduktör     0.247635     0.511142\nkrita         0.442764     0.141838\nledning       0.337868     0.31469\nmedium        0.603554     0.623542\nmotiv         0.353028     0.399022\nnotis         0.212213     0.0501775\nstudie        0.0704426    0.0722197\nundertrycka   0.166747     0.0636877\nuppfattning   0.195435     0.231654\nuppfostran    0            0.216713\nuppläggning   0.29304      0.371512\nuträtta       0            0.277487\nvaktmästare   0            0.514996\nvegetation    0            0.00321758\nCLassification score for swedish\nSpearman score for swedish: 0.36159689463299194\n"
    }
   ],
   "source": [
    "lang = \"swedish\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if lncs2(word, model1, model2, 2) >= 0.5\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - lncs2(word, model1, model2, 2)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}