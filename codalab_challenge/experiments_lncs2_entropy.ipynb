{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from cade.metrics.comparative import moving_lncs2, lncs2, get_neighbors_set\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import entropy\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "from tabulate import tabulate\n",
    "from config import CURRENT_EXP_DIR, config, get_logger, log_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load language models and groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(lang: str):\n",
    "    model1 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus1.model\"\n",
    "    )\n",
    "    model2 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus2.model\"\n",
    "    )\n",
    "    return model1, model2\n",
    "\n",
    "def get_gt(lang: str, binary=True):\n",
    "    binary_truth = numpy.loadtxt(\n",
    "        \"./data/\"\n",
    "        + lang\n",
    "        + \"/semeval2020_ulscd_\"\n",
    "        + lang[:3]\n",
    "        + \"/truth/\" + (\"binary\" if binary else \"graded\") + \".txt\",\n",
    "        dtype=str,\n",
    "        delimiter=\"\\t\",\n",
    "    )\n",
    "    return binary_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefined LNCS2 with softmax and normalized entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lncs2_entropy(word, m1, m2, topn, verbose=False):\n",
    "    \"\"\"\n",
    "    https://www.aclweb.org/anthology/D16-1229/\n",
    "\n",
    "    :param word:\n",
    "    :param m1:\n",
    "    :param m2:\n",
    "    :param topn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    words_m1 = list(get_neighbors_set(word, m1, topn))\n",
    "    words_m2 = list(get_neighbors_set(word, m2, topn))\n",
    "\n",
    "    vec_1 = []\n",
    "    vec_2 = []\n",
    "    avg = 0\n",
    "    mean = False\n",
    "\n",
    "    # Cosine similarity between \"word\" and every word in its m1-neighbour\n",
    "    # within the m1 space\n",
    "    for wtest in words_m1:\n",
    "        vec_1.append(1 - cosine(m1.wv[word], m1.wv[wtest]))\n",
    "\n",
    "    # Cosine similarity between \"word\" and every word in its m2-neighbour\n",
    "    # within the m1 space\n",
    "    for wtest in words_m2:\n",
    "        if wtest not in m1.wv.vocab:\n",
    "            if not mean:\n",
    "                # Represent OOV words in m1 space empirically with its mean\n",
    "                avg = numpy.average(m1[m1.wv.vocab], axis=0)\n",
    "                mean = True\n",
    "            vec_1.append(1 - cosine(m1[word], avg))\n",
    "        else:\n",
    "            vec_1.append(1 - cosine(m1.wv[word], m1.wv[wtest]))\n",
    "\n",
    "    avg = 0\n",
    "    mean = False\n",
    "\n",
    "    # Cosine similarity between \"word\" and every word in its m1-neighbour\n",
    "    # within the m2 space\n",
    "    for wtest in words_m1:\n",
    "        if wtest not in m2.wv.vocab:\n",
    "            if not mean:\n",
    "                # Represent OOV words in m1 space empirically with its mean\n",
    "                avg = numpy.average(m2[m2.wv.vocab], axis=0)\n",
    "                mean = True\n",
    "            vec_2.append(1 - cosine(m2[word], avg))\n",
    "        else:\n",
    "            vec_2.append(1 - cosine(m2.wv[word], m2.wv[wtest]))\n",
    "\n",
    "    # Cosine similarity between \"word\" and every word in its m2-neighbour\n",
    "    # within the m2 space\n",
    "    for wtest in words_m2:\n",
    "        vec_2.append(1 - cosine(m2.wv[word], m2.wv[wtest]))\n",
    "\n",
    "    if verbose:\n",
    "        print(vec_1)\n",
    "    vec_1 = numpy.exp(-numpy.array(vec_1))\n",
    "    vec_1 = vec_1 / vec_1.sum()\n",
    "    if verbose:\n",
    "        print(vec_1)\n",
    "    vec_2 = numpy.exp(-numpy.array(vec_2))\n",
    "    vec_2 = vec_2 / vec_2.sum()\n",
    "\n",
    "    return entropy(vec_1, vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English (Hyper on ACC: thr=0.7329, t=0.6107, NN=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.666697084903717, 0.6352203488349915, 0.5186610221862793, 0.8356345891952515, 0.5687299966812134, 0.49635517597198486, 0.6045973300933838, 0.5100687742233276, 0.5714151859283447, 0.6168639659881592, 0.5856351852416992, 0.5778291821479797, 0.5923551917076111, 0.5167604684829712, 0.5109189748764038, 0.5468693375587463, 0.5224267244338989, 0.5034369230270386, 0.4993384778499603, 0.4993321895599365, 0.5561960935592651, 0.5072870254516602, 0.5236865282058716, 0.6097083687782288, 0.5127843618392944, 0.5087453722953796, 0.5462506413459778, 0.5315718054771423, 0.5868595242500305, 0.5237288475036621, 0.25663283467292786, 0.25663283467292786, 0.5186610221862793, 0.25663283467292786, 0.8356345891952515, 0.41574421525001526, 0.2621910870075226, 0.3715507686138153, 0.10474476218223572, 0.6045973300933838, 0.06434650719165802, 0.25663283467292786, 0.6168639659881592, 0.21564006805419922, 0.09363753348588943, 0.30701854825019836, 0.25663283467292786, 0.4748297929763794, 0.31637004017829895, 0.08262276649475098, 0.19816236197948456, 0.4523712992668152, 0.3416837453842163, 0.48572683334350586, 0.6097083687782288, 0.25663283467292786, 0.3460735082626343, 0.3682735860347748, 0.5868595242500305, 0.3218284845352173]\n[0.01329165 0.01371668 0.01541239 0.01122562 0.01465971 0.01576004\n 0.01414322 0.01554539 0.0146204  0.01397079 0.01441397 0.01452692\n 0.01431743 0.01544171 0.01553218 0.01498371 0.01535446 0.01564883\n 0.0157131  0.01571319 0.01484461 0.01558869 0.01533513 0.01407112\n 0.01550323 0.01556598 0.01499298 0.01521469 0.01439633 0.01533448\n 0.02002938 0.02002938 0.01541239 0.02002938 0.01122562 0.01708308\n 0.01991836 0.01785497 0.0233148  0.01414322 0.02427596 0.02002938\n 0.01397079 0.0208675  0.0235752  0.01904519 0.02002938 0.01610296\n 0.01886792 0.02383631 0.02123542 0.0164687  0.01839629 0.01592844\n 0.01407112 0.02002938 0.01831571 0.01791358 0.01439633 0.01876521]\nWord                Truth    Prediction\n----------------  -------  ------------\nattack_nn               1     0.0343659\nbag_nn                  0     0.0511917\nball_nn                 0     0.0290455\nbit_nn                  1     0.0288706\nchairman_nn             0     0.0338413\ncircle_vb               1     0.0307461\ncontemplation_nn        0     0.0205249\ndonkey_nn               0     0.0251075\nedge_nn                 1     0.0146095\nface_nn                 0     0.0259438\nfiction_nn              0     0.0316537\ngas_nn                  0     0.0543606\ngraft_nn                1     0.0928128\nhead_nn                 1     0.0136075\nland_nn                 1     0.0342901\nlane_nn                 0     0.0340873\nlass_nn                 1     0.0406923\nmultitude_nn            0     0.0383933\nounce_nn                0     0.0469719\npart_nn                 0     0.0290399\npin_vb                  0     0.028613\nplane_nn                1     0.125544\nplayer_nn               1     0.0639871\nprop_nn                 1     0.0399178\nquilt_nn                0     0.0318047\nrag_nn                  1     0.0186201\nrecord_nn               1     0.0751602\nrelationship_nn         0     0.0269006\nrisk_nn                 0     0.0483682\nsavage_nn               0     0.0428883\nstab_nn                 1     0.0404704\nstroke_vb               0     0.0313168\nthump_nn                1     0.0406135\ntip_vb                  1     0.0170159\ntree_nn                 0     0.0152777\ntwist_nn                0     0.0370963\nword_nn                 0     0.0212691\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "' print(\"CLassification score for \" + lang)\\nprint(\\n    \"\\n\"\\n    + classification_report(\\n        binary_truth[:, 1].astype(float),\\n        numpy.array(predictions),\\n        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\\n    )\\n)\\n# Load scores truths\\nscore_truth = get_gt(lang, binary=False)\\ntable = []\\n# Task 2 - Semantic Shift Score\\nscores = []\\ni = 0\\nfor word in score_truth[:, 0]:\\n    score = 1 - moving_lncs2(word, model1, model2, 36, 0.6107)\\n    scores.append(score)\\n    table.append([word, str(score_truth[i, 1]), str(score)])\\n    i += 1\\nprint(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\\nrho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\\nprint(\"CLassification score for \" + lang)\\nprint(\"Spearman score for \" + lang + \": \" + str(rho)) '"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "lang = \"english\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "verbose = True\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = lncs2_entropy(word, model1, model2, 30, verbose)\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "    if verbose:\n",
    "        verbose = False\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "\"\"\" print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - moving_lncs2(word, model1, model2, 36, 0.6107)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German (Hyper on ACC: thr=0.5, t=0.7930, NN=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word                  Truth    Prediction\n------------------  -------  ------------\nabbauen                   1             1\nabdecken                  1             0\nabgebrüht                 0             1\nAbgesang                  1             1\nAckergerät                0             0\nArmenhaus                 0             1\nartikulieren              1             1\naufrechterhalten          0             1\nAusnahmegesetz            0             0\nausspannen                1             1\nbeimischen                0             0\nDynamik                   1             1\nEinreichung               0             0\nEintagsfliege             0             1\nEngpaß                    1             1\nEntscheidung              0             0\nFestspiel                 0             0\nFrechheit                 0             0\nFuß                       0             0\nGesichtsausdruck          0             0\nKnotenpunkt               1             1\nKubikmeter                0             0\nLyzeum                    0             0\nManschette                1             0\nMißklang                  1             0\nMulatte                   0             0\nNaturschönheit            0             0\nOhrwurm                   1             1\nPachtzins                 0             0\npacken                    1             0\nRezeption                 1             1\nSchmiere                  1             0\nSeminar                   0             0\nSensation                 1             1\nSpielball                 0             0\nTier                      0             0\nTitel                     0             1\nTragfähigkeit             0             0\nTruppenteil               0             0\nüberspannen               1             0\nUnentschlossenheit        0             0\nverbauen                  1             1\nvergönnen                 0             0\nvoranstellen              0             0\nvorliegen                 0             0\nvorweisen                 0             0\nweitgreifend              0             0\nzersetzen                 0             0\nCLassification score for german\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.81      0.84      0.83        31\nclass 1 (change)       0.69      0.65      0.67        17\n\n        accuracy                           0.77        48\n       macro avg       0.75      0.74      0.75        48\n    weighted avg       0.77      0.77      0.77        48\n\nWord                    Truth      Rank\n------------------  ---------  --------\nabbauen             0.740115   0.668496\nabdecken            0.606884   0.437669\nabgebrüht           0.832645   0.981729\nAbgesang            0.578548   0.597758\nAckergerät          0          0.441013\nArmenhaus           0.51967    0.545538\nartikulieren        0.615743   0.917052\naufrechterhalten    0.0361091  0.561408\nAusnahmegesetz      0.0931384  0.41077\nausspannen          0.70669    0.531743\nbeimischen          0.307359   0.285912\nDynamik             0.578845   0.648977\nEinreichung         0          0.320492\nEintagsfliege       0.66006    0.915776\nEngpaß              0.819957   0.869187\nEntscheidung        0.141681   0.273794\nFestspiel           0.100364   0.438095\nFrechheit           0.0708387  0.282315\nFuß                 0.564633   0.490477\nGesichtsausdruck    0.0773181  0.289516\nKnotenpunkt         0.647627   0.52841\nKubikmeter          0          0.242053\nLyzeum              0.126381   0.358684\nManschette          0.355802   0.293669\nMißklang            0.379723   0.447301\nMulatte             0          0.479716\nNaturschönheit      0.0715606  0.468078\nOhrwurm             0.832451   0.731565\nPachtzins           0          0.273779\npacken              0.462253   0.311238\nRezeption           0.464989   0.613711\nSchmiere            0.437671   0.454006\nSeminar             0.0644862  0.475705\nSensation           0.406144   0.60467\nSpielball           0.10329    0.365463\nTier                0.0734664  0.316692\nTitel               0.393045   0.523495\nTragfähigkeit       0.114694   0.340359\nTruppenteil         0          0.478664\nüberspannen         0.252066   0.460862\nUnentschlossenheit  0          0.23935\nverbauen            0.578125   0.513182\nvergönnen           0.0711969  0.38024\nvoranstellen        0.124192   0.493326\nvorliegen           0.190266   0.362345\nvorweisen           0.126837   0.36145\nweitgreifend        0          0.427409\nzersetzen           0.50588    0.40412\nCLassification score for german\nSpearman score for german: 0.642314809020256\n"
    }
   ],
   "source": [
    "lang = \"german\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if moving_lncs2(word, model1, model2, 18, 0.7930) >= 0.5\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - moving_lncs2(word, model1, model2, 18, 0.7930)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latin (Hyper on ACC: thr=0.7820, t=7061, NN=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word           Truth    Prediction\n-----------  -------  ------------\nacerbus            0             0\nadsumo             1             1\nancilla            0             1\nbeatus             1             1\ncivitas            1             1\ncohors             1             1\nconsilium          0             0\nconsul             1             1\ncredo              1             1\ndolus              1             1\ndubius             1             1\ndux                1             1\nfidelis            0             1\nhonor              0             0\nhostis             0             0\nhumanitas          1             1\nimperator          1             1\nitero              0             1\njus                1             1\nlicet              1             1\nnecessarius        0             1\nnepos              1             1\nnobilitas          0             0\noportet            0             1\npoena              0             0\npontifex           1             1\npotestas           1             1\nregnum             1             1\nsacramentum        1             1\nsalus              0             1\nsanctus            1             1\nsapientia          0             1\nscriptura          1             1\nsenatus            1             0\nsensus             1             1\nsimplex            0             1\ntemplum            1             0\ntitulus            1             1\nvirtus             1             0\nvoluntas           1             1\nCLassification score for latin\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.67      0.43      0.52        14\nclass 1 (change)       0.74      0.88      0.81        26\n\n        accuracy                           0.73        40\n       macro avg       0.70      0.66      0.66        40\n    weighted avg       0.72      0.72      0.71        40\n\nWord             Truth      Rank\n-----------  ---------  --------\nacerbus      0.169367   0.207035\nadsumo       0.342616   0.546884\nancilla      0          0.355389\nbeatus       0.816392   0.554085\ncivitas      0.322392   0.513859\ncohors       0.28083    0.222891\nconsilium    0.102932   0.193067\nconsul       0.129886   0.259591\ncredo        0.370992   0.270383\ndolus        0.176682   0.285197\ndubius       0.337623   0.291272\ndux          0.289054   0.256449\nfidelis      0.170439   0.561424\nhonor        0.290373   0.202295\nhostis       0          0.161053\nhumanitas    0.455671   0.328254\nimperator    0.846816   0.450168\nitero        0.039678   0.487316\njus          0.350099   0.258068\nlicet        0.506818   0.341681\nnecessarius  0.0951902  0.264311\nnepos        0.364883   0.265674\nnobilitas    0.181606   0.216037\noportet      0.102492   0.250564\npoena        0.230906   0.176204\npontifex     0.9056     0.574545\npotestas     0.548475   0.236949\nregnum       0.355575   0.233391\nsacramentum  0.68804    0.644079\nsalus        0.469503   0.317807\nsanctus      0.425203   0.591967\nsapientia    0.234681   0.278552\nscriptura    0.516652   0.507926\nsenatus      0.264773   0.210229\nsensus       0.39351    0.243457\nsimplex      0.0095403  0.350162\ntemplum      0.370184   0.156515\ntitulus      0.619797   0.402792\nvirtus       0.39711    0.16155\nvoluntas     0.144737   0.321864\nCLassification score for latin\nSpearman score for latin: 0.3415732448994327\n"
    }
   ],
   "source": [
    "lang = \"latin\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if moving_lncs2(word, model1, model2, 43, 0.7061) >= 0.7820\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - moving_lncs2(word, model1, model2, 43, 0.7061)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swedish (Hyper on ACC: thr=0.5539, t=0.2343, NN=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word            Truth    Prediction\n------------  -------  ------------\naktiv               0             0\nannandag            0             0\nantyda              0             1\nbearbeta            0             0\nbedömande           0             0\nberedning           0             0\nblockera            0             0\nbolagsstämma        0             0\nbröllop             0             0\nby                  0             0\ncentral             0             1\nfärg                0             0\nförhandling         0             0\ngagn                0             0\ngranskare           1             0\nkemisk              0             0\nkokärt              0             0\nkonduktör           1             1\nkrita               1             0\nledning             1             0\nmedium              1             1\nmotiv               1             0\nnotis               0             0\nstudie              0             0\nundertrycka         0             0\nuppfattning         1             0\nuppfostran          0             0\nuppläggning         1             1\nuträtta             0             0\nvaktmästare         0             1\nvegetation          0             0\nCLassification score for swedish\n\n                  precision    recall  f1-score   support\n\nclass 0 (stable)       0.80      0.87      0.83        23\nclass 1 (change)       0.50      0.38      0.43         8\n\n        accuracy                           0.74        31\n       macro avg       0.65      0.62      0.63        31\n    weighted avg       0.72      0.74      0.73        31\n\nWord                Truth      Rank\n------------  -----------  --------\naktiv         0.0870752    0.374868\nannandag      0.0708387    0.25088\nantyda        0.143225     0.848428\nbearbeta      0.243557     0.282764\nbedömande     0.000443385  0.199576\nberedning     0.26371      0.384576\nblockera      0.15957      0.232868\nbolagsstämma  0            0.37938\nbröllop       0.0708387    0.210151\nby            0.0746856    0.25969\ncentral       0.122427     0.603701\nfärg          0.163421     0.343846\nförhandling   0.00221775   0.286177\ngagn          0.0711969    0.165051\ngranskare     0.319612     0.407577\nkemisk        0.100908     0.368094\nkokärt        0            0.229245\nkonduktör     0.247635     0.571537\nkrita         0.442764     0.232005\nledning       0.337868     0.211425\nmedium        0.603554     0.528778\nmotiv         0.353028     0.248296\nnotis         0.212213     0.29076\nstudie        0.0704426    0.249318\nundertrycka   0.166747     0.245808\nuppfattning   0.195435     0.273597\nuppfostran    0            0.331533\nuppläggning   0.29304      0.472793\nuträtta       0            0.3908\nvaktmästare   0            0.481701\nvegetation    0            0.188808\nCLassification score for swedish\nSpearman score for swedish: 0.1766503016310028\n"
    }
   ],
   "source": [
    "lang = \"swedish\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Load binary truths\n",
    "binary_truth = get_gt(lang)\n",
    "# Task 1 - Binary Classification\n",
    "table = []\n",
    "predictions = []\n",
    "i = 0\n",
    "for word in binary_truth[:, 0]:\n",
    "    prediction = (\n",
    "        0\n",
    "        if moving_lncs2(word, model1, model2, 10, 0.2343) >= 0.5539\n",
    "        else 1\n",
    "    )\n",
    "    predictions.append(prediction)\n",
    "    table.append([word, str(binary_truth[i, 1]), str(prediction)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Prediction\"]))\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + classification_report(\n",
    "        binary_truth[:, 1].astype(float),\n",
    "        numpy.array(predictions),\n",
    "        target_names=[\"class 0 (stable)\", \"class 1 (change)\"],\n",
    "    )\n",
    ")\n",
    "# Load scores truths\n",
    "score_truth = get_gt(lang, binary=False)\n",
    "table = []\n",
    "# Task 2 - Semantic Shift Score\n",
    "scores = []\n",
    "i = 0\n",
    "for word in score_truth[:, 0]:\n",
    "    score = 1 - moving_lncs2(word, model1, model2, 10, 0.2343)\n",
    "    scores.append(score)\n",
    "    table.append([word, str(score_truth[i, 1]), str(score)])\n",
    "    i += 1\n",
    "print(tabulate(table, headers=[\"Word\",\"Truth\", \"Rank\"]))\n",
    "rho, _ = spearmanr(scores, score_truth[:, 1], nan_policy=\"raise\")\n",
    "print(\"CLassification score for \" + lang)\n",
    "print(\"Spearman score for \" + lang + \": \" + str(rho))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}