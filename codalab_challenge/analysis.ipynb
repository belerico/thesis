{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from cade.metrics.comparative import lncs2, intersection_nn, initialize_avgs, get_neighbors_set\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from pandas import pandas\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "from tabulate import tabulate\n",
    "from config import CURRENT_EXP_DIR, config, get_logger, log_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load language models and groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(lang: str):\n",
    "    model1 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus1.model\"\n",
    "    )\n",
    "    model2 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus2.model\"\n",
    "    )\n",
    "    return model1, model2\n",
    "\n",
    "def get_gt(lang: str, binary=True):\n",
    "    binary_truth = numpy.loadtxt(\n",
    "        \"./data/\"\n",
    "        + lang\n",
    "        + \"/semeval2020_ulscd_\"\n",
    "        + lang[:3]\n",
    "        + \"/truth/\" + (\"binary\" if binary else \"graded\") + \".txt\",\n",
    "        dtype=str,\n",
    "        delimiter=\"\\t\",\n",
    "    )\n",
    "    return binary_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English (get LNCS2, Intersection_NN and Cosine scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang = \"english\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Initialize models avgs\n",
    "initialize_avgs(model1, model2)\n",
    "shared_vocabulary = set(model1.wv.vocab.keys()).intersection(set(model2.wv.vocab.keys()))\n",
    "shared_vocabulary_df = pandas.DataFrame(shared_vocabulary, columns=[\"word\"])\n",
    "shared_vocabulary_df[\"lncs2\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: lncs2(word, model1, model2, 25)\n",
    ")\n",
    "shared_vocabulary_df[\"intersection_nn\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: intersection_nn(word, model1, model2)\n",
    ")\n",
    "shared_vocabulary_df[\"cosine\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: 1 - cosine(model1.wv[word], model2.wv[word])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add mean of the three metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shared_vocabulary_df[\"mean\"] = shared_vocabulary_df[[\"lncs2\", \"cosine\", \"intersection_nn\"]].apply(\n",
    "    lambda x: (x.lncs2 + x.cosine + x.intersection_nn) / 3, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocabulary_df[\"count_m1\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: model1.wv.vocab[word].count\n",
    ")\n",
    "shared_vocabulary_df[\"count_m2\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: model2.wv.vocab[word].count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocabulary_df.to_pickle(\"./shared_vocabulary_metrics.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words changed the most (by LNCS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n10661       pregnant -0.059092            0.988 -0.001763        31       158   \n24935         unused -0.059081            0.987 -0.073233        26        36   \n20337         scarce -0.044323            0.990  0.072006       329        61   \n20078        incline -0.034136            0.988  0.176630       114        25   \n6585              ml -0.029649            0.998 -0.079436         1        18   \n4671           major -0.016591            0.998  0.263108       531      1554   \n5073         someday  0.002855            0.988  0.115850         1       125   \n16236       mentally  0.016916            0.980  0.328319        31        90   \n147            tense  0.032603            0.984  0.138092        11       139   \n19984            err  0.036362            0.974  0.169528        70        11   \n15005        someone  0.045325            0.985  0.050252         3      1589   \n16315        backing  0.045896            0.987 -0.038172         5        42   \n18442          twain  0.055887            0.998  0.011805        29        31   \n26316       juvenile  0.056204            0.974  0.037996        32        56   \n23333             hy  0.056843            0.992  0.008580       109        11   \n7053   significantly  0.058707            0.998 -0.010645        31       123   \n20197          today  0.063035            0.983  0.364674       402      1973   \n10305        hearing  0.064546            0.990 -0.008649         1       149   \n17195         some--  0.068811            0.964 -0.110117         2         6   \n20128        drastic  0.069603            0.993  0.058760         3        48   \n17361     detachment  0.070652            0.993  0.089520        77        44   \n11084     deflection  0.071443            0.983  0.267019        15         3   \n21890        discard  0.078995            0.995  0.215280        57       109   \n16451      cheapness  0.079505            0.984  0.070801        17         4   \n16344        roughly  0.084750            0.997  0.068283        50       182   \n23643     conveyance  0.085479            0.985  0.159299        44         4   \n10956         boring  0.089093            0.991 -0.009177         8        77   \n12508          check  0.091327            0.979  0.178886       367      1028   \n11922    frightening  0.095688            0.992 -0.102361         3        76   \n18608        burthen  0.101387            0.981  0.162739        49         1   \n\n           mean  \n10661  0.309048  \n24935  0.284895  \n20337  0.339228  \n20078  0.376831  \n6585   0.296305  \n4671   0.414839  \n5073   0.368902  \n16236  0.441745  \n147    0.384898  \n19984  0.393297  \n15005  0.360192  \n16315  0.331575  \n18442  0.355231  \n26316  0.356067  \n23333  0.352474  \n7053   0.348688  \n20197  0.470237  \n10305  0.348632  \n17195  0.307565  \n20128  0.373788  \n17361  0.384391  \n11084  0.440487  \n21890  0.429758  \n16451  0.378102  \n16344  0.383344  \n23643  0.409926  \n10956  0.356972  \n12508  0.416404  \n11922  0.328442  \n18608  0.415042  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10661</th>\n      <td>pregnant</td>\n      <td>-0.059092</td>\n      <td>0.988</td>\n      <td>-0.001763</td>\n      <td>31</td>\n      <td>158</td>\n      <td>0.309048</td>\n    </tr>\n    <tr>\n      <th>24935</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.987</td>\n      <td>-0.073233</td>\n      <td>26</td>\n      <td>36</td>\n      <td>0.284895</td>\n    </tr>\n    <tr>\n      <th>20337</th>\n      <td>scarce</td>\n      <td>-0.044323</td>\n      <td>0.990</td>\n      <td>0.072006</td>\n      <td>329</td>\n      <td>61</td>\n      <td>0.339228</td>\n    </tr>\n    <tr>\n      <th>20078</th>\n      <td>incline</td>\n      <td>-0.034136</td>\n      <td>0.988</td>\n      <td>0.176630</td>\n      <td>114</td>\n      <td>25</td>\n      <td>0.376831</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>4671</th>\n      <td>major</td>\n      <td>-0.016591</td>\n      <td>0.998</td>\n      <td>0.263108</td>\n      <td>531</td>\n      <td>1554</td>\n      <td>0.414839</td>\n    </tr>\n    <tr>\n      <th>5073</th>\n      <td>someday</td>\n      <td>0.002855</td>\n      <td>0.988</td>\n      <td>0.115850</td>\n      <td>1</td>\n      <td>125</td>\n      <td>0.368902</td>\n    </tr>\n    <tr>\n      <th>16236</th>\n      <td>mentally</td>\n      <td>0.016916</td>\n      <td>0.980</td>\n      <td>0.328319</td>\n      <td>31</td>\n      <td>90</td>\n      <td>0.441745</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>tense</td>\n      <td>0.032603</td>\n      <td>0.984</td>\n      <td>0.138092</td>\n      <td>11</td>\n      <td>139</td>\n      <td>0.384898</td>\n    </tr>\n    <tr>\n      <th>19984</th>\n      <td>err</td>\n      <td>0.036362</td>\n      <td>0.974</td>\n      <td>0.169528</td>\n      <td>70</td>\n      <td>11</td>\n      <td>0.393297</td>\n    </tr>\n    <tr>\n      <th>15005</th>\n      <td>someone</td>\n      <td>0.045325</td>\n      <td>0.985</td>\n      <td>0.050252</td>\n      <td>3</td>\n      <td>1589</td>\n      <td>0.360192</td>\n    </tr>\n    <tr>\n      <th>16315</th>\n      <td>backing</td>\n      <td>0.045896</td>\n      <td>0.987</td>\n      <td>-0.038172</td>\n      <td>5</td>\n      <td>42</td>\n      <td>0.331575</td>\n    </tr>\n    <tr>\n      <th>18442</th>\n      <td>twain</td>\n      <td>0.055887</td>\n      <td>0.998</td>\n      <td>0.011805</td>\n      <td>29</td>\n      <td>31</td>\n      <td>0.355231</td>\n    </tr>\n    <tr>\n      <th>26316</th>\n      <td>juvenile</td>\n      <td>0.056204</td>\n      <td>0.974</td>\n      <td>0.037996</td>\n      <td>32</td>\n      <td>56</td>\n      <td>0.356067</td>\n    </tr>\n    <tr>\n      <th>23333</th>\n      <td>hy</td>\n      <td>0.056843</td>\n      <td>0.992</td>\n      <td>0.008580</td>\n      <td>109</td>\n      <td>11</td>\n      <td>0.352474</td>\n    </tr>\n    <tr>\n      <th>7053</th>\n      <td>significantly</td>\n      <td>0.058707</td>\n      <td>0.998</td>\n      <td>-0.010645</td>\n      <td>31</td>\n      <td>123</td>\n      <td>0.348688</td>\n    </tr>\n    <tr>\n      <th>20197</th>\n      <td>today</td>\n      <td>0.063035</td>\n      <td>0.983</td>\n      <td>0.364674</td>\n      <td>402</td>\n      <td>1973</td>\n      <td>0.470237</td>\n    </tr>\n    <tr>\n      <th>10305</th>\n      <td>hearing</td>\n      <td>0.064546</td>\n      <td>0.990</td>\n      <td>-0.008649</td>\n      <td>1</td>\n      <td>149</td>\n      <td>0.348632</td>\n    </tr>\n    <tr>\n      <th>17195</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.964</td>\n      <td>-0.110117</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.307565</td>\n    </tr>\n    <tr>\n      <th>20128</th>\n      <td>drastic</td>\n      <td>0.069603</td>\n      <td>0.993</td>\n      <td>0.058760</td>\n      <td>3</td>\n      <td>48</td>\n      <td>0.373788</td>\n    </tr>\n    <tr>\n      <th>17361</th>\n      <td>detachment</td>\n      <td>0.070652</td>\n      <td>0.993</td>\n      <td>0.089520</td>\n      <td>77</td>\n      <td>44</td>\n      <td>0.384391</td>\n    </tr>\n    <tr>\n      <th>11084</th>\n      <td>deflection</td>\n      <td>0.071443</td>\n      <td>0.983</td>\n      <td>0.267019</td>\n      <td>15</td>\n      <td>3</td>\n      <td>0.440487</td>\n    </tr>\n    <tr>\n      <th>21890</th>\n      <td>discard</td>\n      <td>0.078995</td>\n      <td>0.995</td>\n      <td>0.215280</td>\n      <td>57</td>\n      <td>109</td>\n      <td>0.429758</td>\n    </tr>\n    <tr>\n      <th>16451</th>\n      <td>cheapness</td>\n      <td>0.079505</td>\n      <td>0.984</td>\n      <td>0.070801</td>\n      <td>17</td>\n      <td>4</td>\n      <td>0.378102</td>\n    </tr>\n    <tr>\n      <th>16344</th>\n      <td>roughly</td>\n      <td>0.084750</td>\n      <td>0.997</td>\n      <td>0.068283</td>\n      <td>50</td>\n      <td>182</td>\n      <td>0.383344</td>\n    </tr>\n    <tr>\n      <th>23643</th>\n      <td>conveyance</td>\n      <td>0.085479</td>\n      <td>0.985</td>\n      <td>0.159299</td>\n      <td>44</td>\n      <td>4</td>\n      <td>0.409926</td>\n    </tr>\n    <tr>\n      <th>10956</th>\n      <td>boring</td>\n      <td>0.089093</td>\n      <td>0.991</td>\n      <td>-0.009177</td>\n      <td>8</td>\n      <td>77</td>\n      <td>0.356972</td>\n    </tr>\n    <tr>\n      <th>12508</th>\n      <td>check</td>\n      <td>0.091327</td>\n      <td>0.979</td>\n      <td>0.178886</td>\n      <td>367</td>\n      <td>1028</td>\n      <td>0.416404</td>\n    </tr>\n    <tr>\n      <th>11922</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.992</td>\n      <td>-0.102361</td>\n      <td>3</td>\n      <td>76</td>\n      <td>0.328442</td>\n    </tr>\n    <tr>\n      <th>18608</th>\n      <td>burthen</td>\n      <td>0.101387</td>\n      <td>0.981</td>\n      <td>0.162739</td>\n      <td>49</td>\n      <td>1</td>\n      <td>0.415042</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"lncs2\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words changed the less by LNCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n15594           ten  0.998332            0.665  0.834361      1140      1154   \n6633         eleven  0.998247            0.661  0.813450       128       214   \n17000       fifteen  0.997878            0.668  0.802010       326       350   \n2279         twenty  0.997811            0.671  0.830547       791       536   \n16533        twelve  0.997662            0.670  0.791642       503       340   \n18164            13  0.997070            0.696  0.669598        41       363   \n14366            11  0.996741            0.714  0.628893        49       526   \n16197         eight  0.996465            0.674  0.848650       524       868   \n21638            12  0.995956            0.653  0.760477        73       529   \n73      twenty-five  0.995953            0.648  0.815984       162       143   \n23040        thirty  0.995821            0.679  0.823355       466       359   \n18880          five  0.995344            0.665  0.842608      1177      2129   \n9154             24  0.995152            0.694  0.756793        31       292   \n26209            14  0.994896            0.696  0.676747        51       396   \n3449          forty  0.994742            0.673  0.825067       330       249   \n3619             20  0.994682            0.663  0.665067        58       742   \n25460           six  0.994679            0.681  0.866468       976      1495   \n14515   thirty-five  0.994590            0.672  0.708199        40        71   \n24891       sixteen  0.994375            0.655  0.794630       165       139   \n25386      fourteen  0.994299            0.666  0.780336       123       156   \n14766            10  0.994153            0.678  0.670698       104      1074   \n22000      eighteen  0.994082            0.668  0.813038       201       125   \n19948            16  0.993810            0.691  0.696257        42       352   \n4369   twenty-seven  0.993109            0.672  0.598287        27        25   \n23496         sixty  0.992328            0.671  0.810002       198       122   \n17035     seventeen  0.991974            0.688  0.782525       111        82   \n5122           nine  0.991471            0.673  0.800287       324       558   \n14373            30  0.991453            0.677  0.648498        70       673   \n25619         three  0.990792            0.719  0.850726      3089      4548   \n6888       thirteen  0.990453            0.710  0.706869       129       105   \n\n           mean  \n15594  0.832564  \n6633   0.824232  \n17000  0.822630  \n2279   0.833119  \n16533  0.819768  \n18164  0.787556  \n14366  0.779878  \n16197  0.839705  \n21638  0.803144  \n73     0.819979  \n23040  0.832725  \n18880  0.834317  \n9154   0.815315  \n26209  0.789214  \n3449   0.830936  \n3619   0.774250  \n25460  0.847382  \n14515  0.791596  \n24891  0.814668  \n25386  0.813545  \n14766  0.780950  \n22000  0.825040  \n19948  0.793689  \n4369   0.754465  \n23496  0.824443  \n17035  0.820833  \n5122   0.821586  \n14373  0.772317  \n25619  0.853506  \n6888   0.802441  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15594</th>\n      <td>ten</td>\n      <td>0.998332</td>\n      <td>0.665</td>\n      <td>0.834361</td>\n      <td>1140</td>\n      <td>1154</td>\n      <td>0.832564</td>\n    </tr>\n    <tr>\n      <th>6633</th>\n      <td>eleven</td>\n      <td>0.998247</td>\n      <td>0.661</td>\n      <td>0.813450</td>\n      <td>128</td>\n      <td>214</td>\n      <td>0.824232</td>\n    </tr>\n    <tr>\n      <th>17000</th>\n      <td>fifteen</td>\n      <td>0.997878</td>\n      <td>0.668</td>\n      <td>0.802010</td>\n      <td>326</td>\n      <td>350</td>\n      <td>0.822630</td>\n    </tr>\n    <tr>\n      <th>2279</th>\n      <td>twenty</td>\n      <td>0.997811</td>\n      <td>0.671</td>\n      <td>0.830547</td>\n      <td>791</td>\n      <td>536</td>\n      <td>0.833119</td>\n    </tr>\n    <tr>\n      <th>16533</th>\n      <td>twelve</td>\n      <td>0.997662</td>\n      <td>0.670</td>\n      <td>0.791642</td>\n      <td>503</td>\n      <td>340</td>\n      <td>0.819768</td>\n    </tr>\n    <tr>\n      <th>18164</th>\n      <td>13</td>\n      <td>0.997070</td>\n      <td>0.696</td>\n      <td>0.669598</td>\n      <td>41</td>\n      <td>363</td>\n      <td>0.787556</td>\n    </tr>\n    <tr>\n      <th>14366</th>\n      <td>11</td>\n      <td>0.996741</td>\n      <td>0.714</td>\n      <td>0.628893</td>\n      <td>49</td>\n      <td>526</td>\n      <td>0.779878</td>\n    </tr>\n    <tr>\n      <th>16197</th>\n      <td>eight</td>\n      <td>0.996465</td>\n      <td>0.674</td>\n      <td>0.848650</td>\n      <td>524</td>\n      <td>868</td>\n      <td>0.839705</td>\n    </tr>\n    <tr>\n      <th>21638</th>\n      <td>12</td>\n      <td>0.995956</td>\n      <td>0.653</td>\n      <td>0.760477</td>\n      <td>73</td>\n      <td>529</td>\n      <td>0.803144</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>twenty-five</td>\n      <td>0.995953</td>\n      <td>0.648</td>\n      <td>0.815984</td>\n      <td>162</td>\n      <td>143</td>\n      <td>0.819979</td>\n    </tr>\n    <tr>\n      <th>23040</th>\n      <td>thirty</td>\n      <td>0.995821</td>\n      <td>0.679</td>\n      <td>0.823355</td>\n      <td>466</td>\n      <td>359</td>\n      <td>0.832725</td>\n    </tr>\n    <tr>\n      <th>18880</th>\n      <td>five</td>\n      <td>0.995344</td>\n      <td>0.665</td>\n      <td>0.842608</td>\n      <td>1177</td>\n      <td>2129</td>\n      <td>0.834317</td>\n    </tr>\n    <tr>\n      <th>9154</th>\n      <td>24</td>\n      <td>0.995152</td>\n      <td>0.694</td>\n      <td>0.756793</td>\n      <td>31</td>\n      <td>292</td>\n      <td>0.815315</td>\n    </tr>\n    <tr>\n      <th>26209</th>\n      <td>14</td>\n      <td>0.994896</td>\n      <td>0.696</td>\n      <td>0.676747</td>\n      <td>51</td>\n      <td>396</td>\n      <td>0.789214</td>\n    </tr>\n    <tr>\n      <th>3449</th>\n      <td>forty</td>\n      <td>0.994742</td>\n      <td>0.673</td>\n      <td>0.825067</td>\n      <td>330</td>\n      <td>249</td>\n      <td>0.830936</td>\n    </tr>\n    <tr>\n      <th>3619</th>\n      <td>20</td>\n      <td>0.994682</td>\n      <td>0.663</td>\n      <td>0.665067</td>\n      <td>58</td>\n      <td>742</td>\n      <td>0.774250</td>\n    </tr>\n    <tr>\n      <th>25460</th>\n      <td>six</td>\n      <td>0.994679</td>\n      <td>0.681</td>\n      <td>0.866468</td>\n      <td>976</td>\n      <td>1495</td>\n      <td>0.847382</td>\n    </tr>\n    <tr>\n      <th>14515</th>\n      <td>thirty-five</td>\n      <td>0.994590</td>\n      <td>0.672</td>\n      <td>0.708199</td>\n      <td>40</td>\n      <td>71</td>\n      <td>0.791596</td>\n    </tr>\n    <tr>\n      <th>24891</th>\n      <td>sixteen</td>\n      <td>0.994375</td>\n      <td>0.655</td>\n      <td>0.794630</td>\n      <td>165</td>\n      <td>139</td>\n      <td>0.814668</td>\n    </tr>\n    <tr>\n      <th>25386</th>\n      <td>fourteen</td>\n      <td>0.994299</td>\n      <td>0.666</td>\n      <td>0.780336</td>\n      <td>123</td>\n      <td>156</td>\n      <td>0.813545</td>\n    </tr>\n    <tr>\n      <th>14766</th>\n      <td>10</td>\n      <td>0.994153</td>\n      <td>0.678</td>\n      <td>0.670698</td>\n      <td>104</td>\n      <td>1074</td>\n      <td>0.780950</td>\n    </tr>\n    <tr>\n      <th>22000</th>\n      <td>eighteen</td>\n      <td>0.994082</td>\n      <td>0.668</td>\n      <td>0.813038</td>\n      <td>201</td>\n      <td>125</td>\n      <td>0.825040</td>\n    </tr>\n    <tr>\n      <th>19948</th>\n      <td>16</td>\n      <td>0.993810</td>\n      <td>0.691</td>\n      <td>0.696257</td>\n      <td>42</td>\n      <td>352</td>\n      <td>0.793689</td>\n    </tr>\n    <tr>\n      <th>4369</th>\n      <td>twenty-seven</td>\n      <td>0.993109</td>\n      <td>0.672</td>\n      <td>0.598287</td>\n      <td>27</td>\n      <td>25</td>\n      <td>0.754465</td>\n    </tr>\n    <tr>\n      <th>23496</th>\n      <td>sixty</td>\n      <td>0.992328</td>\n      <td>0.671</td>\n      <td>0.810002</td>\n      <td>198</td>\n      <td>122</td>\n      <td>0.824443</td>\n    </tr>\n    <tr>\n      <th>17035</th>\n      <td>seventeen</td>\n      <td>0.991974</td>\n      <td>0.688</td>\n      <td>0.782525</td>\n      <td>111</td>\n      <td>82</td>\n      <td>0.820833</td>\n    </tr>\n    <tr>\n      <th>5122</th>\n      <td>nine</td>\n      <td>0.991471</td>\n      <td>0.673</td>\n      <td>0.800287</td>\n      <td>324</td>\n      <td>558</td>\n      <td>0.821586</td>\n    </tr>\n    <tr>\n      <th>14373</th>\n      <td>30</td>\n      <td>0.991453</td>\n      <td>0.677</td>\n      <td>0.648498</td>\n      <td>70</td>\n      <td>673</td>\n      <td>0.772317</td>\n    </tr>\n    <tr>\n      <th>25619</th>\n      <td>three</td>\n      <td>0.990792</td>\n      <td>0.719</td>\n      <td>0.850726</td>\n      <td>3089</td>\n      <td>4548</td>\n      <td>0.853506</td>\n    </tr>\n    <tr>\n      <th>6888</th>\n      <td>thirteen</td>\n      <td>0.990453</td>\n      <td>0.710</td>\n      <td>0.706869</td>\n      <td>129</td>\n      <td>105</td>\n      <td>0.802441</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"lncs2\"], ascending=False)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by Intersection_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n4043           virus  0.323960            1.000 -0.149479        13       166   \n21585     clumsiness  0.552998            0.999  0.532715         1         4   \n17746      rusticity  0.613301            0.999  0.544417         3         2   \n23853        funding  0.344045            0.999  0.151782         3       119   \n7452            coke  0.272922            0.999  0.071755        12        88   \n22296       uprising  0.307683            0.998  0.156027        10        39   \n17630           aura  0.217044            0.998 -0.075655         9        52   \n4266           media  0.323505            0.998  0.092881        49       394   \n25496           rove  0.151496            0.998  0.073780        48        19   \n18442          twain  0.055887            0.998  0.011805        29        31   \n6585              ml -0.029649            0.998 -0.079436         1        18   \n4671           major -0.016591            0.998  0.263108       531      1554   \n25637      projector  0.393249            0.998 -0.094298        18        33   \n9065       inclusive  0.339527            0.998  0.072305        13        21   \n25410           lacy  0.361720            0.998  0.061578         7        20   \n7053   significantly  0.058707            0.998 -0.010645        31       123   \n16344        roughly  0.084750            0.997  0.068283        50       182   \n381            teeny  0.563224            0.997  0.144121         2        10   \n20932         psyche  0.215099            0.996  0.096274         7        35   \n18791           gage  0.496162            0.996  0.145964        42        21   \n8811         spinoza  0.646836            0.996  0.591190         2         3   \n2624            vail  0.317838            0.996 -0.031890        16        34   \n19380          whiff  0.406077            0.996  0.234451        12        28   \n13000          trump  0.340584            0.996  0.167791        53        41   \n26630        clarify  0.141261            0.996  0.063697         5        50   \n21436     cavalierly  0.700440            0.996  0.305752         3         3   \n14062          xx_jj  0.570590            0.995  0.499882         3         3   \n21890        discard  0.078995            0.995  0.215280        57       109   \n12087   recuperation  0.590249            0.995  0.372132         2         5   \n6130            barb  0.414557            0.995  0.044461         9        33   \n\n           mean  \n4043   0.391494  \n21585  0.694905  \n17746  0.718906  \n23853  0.498276  \n7452   0.447892  \n22296  0.487237  \n17630  0.379796  \n4266   0.471462  \n25496  0.407758  \n18442  0.355231  \n6585   0.296305  \n4671   0.414839  \n25637  0.432317  \n9065   0.469944  \n25410  0.473766  \n7053   0.348688  \n16344  0.383344  \n381    0.568115  \n20932  0.435791  \n18791  0.546042  \n8811   0.744675  \n2624   0.427316  \n19380  0.545509  \n13000  0.501458  \n26630  0.400319  \n21436  0.667397  \n14062  0.688491  \n21890  0.429758  \n12087  0.652460  \n6130   0.484673  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4043</th>\n      <td>virus</td>\n      <td>0.323960</td>\n      <td>1.000</td>\n      <td>-0.149479</td>\n      <td>13</td>\n      <td>166</td>\n      <td>0.391494</td>\n    </tr>\n    <tr>\n      <th>21585</th>\n      <td>clumsiness</td>\n      <td>0.552998</td>\n      <td>0.999</td>\n      <td>0.532715</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0.694905</td>\n    </tr>\n    <tr>\n      <th>17746</th>\n      <td>rusticity</td>\n      <td>0.613301</td>\n      <td>0.999</td>\n      <td>0.544417</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0.718906</td>\n    </tr>\n    <tr>\n      <th>23853</th>\n      <td>funding</td>\n      <td>0.344045</td>\n      <td>0.999</td>\n      <td>0.151782</td>\n      <td>3</td>\n      <td>119</td>\n      <td>0.498276</td>\n    </tr>\n    <tr>\n      <th>7452</th>\n      <td>coke</td>\n      <td>0.272922</td>\n      <td>0.999</td>\n      <td>0.071755</td>\n      <td>12</td>\n      <td>88</td>\n      <td>0.447892</td>\n    </tr>\n    <tr>\n      <th>22296</th>\n      <td>uprising</td>\n      <td>0.307683</td>\n      <td>0.998</td>\n      <td>0.156027</td>\n      <td>10</td>\n      <td>39</td>\n      <td>0.487237</td>\n    </tr>\n    <tr>\n      <th>17630</th>\n      <td>aura</td>\n      <td>0.217044</td>\n      <td>0.998</td>\n      <td>-0.075655</td>\n      <td>9</td>\n      <td>52</td>\n      <td>0.379796</td>\n    </tr>\n    <tr>\n      <th>4266</th>\n      <td>media</td>\n      <td>0.323505</td>\n      <td>0.998</td>\n      <td>0.092881</td>\n      <td>49</td>\n      <td>394</td>\n      <td>0.471462</td>\n    </tr>\n    <tr>\n      <th>25496</th>\n      <td>rove</td>\n      <td>0.151496</td>\n      <td>0.998</td>\n      <td>0.073780</td>\n      <td>48</td>\n      <td>19</td>\n      <td>0.407758</td>\n    </tr>\n    <tr>\n      <th>18442</th>\n      <td>twain</td>\n      <td>0.055887</td>\n      <td>0.998</td>\n      <td>0.011805</td>\n      <td>29</td>\n      <td>31</td>\n      <td>0.355231</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>4671</th>\n      <td>major</td>\n      <td>-0.016591</td>\n      <td>0.998</td>\n      <td>0.263108</td>\n      <td>531</td>\n      <td>1554</td>\n      <td>0.414839</td>\n    </tr>\n    <tr>\n      <th>25637</th>\n      <td>projector</td>\n      <td>0.393249</td>\n      <td>0.998</td>\n      <td>-0.094298</td>\n      <td>18</td>\n      <td>33</td>\n      <td>0.432317</td>\n    </tr>\n    <tr>\n      <th>9065</th>\n      <td>inclusive</td>\n      <td>0.339527</td>\n      <td>0.998</td>\n      <td>0.072305</td>\n      <td>13</td>\n      <td>21</td>\n      <td>0.469944</td>\n    </tr>\n    <tr>\n      <th>25410</th>\n      <td>lacy</td>\n      <td>0.361720</td>\n      <td>0.998</td>\n      <td>0.061578</td>\n      <td>7</td>\n      <td>20</td>\n      <td>0.473766</td>\n    </tr>\n    <tr>\n      <th>7053</th>\n      <td>significantly</td>\n      <td>0.058707</td>\n      <td>0.998</td>\n      <td>-0.010645</td>\n      <td>31</td>\n      <td>123</td>\n      <td>0.348688</td>\n    </tr>\n    <tr>\n      <th>16344</th>\n      <td>roughly</td>\n      <td>0.084750</td>\n      <td>0.997</td>\n      <td>0.068283</td>\n      <td>50</td>\n      <td>182</td>\n      <td>0.383344</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>teeny</td>\n      <td>0.563224</td>\n      <td>0.997</td>\n      <td>0.144121</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0.568115</td>\n    </tr>\n    <tr>\n      <th>20932</th>\n      <td>psyche</td>\n      <td>0.215099</td>\n      <td>0.996</td>\n      <td>0.096274</td>\n      <td>7</td>\n      <td>35</td>\n      <td>0.435791</td>\n    </tr>\n    <tr>\n      <th>18791</th>\n      <td>gage</td>\n      <td>0.496162</td>\n      <td>0.996</td>\n      <td>0.145964</td>\n      <td>42</td>\n      <td>21</td>\n      <td>0.546042</td>\n    </tr>\n    <tr>\n      <th>8811</th>\n      <td>spinoza</td>\n      <td>0.646836</td>\n      <td>0.996</td>\n      <td>0.591190</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.744675</td>\n    </tr>\n    <tr>\n      <th>2624</th>\n      <td>vail</td>\n      <td>0.317838</td>\n      <td>0.996</td>\n      <td>-0.031890</td>\n      <td>16</td>\n      <td>34</td>\n      <td>0.427316</td>\n    </tr>\n    <tr>\n      <th>19380</th>\n      <td>whiff</td>\n      <td>0.406077</td>\n      <td>0.996</td>\n      <td>0.234451</td>\n      <td>12</td>\n      <td>28</td>\n      <td>0.545509</td>\n    </tr>\n    <tr>\n      <th>13000</th>\n      <td>trump</td>\n      <td>0.340584</td>\n      <td>0.996</td>\n      <td>0.167791</td>\n      <td>53</td>\n      <td>41</td>\n      <td>0.501458</td>\n    </tr>\n    <tr>\n      <th>26630</th>\n      <td>clarify</td>\n      <td>0.141261</td>\n      <td>0.996</td>\n      <td>0.063697</td>\n      <td>5</td>\n      <td>50</td>\n      <td>0.400319</td>\n    </tr>\n    <tr>\n      <th>21436</th>\n      <td>cavalierly</td>\n      <td>0.700440</td>\n      <td>0.996</td>\n      <td>0.305752</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.667397</td>\n    </tr>\n    <tr>\n      <th>14062</th>\n      <td>xx_jj</td>\n      <td>0.570590</td>\n      <td>0.995</td>\n      <td>0.499882</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.688491</td>\n    </tr>\n    <tr>\n      <th>21890</th>\n      <td>discard</td>\n      <td>0.078995</td>\n      <td>0.995</td>\n      <td>0.215280</td>\n      <td>57</td>\n      <td>109</td>\n      <td>0.429758</td>\n    </tr>\n    <tr>\n      <th>12087</th>\n      <td>recuperation</td>\n      <td>0.590249</td>\n      <td>0.995</td>\n      <td>0.372132</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0.652460</td>\n    </tr>\n    <tr>\n      <th>6130</th>\n      <td>barb</td>\n      <td>0.414557</td>\n      <td>0.995</td>\n      <td>0.044461</td>\n      <td>9</td>\n      <td>33</td>\n      <td>0.484673</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"intersection_nn\"], ascending=False)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words changed the less by Intersection_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n11083    convince  0.945782            0.514  0.781441       476       450   \n6945      believe  0.971778            0.522  0.758603      2948      2618   \n1709        swing  0.971353            0.538  0.728393       166       626   \n148          deny  0.964949            0.546  0.691601       682       414   \n7501   understand  0.974703            0.558  0.727781      1475      1683   \n9286         find  0.928129            0.558  0.849056      7549      6846   \n617         grass  0.945075            0.560  0.797416       378       483   \n4771        slope  0.930147            0.566  0.855519       137       200   \n2186        trust  0.957087            0.566  0.670041      1050       562   \n15394       trunk  0.952310            0.568  0.776542       322       180   \n429       tree_nn  0.973066            0.569  0.871842      2322      1596   \n16268         leg  0.964123            0.569  0.840654       376      1191   \n16234        hang  0.958704            0.570  0.809736      1049      1189   \n26917        hair  0.912894            0.570  0.867503       974      1970   \n21156        leaf  0.932912            0.571  0.840396       773       477   \n21794    shoulder  0.972574            0.573  0.826431       677      1237   \n22643     satisfy  0.869777            0.573  0.681398       521       200   \n1423        fence  0.921338            0.575  0.829482       179       260   \n10701        rope  0.946561            0.575  0.830084       175       276   \n18862      assure  0.933151            0.575  0.711387       611       345   \n126        carpet  0.924440            0.575  0.777948       126       174   \n13307      collar  0.949337            0.575  0.783521       112       213   \n25176        pull  0.972609            0.576  0.750570       379      1989   \n10274       prove  0.958826            0.577  0.727998      1436       825   \n13489         dry  0.924254            0.578  0.805396       522       691   \n5782        bring  0.983159            0.579  0.865412      3918      3332   \n26842        neck  0.945422            0.579  0.832751       564       714   \n23704        thin  0.927979            0.579  0.801362       407       586   \n5043         hear  0.958789            0.580  0.851877      4691      4058   \n14119     boiling  0.913699            0.580  0.852232        53        48   \n\n           mean  \n11083  0.747074  \n6945   0.750794  \n1709   0.745915  \n148    0.734183  \n7501   0.753495  \n9286   0.778395  \n617    0.767497  \n4771   0.783888  \n2186   0.731043  \n15394  0.765617  \n429    0.804636  \n16268  0.791259  \n16234  0.779480  \n26917  0.783466  \n21156  0.781436  \n21794  0.790668  \n22643  0.708058  \n1423   0.775273  \n10701  0.783882  \n18862  0.739846  \n126    0.759129  \n13307  0.769286  \n25176  0.766393  \n10274  0.754608  \n13489  0.769217  \n5782   0.809190  \n26842  0.785724  \n23704  0.769447  \n5043   0.796889  \n14119  0.781977  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11083</th>\n      <td>convince</td>\n      <td>0.945782</td>\n      <td>0.514</td>\n      <td>0.781441</td>\n      <td>476</td>\n      <td>450</td>\n      <td>0.747074</td>\n    </tr>\n    <tr>\n      <th>6945</th>\n      <td>believe</td>\n      <td>0.971778</td>\n      <td>0.522</td>\n      <td>0.758603</td>\n      <td>2948</td>\n      <td>2618</td>\n      <td>0.750794</td>\n    </tr>\n    <tr>\n      <th>1709</th>\n      <td>swing</td>\n      <td>0.971353</td>\n      <td>0.538</td>\n      <td>0.728393</td>\n      <td>166</td>\n      <td>626</td>\n      <td>0.745915</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>deny</td>\n      <td>0.964949</td>\n      <td>0.546</td>\n      <td>0.691601</td>\n      <td>682</td>\n      <td>414</td>\n      <td>0.734183</td>\n    </tr>\n    <tr>\n      <th>7501</th>\n      <td>understand</td>\n      <td>0.974703</td>\n      <td>0.558</td>\n      <td>0.727781</td>\n      <td>1475</td>\n      <td>1683</td>\n      <td>0.753495</td>\n    </tr>\n    <tr>\n      <th>9286</th>\n      <td>find</td>\n      <td>0.928129</td>\n      <td>0.558</td>\n      <td>0.849056</td>\n      <td>7549</td>\n      <td>6846</td>\n      <td>0.778395</td>\n    </tr>\n    <tr>\n      <th>617</th>\n      <td>grass</td>\n      <td>0.945075</td>\n      <td>0.560</td>\n      <td>0.797416</td>\n      <td>378</td>\n      <td>483</td>\n      <td>0.767497</td>\n    </tr>\n    <tr>\n      <th>4771</th>\n      <td>slope</td>\n      <td>0.930147</td>\n      <td>0.566</td>\n      <td>0.855519</td>\n      <td>137</td>\n      <td>200</td>\n      <td>0.783888</td>\n    </tr>\n    <tr>\n      <th>2186</th>\n      <td>trust</td>\n      <td>0.957087</td>\n      <td>0.566</td>\n      <td>0.670041</td>\n      <td>1050</td>\n      <td>562</td>\n      <td>0.731043</td>\n    </tr>\n    <tr>\n      <th>15394</th>\n      <td>trunk</td>\n      <td>0.952310</td>\n      <td>0.568</td>\n      <td>0.776542</td>\n      <td>322</td>\n      <td>180</td>\n      <td>0.765617</td>\n    </tr>\n    <tr>\n      <th>429</th>\n      <td>tree_nn</td>\n      <td>0.973066</td>\n      <td>0.569</td>\n      <td>0.871842</td>\n      <td>2322</td>\n      <td>1596</td>\n      <td>0.804636</td>\n    </tr>\n    <tr>\n      <th>16268</th>\n      <td>leg</td>\n      <td>0.964123</td>\n      <td>0.569</td>\n      <td>0.840654</td>\n      <td>376</td>\n      <td>1191</td>\n      <td>0.791259</td>\n    </tr>\n    <tr>\n      <th>16234</th>\n      <td>hang</td>\n      <td>0.958704</td>\n      <td>0.570</td>\n      <td>0.809736</td>\n      <td>1049</td>\n      <td>1189</td>\n      <td>0.779480</td>\n    </tr>\n    <tr>\n      <th>26917</th>\n      <td>hair</td>\n      <td>0.912894</td>\n      <td>0.570</td>\n      <td>0.867503</td>\n      <td>974</td>\n      <td>1970</td>\n      <td>0.783466</td>\n    </tr>\n    <tr>\n      <th>21156</th>\n      <td>leaf</td>\n      <td>0.932912</td>\n      <td>0.571</td>\n      <td>0.840396</td>\n      <td>773</td>\n      <td>477</td>\n      <td>0.781436</td>\n    </tr>\n    <tr>\n      <th>21794</th>\n      <td>shoulder</td>\n      <td>0.972574</td>\n      <td>0.573</td>\n      <td>0.826431</td>\n      <td>677</td>\n      <td>1237</td>\n      <td>0.790668</td>\n    </tr>\n    <tr>\n      <th>22643</th>\n      <td>satisfy</td>\n      <td>0.869777</td>\n      <td>0.573</td>\n      <td>0.681398</td>\n      <td>521</td>\n      <td>200</td>\n      <td>0.708058</td>\n    </tr>\n    <tr>\n      <th>1423</th>\n      <td>fence</td>\n      <td>0.921338</td>\n      <td>0.575</td>\n      <td>0.829482</td>\n      <td>179</td>\n      <td>260</td>\n      <td>0.775273</td>\n    </tr>\n    <tr>\n      <th>10701</th>\n      <td>rope</td>\n      <td>0.946561</td>\n      <td>0.575</td>\n      <td>0.830084</td>\n      <td>175</td>\n      <td>276</td>\n      <td>0.783882</td>\n    </tr>\n    <tr>\n      <th>18862</th>\n      <td>assure</td>\n      <td>0.933151</td>\n      <td>0.575</td>\n      <td>0.711387</td>\n      <td>611</td>\n      <td>345</td>\n      <td>0.739846</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>carpet</td>\n      <td>0.924440</td>\n      <td>0.575</td>\n      <td>0.777948</td>\n      <td>126</td>\n      <td>174</td>\n      <td>0.759129</td>\n    </tr>\n    <tr>\n      <th>13307</th>\n      <td>collar</td>\n      <td>0.949337</td>\n      <td>0.575</td>\n      <td>0.783521</td>\n      <td>112</td>\n      <td>213</td>\n      <td>0.769286</td>\n    </tr>\n    <tr>\n      <th>25176</th>\n      <td>pull</td>\n      <td>0.972609</td>\n      <td>0.576</td>\n      <td>0.750570</td>\n      <td>379</td>\n      <td>1989</td>\n      <td>0.766393</td>\n    </tr>\n    <tr>\n      <th>10274</th>\n      <td>prove</td>\n      <td>0.958826</td>\n      <td>0.577</td>\n      <td>0.727998</td>\n      <td>1436</td>\n      <td>825</td>\n      <td>0.754608</td>\n    </tr>\n    <tr>\n      <th>13489</th>\n      <td>dry</td>\n      <td>0.924254</td>\n      <td>0.578</td>\n      <td>0.805396</td>\n      <td>522</td>\n      <td>691</td>\n      <td>0.769217</td>\n    </tr>\n    <tr>\n      <th>5782</th>\n      <td>bring</td>\n      <td>0.983159</td>\n      <td>0.579</td>\n      <td>0.865412</td>\n      <td>3918</td>\n      <td>3332</td>\n      <td>0.809190</td>\n    </tr>\n    <tr>\n      <th>26842</th>\n      <td>neck</td>\n      <td>0.945422</td>\n      <td>0.579</td>\n      <td>0.832751</td>\n      <td>564</td>\n      <td>714</td>\n      <td>0.785724</td>\n    </tr>\n    <tr>\n      <th>23704</th>\n      <td>thin</td>\n      <td>0.927979</td>\n      <td>0.579</td>\n      <td>0.801362</td>\n      <td>407</td>\n      <td>586</td>\n      <td>0.769447</td>\n    </tr>\n    <tr>\n      <th>5043</th>\n      <td>hear</td>\n      <td>0.958789</td>\n      <td>0.580</td>\n      <td>0.851877</td>\n      <td>4691</td>\n      <td>4058</td>\n      <td>0.796889</td>\n    </tr>\n    <tr>\n      <th>14119</th>\n      <td>boiling</td>\n      <td>0.913699</td>\n      <td>0.580</td>\n      <td>0.852232</td>\n      <td>53</td>\n      <td>48</td>\n      <td>0.781977</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"intersection_nn\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n16205           ski  0.287075            0.983 -0.166819         2       218   \n4043          virus  0.323960            1.000 -0.149479        13       166   \n7259            fer  0.259189            0.966 -0.132545         4        47   \n18503            op  0.695739            0.974 -0.127812        21        31   \n17938        wilmer  0.500615            0.950 -0.111061        11        16   \n17195        some--  0.068811            0.964 -0.110117         2         6   \n1684        setting  0.200999            0.991 -0.109949         1        62   \n11922   frightening  0.095688            0.992 -0.102361         3        76   \n15049          gist  0.272670            0.994 -0.098656        15        15   \n17813           cal  0.479762            0.980 -0.098061        13        66   \n5256     intriguing  0.198948            0.970 -0.095689        13        43   \n25637     projector  0.393249            0.998 -0.094298        18        33   \n23774           int  0.534314            0.963 -0.084740         5        77   \n449             cir  0.372242            0.981 -0.082652        10         1   \n5775         scurvy  0.292371            0.986 -0.081057         5         6   \n8812          flume  0.600722            0.972 -0.080179         6         1   \n6585             ml -0.029649            0.998 -0.079436         1        18   \n2193             uc  0.517708            0.985 -0.077761         1        15   \n17630          aura  0.217044            0.998 -0.075655         9        52   \n17288         ethel  0.413615            0.974 -0.075074         1        40   \n24935        unused -0.059081            0.987 -0.073233        26        36   \n9359          lotus  0.677376            0.971 -0.069215         2        22   \n4905        onwards  0.414718            0.968 -0.068938        11         9   \n20737        walden  0.373120            0.990 -0.068412        12         8   \n8234   ridiculously  0.558388            0.979 -0.065712         7        14   \n14593       adeline  0.466286            0.965 -0.064648        32         1   \n2159          humus  0.268067            0.969 -0.064053         1        10   \n2587            doc  0.439268            0.975 -0.062904         5       107   \n21108         elias  0.615312            0.960 -0.059430        13        37   \n24062  fruitfulness  0.413633            0.965 -0.057541        13         2   \n\n           mean  \n16205  0.367752  \n4043   0.391494  \n7259   0.364215  \n18503  0.513976  \n17938  0.446518  \n17195  0.307565  \n1684   0.360683  \n11922  0.328442  \n15049  0.389338  \n17813  0.453900  \n5256   0.357753  \n25637  0.432317  \n23774  0.470858  \n449    0.423530  \n5775   0.399105  \n8812   0.497514  \n6585   0.296305  \n2193   0.474982  \n17630  0.379796  \n17288  0.437514  \n24935  0.284895  \n9359   0.526387  \n4905   0.437927  \n20737  0.431570  \n8234   0.490559  \n14593  0.455546  \n2159   0.391005  \n2587   0.450455  \n21108  0.505294  \n24062  0.440364  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16205</th>\n      <td>ski</td>\n      <td>0.287075</td>\n      <td>0.983</td>\n      <td>-0.166819</td>\n      <td>2</td>\n      <td>218</td>\n      <td>0.367752</td>\n    </tr>\n    <tr>\n      <th>4043</th>\n      <td>virus</td>\n      <td>0.323960</td>\n      <td>1.000</td>\n      <td>-0.149479</td>\n      <td>13</td>\n      <td>166</td>\n      <td>0.391494</td>\n    </tr>\n    <tr>\n      <th>7259</th>\n      <td>fer</td>\n      <td>0.259189</td>\n      <td>0.966</td>\n      <td>-0.132545</td>\n      <td>4</td>\n      <td>47</td>\n      <td>0.364215</td>\n    </tr>\n    <tr>\n      <th>18503</th>\n      <td>op</td>\n      <td>0.695739</td>\n      <td>0.974</td>\n      <td>-0.127812</td>\n      <td>21</td>\n      <td>31</td>\n      <td>0.513976</td>\n    </tr>\n    <tr>\n      <th>17938</th>\n      <td>wilmer</td>\n      <td>0.500615</td>\n      <td>0.950</td>\n      <td>-0.111061</td>\n      <td>11</td>\n      <td>16</td>\n      <td>0.446518</td>\n    </tr>\n    <tr>\n      <th>17195</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.964</td>\n      <td>-0.110117</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.307565</td>\n    </tr>\n    <tr>\n      <th>1684</th>\n      <td>setting</td>\n      <td>0.200999</td>\n      <td>0.991</td>\n      <td>-0.109949</td>\n      <td>1</td>\n      <td>62</td>\n      <td>0.360683</td>\n    </tr>\n    <tr>\n      <th>11922</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.992</td>\n      <td>-0.102361</td>\n      <td>3</td>\n      <td>76</td>\n      <td>0.328442</td>\n    </tr>\n    <tr>\n      <th>15049</th>\n      <td>gist</td>\n      <td>0.272670</td>\n      <td>0.994</td>\n      <td>-0.098656</td>\n      <td>15</td>\n      <td>15</td>\n      <td>0.389338</td>\n    </tr>\n    <tr>\n      <th>17813</th>\n      <td>cal</td>\n      <td>0.479762</td>\n      <td>0.980</td>\n      <td>-0.098061</td>\n      <td>13</td>\n      <td>66</td>\n      <td>0.453900</td>\n    </tr>\n    <tr>\n      <th>5256</th>\n      <td>intriguing</td>\n      <td>0.198948</td>\n      <td>0.970</td>\n      <td>-0.095689</td>\n      <td>13</td>\n      <td>43</td>\n      <td>0.357753</td>\n    </tr>\n    <tr>\n      <th>25637</th>\n      <td>projector</td>\n      <td>0.393249</td>\n      <td>0.998</td>\n      <td>-0.094298</td>\n      <td>18</td>\n      <td>33</td>\n      <td>0.432317</td>\n    </tr>\n    <tr>\n      <th>23774</th>\n      <td>int</td>\n      <td>0.534314</td>\n      <td>0.963</td>\n      <td>-0.084740</td>\n      <td>5</td>\n      <td>77</td>\n      <td>0.470858</td>\n    </tr>\n    <tr>\n      <th>449</th>\n      <td>cir</td>\n      <td>0.372242</td>\n      <td>0.981</td>\n      <td>-0.082652</td>\n      <td>10</td>\n      <td>1</td>\n      <td>0.423530</td>\n    </tr>\n    <tr>\n      <th>5775</th>\n      <td>scurvy</td>\n      <td>0.292371</td>\n      <td>0.986</td>\n      <td>-0.081057</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0.399105</td>\n    </tr>\n    <tr>\n      <th>8812</th>\n      <td>flume</td>\n      <td>0.600722</td>\n      <td>0.972</td>\n      <td>-0.080179</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.497514</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>2193</th>\n      <td>uc</td>\n      <td>0.517708</td>\n      <td>0.985</td>\n      <td>-0.077761</td>\n      <td>1</td>\n      <td>15</td>\n      <td>0.474982</td>\n    </tr>\n    <tr>\n      <th>17630</th>\n      <td>aura</td>\n      <td>0.217044</td>\n      <td>0.998</td>\n      <td>-0.075655</td>\n      <td>9</td>\n      <td>52</td>\n      <td>0.379796</td>\n    </tr>\n    <tr>\n      <th>17288</th>\n      <td>ethel</td>\n      <td>0.413615</td>\n      <td>0.974</td>\n      <td>-0.075074</td>\n      <td>1</td>\n      <td>40</td>\n      <td>0.437514</td>\n    </tr>\n    <tr>\n      <th>24935</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.987</td>\n      <td>-0.073233</td>\n      <td>26</td>\n      <td>36</td>\n      <td>0.284895</td>\n    </tr>\n    <tr>\n      <th>9359</th>\n      <td>lotus</td>\n      <td>0.677376</td>\n      <td>0.971</td>\n      <td>-0.069215</td>\n      <td>2</td>\n      <td>22</td>\n      <td>0.526387</td>\n    </tr>\n    <tr>\n      <th>4905</th>\n      <td>onwards</td>\n      <td>0.414718</td>\n      <td>0.968</td>\n      <td>-0.068938</td>\n      <td>11</td>\n      <td>9</td>\n      <td>0.437927</td>\n    </tr>\n    <tr>\n      <th>20737</th>\n      <td>walden</td>\n      <td>0.373120</td>\n      <td>0.990</td>\n      <td>-0.068412</td>\n      <td>12</td>\n      <td>8</td>\n      <td>0.431570</td>\n    </tr>\n    <tr>\n      <th>8234</th>\n      <td>ridiculously</td>\n      <td>0.558388</td>\n      <td>0.979</td>\n      <td>-0.065712</td>\n      <td>7</td>\n      <td>14</td>\n      <td>0.490559</td>\n    </tr>\n    <tr>\n      <th>14593</th>\n      <td>adeline</td>\n      <td>0.466286</td>\n      <td>0.965</td>\n      <td>-0.064648</td>\n      <td>32</td>\n      <td>1</td>\n      <td>0.455546</td>\n    </tr>\n    <tr>\n      <th>2159</th>\n      <td>humus</td>\n      <td>0.268067</td>\n      <td>0.969</td>\n      <td>-0.064053</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.391005</td>\n    </tr>\n    <tr>\n      <th>2587</th>\n      <td>doc</td>\n      <td>0.439268</td>\n      <td>0.975</td>\n      <td>-0.062904</td>\n      <td>5</td>\n      <td>107</td>\n      <td>0.450455</td>\n    </tr>\n    <tr>\n      <th>21108</th>\n      <td>elias</td>\n      <td>0.615312</td>\n      <td>0.960</td>\n      <td>-0.059430</td>\n      <td>13</td>\n      <td>37</td>\n      <td>0.505294</td>\n    </tr>\n    <tr>\n      <th>24062</th>\n      <td>fruitfulness</td>\n      <td>0.413633</td>\n      <td>0.965</td>\n      <td>-0.057541</td>\n      <td>13</td>\n      <td>2</td>\n      <td>0.440364</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"cosine\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words changed the less by cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n25870       an  0.458082            0.893  0.924624     20796     22248   \n1550       who  0.809027            0.872  0.919161     17530     16983   \n4443      than  0.405342            0.877  0.908293     12372     10985   \n11142      new  0.464222            0.868  0.903266      5012     10031   \n18502     more  0.790212            0.851  0.901612     14909     14446   \n24375      far  0.791386            0.812  0.900840      5100      3903   \n719       whom  0.829928            0.782  0.900759      3449       918   \n6656      from  0.577680            0.898  0.900104     32523     29181   \n9741       eye  0.942752            0.670  0.898691      6165      4951   \n18241    there  0.622997            0.834  0.898492     16028     17609   \n24444     foot  0.961278            0.618  0.895145      2843      2650   \n566       mile  0.853808            0.699  0.892059      1313      1390   \n2798   evening  0.969631            0.759  0.891401      1271       957   \n1206      side  0.945507            0.627  0.891220      3347      3271   \n11185  face_nn  0.934642            0.645  0.891037      3394      3932   \n14998      sun  0.948069            0.635  0.890343      1438      1125   \n10684  between  0.559891            0.870  0.889165      3235      4114   \n21205       as  0.695247            0.868  0.888889     55498     45094   \n21344     into  0.931223            0.831  0.887388     11346     13789   \n23569     door  0.922105            0.683  0.887247      2153      3406   \n771       give  0.935255            0.607  0.885133      9486      7412   \n10627  through  0.887455            0.778  0.884283      5494      6906   \n23856     inch  0.839905            0.714  0.882523       499       634   \n21330    water  0.971430            0.603  0.882270      2720      2924   \n1897        on  0.781084            0.843  0.880777     33481     47478   \n25919     long  0.750341            0.829  0.879172      6873      6531   \n23906  o'clock  0.814487            0.820  0.878836       288       242   \n10874    along  0.910810            0.724  0.878565      1738      2484   \n22216    least  0.718551            0.791  0.878515      1998      2249   \n15917     book  0.948878            0.729  0.878083      1866      2530   \n\n           mean  \n25870  0.758568  \n1550   0.866729  \n4443   0.730212  \n11142  0.745163  \n18502  0.847608  \n24375  0.834742  \n719    0.837562  \n6656   0.791928  \n9741   0.837148  \n18241  0.785163  \n24444  0.824808  \n566    0.814956  \n2798   0.873344  \n1206   0.821242  \n11185  0.823560  \n14998  0.824470  \n10684  0.773019  \n21205  0.817379  \n21344  0.883204  \n23569  0.830784  \n771    0.809129  \n10627  0.849913  \n23856  0.812143  \n21330  0.818900  \n1897   0.834954  \n25919  0.819504  \n23906  0.837774  \n10874  0.837792  \n22216  0.796022  \n15917  0.851987  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>25870</th>\n      <td>an</td>\n      <td>0.458082</td>\n      <td>0.893</td>\n      <td>0.924624</td>\n      <td>20796</td>\n      <td>22248</td>\n      <td>0.758568</td>\n    </tr>\n    <tr>\n      <th>1550</th>\n      <td>who</td>\n      <td>0.809027</td>\n      <td>0.872</td>\n      <td>0.919161</td>\n      <td>17530</td>\n      <td>16983</td>\n      <td>0.866729</td>\n    </tr>\n    <tr>\n      <th>4443</th>\n      <td>than</td>\n      <td>0.405342</td>\n      <td>0.877</td>\n      <td>0.908293</td>\n      <td>12372</td>\n      <td>10985</td>\n      <td>0.730212</td>\n    </tr>\n    <tr>\n      <th>11142</th>\n      <td>new</td>\n      <td>0.464222</td>\n      <td>0.868</td>\n      <td>0.903266</td>\n      <td>5012</td>\n      <td>10031</td>\n      <td>0.745163</td>\n    </tr>\n    <tr>\n      <th>18502</th>\n      <td>more</td>\n      <td>0.790212</td>\n      <td>0.851</td>\n      <td>0.901612</td>\n      <td>14909</td>\n      <td>14446</td>\n      <td>0.847608</td>\n    </tr>\n    <tr>\n      <th>24375</th>\n      <td>far</td>\n      <td>0.791386</td>\n      <td>0.812</td>\n      <td>0.900840</td>\n      <td>5100</td>\n      <td>3903</td>\n      <td>0.834742</td>\n    </tr>\n    <tr>\n      <th>719</th>\n      <td>whom</td>\n      <td>0.829928</td>\n      <td>0.782</td>\n      <td>0.900759</td>\n      <td>3449</td>\n      <td>918</td>\n      <td>0.837562</td>\n    </tr>\n    <tr>\n      <th>6656</th>\n      <td>from</td>\n      <td>0.577680</td>\n      <td>0.898</td>\n      <td>0.900104</td>\n      <td>32523</td>\n      <td>29181</td>\n      <td>0.791928</td>\n    </tr>\n    <tr>\n      <th>9741</th>\n      <td>eye</td>\n      <td>0.942752</td>\n      <td>0.670</td>\n      <td>0.898691</td>\n      <td>6165</td>\n      <td>4951</td>\n      <td>0.837148</td>\n    </tr>\n    <tr>\n      <th>18241</th>\n      <td>there</td>\n      <td>0.622997</td>\n      <td>0.834</td>\n      <td>0.898492</td>\n      <td>16028</td>\n      <td>17609</td>\n      <td>0.785163</td>\n    </tr>\n    <tr>\n      <th>24444</th>\n      <td>foot</td>\n      <td>0.961278</td>\n      <td>0.618</td>\n      <td>0.895145</td>\n      <td>2843</td>\n      <td>2650</td>\n      <td>0.824808</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>mile</td>\n      <td>0.853808</td>\n      <td>0.699</td>\n      <td>0.892059</td>\n      <td>1313</td>\n      <td>1390</td>\n      <td>0.814956</td>\n    </tr>\n    <tr>\n      <th>2798</th>\n      <td>evening</td>\n      <td>0.969631</td>\n      <td>0.759</td>\n      <td>0.891401</td>\n      <td>1271</td>\n      <td>957</td>\n      <td>0.873344</td>\n    </tr>\n    <tr>\n      <th>1206</th>\n      <td>side</td>\n      <td>0.945507</td>\n      <td>0.627</td>\n      <td>0.891220</td>\n      <td>3347</td>\n      <td>3271</td>\n      <td>0.821242</td>\n    </tr>\n    <tr>\n      <th>11185</th>\n      <td>face_nn</td>\n      <td>0.934642</td>\n      <td>0.645</td>\n      <td>0.891037</td>\n      <td>3394</td>\n      <td>3932</td>\n      <td>0.823560</td>\n    </tr>\n    <tr>\n      <th>14998</th>\n      <td>sun</td>\n      <td>0.948069</td>\n      <td>0.635</td>\n      <td>0.890343</td>\n      <td>1438</td>\n      <td>1125</td>\n      <td>0.824470</td>\n    </tr>\n    <tr>\n      <th>10684</th>\n      <td>between</td>\n      <td>0.559891</td>\n      <td>0.870</td>\n      <td>0.889165</td>\n      <td>3235</td>\n      <td>4114</td>\n      <td>0.773019</td>\n    </tr>\n    <tr>\n      <th>21205</th>\n      <td>as</td>\n      <td>0.695247</td>\n      <td>0.868</td>\n      <td>0.888889</td>\n      <td>55498</td>\n      <td>45094</td>\n      <td>0.817379</td>\n    </tr>\n    <tr>\n      <th>21344</th>\n      <td>into</td>\n      <td>0.931223</td>\n      <td>0.831</td>\n      <td>0.887388</td>\n      <td>11346</td>\n      <td>13789</td>\n      <td>0.883204</td>\n    </tr>\n    <tr>\n      <th>23569</th>\n      <td>door</td>\n      <td>0.922105</td>\n      <td>0.683</td>\n      <td>0.887247</td>\n      <td>2153</td>\n      <td>3406</td>\n      <td>0.830784</td>\n    </tr>\n    <tr>\n      <th>771</th>\n      <td>give</td>\n      <td>0.935255</td>\n      <td>0.607</td>\n      <td>0.885133</td>\n      <td>9486</td>\n      <td>7412</td>\n      <td>0.809129</td>\n    </tr>\n    <tr>\n      <th>10627</th>\n      <td>through</td>\n      <td>0.887455</td>\n      <td>0.778</td>\n      <td>0.884283</td>\n      <td>5494</td>\n      <td>6906</td>\n      <td>0.849913</td>\n    </tr>\n    <tr>\n      <th>23856</th>\n      <td>inch</td>\n      <td>0.839905</td>\n      <td>0.714</td>\n      <td>0.882523</td>\n      <td>499</td>\n      <td>634</td>\n      <td>0.812143</td>\n    </tr>\n    <tr>\n      <th>21330</th>\n      <td>water</td>\n      <td>0.971430</td>\n      <td>0.603</td>\n      <td>0.882270</td>\n      <td>2720</td>\n      <td>2924</td>\n      <td>0.818900</td>\n    </tr>\n    <tr>\n      <th>1897</th>\n      <td>on</td>\n      <td>0.781084</td>\n      <td>0.843</td>\n      <td>0.880777</td>\n      <td>33481</td>\n      <td>47478</td>\n      <td>0.834954</td>\n    </tr>\n    <tr>\n      <th>25919</th>\n      <td>long</td>\n      <td>0.750341</td>\n      <td>0.829</td>\n      <td>0.879172</td>\n      <td>6873</td>\n      <td>6531</td>\n      <td>0.819504</td>\n    </tr>\n    <tr>\n      <th>23906</th>\n      <td>o'clock</td>\n      <td>0.814487</td>\n      <td>0.820</td>\n      <td>0.878836</td>\n      <td>288</td>\n      <td>242</td>\n      <td>0.837774</td>\n    </tr>\n    <tr>\n      <th>10874</th>\n      <td>along</td>\n      <td>0.910810</td>\n      <td>0.724</td>\n      <td>0.878565</td>\n      <td>1738</td>\n      <td>2484</td>\n      <td>0.837792</td>\n    </tr>\n    <tr>\n      <th>22216</th>\n      <td>least</td>\n      <td>0.718551</td>\n      <td>0.791</td>\n      <td>0.878515</td>\n      <td>1998</td>\n      <td>2249</td>\n      <td>0.796022</td>\n    </tr>\n    <tr>\n      <th>15917</th>\n      <td>book</td>\n      <td>0.948878</td>\n      <td>0.729</td>\n      <td>0.878083</td>\n      <td>1866</td>\n      <td>2530</td>\n      <td>0.851987</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"cosine\"], ascending=False)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594667553763",
   "display_name": "Python 3.7.7 64-bit ('thesis': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}