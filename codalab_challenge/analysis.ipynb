{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from cade.metrics.comparative import lncs2, intersection_nn, initialize_avgs, get_neighbors_set\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from pandas import pandas\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "from tabulate import tabulate\n",
    "from config import CURRENT_EXP_DIR, config, get_logger, log_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load language models and groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(lang: str):\n",
    "    model1 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus1.model\"\n",
    "    )\n",
    "    model2 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus2.model\"\n",
    "    )\n",
    "    return model1, model2\n",
    "\n",
    "def get_gt(lang: str, binary=True):\n",
    "    binary_truth = numpy.loadtxt(\n",
    "        \"./data/\"\n",
    "        + lang\n",
    "        + \"/semeval2020_ulscd_\"\n",
    "        + lang[:3]\n",
    "        + \"/truth/\" + (\"binary\" if binary else \"graded\") + \".txt\",\n",
    "        dtype=str,\n",
    "        delimiter=\"\\t\",\n",
    "    )\n",
    "    return binary_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English (get LNCS2, Intersection_NN and Cosine scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang = \"english\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Initialize models avgs\n",
    "initialize_avgs(model1, model2)\n",
    "shared_vocabulary = set(model1.wv.vocab.keys()).intersection(set(model2.wv.vocab.keys()))\n",
    "shared_vocabulary_df = pandas.DataFrame(shared_vocabulary, columns=[\"word\"])\n",
    "shared_vocabulary_df[\"lncs2\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: lncs2(word, model1, model2, 25)\n",
    ")\n",
    "shared_vocabulary_df[\"intersection_nn\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: intersection_nn(word, model1, model2)\n",
    ")\n",
    "shared_vocabulary_df[\"cosine\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: 1 - cosine(model1.wv[word], model2.wv[word])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add mean of the three metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shared_vocabulary_df[\"mean\"] = shared_vocabulary_df[[\"lncs2\", \"cosine\", \"intersection_nn\"]].apply(\n",
    "    lambda x: (x.lncs2 + x.cosine + x.intersection_nn) / 3, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocabulary_df[\"count_m1\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: model1.wv.vocab[word].count\n",
    ")\n",
    "shared_vocabulary_df[\"count_m2\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: model2.wv.vocab[word].count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocabulary_df.to_pickle(\"./shared_vocabulary_metrics.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words changed the most (by LNCS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                word     lncs2  intersection_nn    cosine      mean  count_m1  \\\n10916       pregnant -0.059092            0.012 -0.001763 -0.016285        31   \n12134         unused -0.059081            0.013 -0.073233 -0.039771        26   \n13989         scarce -0.044323            0.010  0.072006  0.012561       329   \n4183         incline -0.034136            0.012  0.176630  0.051498       114   \n11680             ml -0.029649            0.002 -0.079436 -0.035695         1   \n21938          major -0.016591            0.002  0.263108  0.082839       531   \n12794        someday  0.002855            0.012  0.115850  0.043568         1   \n21775       mentally  0.016916            0.020  0.328319  0.121745        31   \n8043           tense  0.032603            0.016  0.138092  0.062232        11   \n23205            err  0.036362            0.026  0.169528  0.077297        70   \n14235        someone  0.045325            0.015  0.050252  0.036859         3   \n17220        backing  0.045896            0.013 -0.038172  0.006908         5   \n805            twain  0.055887            0.002  0.011805  0.023231        29   \n26996       juvenile  0.056204            0.026  0.037996  0.040067        32   \n2797              hy  0.056843            0.008  0.008580  0.024474       109   \n17606  significantly  0.058707            0.002 -0.010645  0.016688        31   \n7224           today  0.063035            0.017  0.364674  0.148237       402   \n18244        hearing  0.064546            0.010 -0.008649  0.021966         1   \n18557         some--  0.068811            0.036 -0.110117 -0.001769         2   \n15991        drastic  0.069603            0.007  0.058760  0.045121         3   \n289       detachment  0.070652            0.007  0.089520  0.055724        77   \n6226      deflection  0.071443            0.017  0.267019  0.118487        15   \n26863        discard  0.078995            0.005  0.215280  0.099758        57   \n23985      cheapness  0.079505            0.016  0.070801  0.055435        17   \n8205         roughly  0.084750            0.003  0.068283  0.052011        50   \n16297     conveyance  0.085479            0.015  0.159299  0.086593        44   \n1345          boring  0.089093            0.009 -0.009177  0.029639         8   \n22117          check  0.091327            0.021  0.178886  0.097071       367   \n12377    frightening  0.095688            0.008 -0.102361  0.000442         3   \n20731        burthen  0.101387            0.019  0.162739  0.094375        49   \n\n       count_m2  \n10916       158  \n12134        36  \n13989        61  \n4183         25  \n11680        18  \n21938      1554  \n12794       125  \n21775        90  \n8043        139  \n23205        11  \n14235      1589  \n17220        42  \n805          31  \n26996        56  \n2797         11  \n17606       123  \n7224       1973  \n18244       149  \n18557         6  \n15991        48  \n289          44  \n6226          3  \n26863       109  \n23985         4  \n8205        182  \n16297         4  \n1345         77  \n22117      1028  \n12377        76  \n20731         1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>mean</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10916</th>\n      <td>pregnant</td>\n      <td>-0.059092</td>\n      <td>0.012</td>\n      <td>-0.001763</td>\n      <td>-0.016285</td>\n      <td>31</td>\n      <td>158</td>\n    </tr>\n    <tr>\n      <th>12134</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.013</td>\n      <td>-0.073233</td>\n      <td>-0.039771</td>\n      <td>26</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>13989</th>\n      <td>scarce</td>\n      <td>-0.044323</td>\n      <td>0.010</td>\n      <td>0.072006</td>\n      <td>0.012561</td>\n      <td>329</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>4183</th>\n      <td>incline</td>\n      <td>-0.034136</td>\n      <td>0.012</td>\n      <td>0.176630</td>\n      <td>0.051498</td>\n      <td>114</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>11680</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.002</td>\n      <td>-0.079436</td>\n      <td>-0.035695</td>\n      <td>1</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>21938</th>\n      <td>major</td>\n      <td>-0.016591</td>\n      <td>0.002</td>\n      <td>0.263108</td>\n      <td>0.082839</td>\n      <td>531</td>\n      <td>1554</td>\n    </tr>\n    <tr>\n      <th>12794</th>\n      <td>someday</td>\n      <td>0.002855</td>\n      <td>0.012</td>\n      <td>0.115850</td>\n      <td>0.043568</td>\n      <td>1</td>\n      <td>125</td>\n    </tr>\n    <tr>\n      <th>21775</th>\n      <td>mentally</td>\n      <td>0.016916</td>\n      <td>0.020</td>\n      <td>0.328319</td>\n      <td>0.121745</td>\n      <td>31</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>8043</th>\n      <td>tense</td>\n      <td>0.032603</td>\n      <td>0.016</td>\n      <td>0.138092</td>\n      <td>0.062232</td>\n      <td>11</td>\n      <td>139</td>\n    </tr>\n    <tr>\n      <th>23205</th>\n      <td>err</td>\n      <td>0.036362</td>\n      <td>0.026</td>\n      <td>0.169528</td>\n      <td>0.077297</td>\n      <td>70</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>14235</th>\n      <td>someone</td>\n      <td>0.045325</td>\n      <td>0.015</td>\n      <td>0.050252</td>\n      <td>0.036859</td>\n      <td>3</td>\n      <td>1589</td>\n    </tr>\n    <tr>\n      <th>17220</th>\n      <td>backing</td>\n      <td>0.045896</td>\n      <td>0.013</td>\n      <td>-0.038172</td>\n      <td>0.006908</td>\n      <td>5</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>805</th>\n      <td>twain</td>\n      <td>0.055887</td>\n      <td>0.002</td>\n      <td>0.011805</td>\n      <td>0.023231</td>\n      <td>29</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>26996</th>\n      <td>juvenile</td>\n      <td>0.056204</td>\n      <td>0.026</td>\n      <td>0.037996</td>\n      <td>0.040067</td>\n      <td>32</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>2797</th>\n      <td>hy</td>\n      <td>0.056843</td>\n      <td>0.008</td>\n      <td>0.008580</td>\n      <td>0.024474</td>\n      <td>109</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>17606</th>\n      <td>significantly</td>\n      <td>0.058707</td>\n      <td>0.002</td>\n      <td>-0.010645</td>\n      <td>0.016688</td>\n      <td>31</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>7224</th>\n      <td>today</td>\n      <td>0.063035</td>\n      <td>0.017</td>\n      <td>0.364674</td>\n      <td>0.148237</td>\n      <td>402</td>\n      <td>1973</td>\n    </tr>\n    <tr>\n      <th>18244</th>\n      <td>hearing</td>\n      <td>0.064546</td>\n      <td>0.010</td>\n      <td>-0.008649</td>\n      <td>0.021966</td>\n      <td>1</td>\n      <td>149</td>\n    </tr>\n    <tr>\n      <th>18557</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.036</td>\n      <td>-0.110117</td>\n      <td>-0.001769</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>15991</th>\n      <td>drastic</td>\n      <td>0.069603</td>\n      <td>0.007</td>\n      <td>0.058760</td>\n      <td>0.045121</td>\n      <td>3</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>289</th>\n      <td>detachment</td>\n      <td>0.070652</td>\n      <td>0.007</td>\n      <td>0.089520</td>\n      <td>0.055724</td>\n      <td>77</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>6226</th>\n      <td>deflection</td>\n      <td>0.071443</td>\n      <td>0.017</td>\n      <td>0.267019</td>\n      <td>0.118487</td>\n      <td>15</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>26863</th>\n      <td>discard</td>\n      <td>0.078995</td>\n      <td>0.005</td>\n      <td>0.215280</td>\n      <td>0.099758</td>\n      <td>57</td>\n      <td>109</td>\n    </tr>\n    <tr>\n      <th>23985</th>\n      <td>cheapness</td>\n      <td>0.079505</td>\n      <td>0.016</td>\n      <td>0.070801</td>\n      <td>0.055435</td>\n      <td>17</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>8205</th>\n      <td>roughly</td>\n      <td>0.084750</td>\n      <td>0.003</td>\n      <td>0.068283</td>\n      <td>0.052011</td>\n      <td>50</td>\n      <td>182</td>\n    </tr>\n    <tr>\n      <th>16297</th>\n      <td>conveyance</td>\n      <td>0.085479</td>\n      <td>0.015</td>\n      <td>0.159299</td>\n      <td>0.086593</td>\n      <td>44</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1345</th>\n      <td>boring</td>\n      <td>0.089093</td>\n      <td>0.009</td>\n      <td>-0.009177</td>\n      <td>0.029639</td>\n      <td>8</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>22117</th>\n      <td>check</td>\n      <td>0.091327</td>\n      <td>0.021</td>\n      <td>0.178886</td>\n      <td>0.097071</td>\n      <td>367</td>\n      <td>1028</td>\n    </tr>\n    <tr>\n      <th>12377</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.008</td>\n      <td>-0.102361</td>\n      <td>0.000442</td>\n      <td>3</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>20731</th>\n      <td>burthen</td>\n      <td>0.101387</td>\n      <td>0.019</td>\n      <td>0.162739</td>\n      <td>0.094375</td>\n      <td>49</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"lncs2\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words changed the less by LNCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               word     lncs2  intersection_nn    cosine      mean  count_m1  \\\n8807            ten  0.998332            0.335  0.834361  0.722564      1140   \n22068        eleven  0.998247            0.339  0.813450  0.716899       128   \n10773       fifteen  0.997878            0.332  0.802010  0.710630       326   \n20937        twenty  0.997811            0.329  0.830547  0.719119       791   \n14380        twelve  0.997662            0.330  0.791642  0.706435       503   \n23289            13  0.997070            0.304  0.669598  0.656889        41   \n7267             11  0.996741            0.286  0.628893  0.637211        49   \n1607          eight  0.996465            0.326  0.848650  0.723705       524   \n11522            12  0.995956            0.347  0.760477  0.701144        73   \n20191   twenty-five  0.995953            0.352  0.815984  0.721312       162   \n8087         thirty  0.995821            0.321  0.823355  0.713392       466   \n7823           five  0.995344            0.335  0.842608  0.724317      1177   \n15932            24  0.995152            0.306  0.756793  0.685981        31   \n19778            14  0.994896            0.304  0.676747  0.658548        51   \n5296          forty  0.994742            0.327  0.825067  0.715603       330   \n21238            20  0.994682            0.337  0.665067  0.665583        58   \n7437            six  0.994679            0.319  0.866468  0.726716       976   \n8375    thirty-five  0.994590            0.328  0.708199  0.676929        40   \n11066       sixteen  0.994375            0.345  0.794630  0.711335       165   \n20058      fourteen  0.994299            0.334  0.780336  0.702879       123   \n11786            10  0.994153            0.322  0.670698  0.662284       104   \n2682       eighteen  0.994082            0.332  0.813038  0.713040       201   \n10375            16  0.993810            0.309  0.696257  0.666356        42   \n3776   twenty-seven  0.993109            0.328  0.598287  0.639798        27   \n26755         sixty  0.992328            0.329  0.810002  0.710443       198   \n22660     seventeen  0.991974            0.312  0.782525  0.695500       111   \n15725          nine  0.991471            0.327  0.800287  0.706253       324   \n5460             30  0.991453            0.323  0.648498  0.654317        70   \n5035          three  0.990792            0.281  0.850726  0.707506      3089   \n14640      thirteen  0.990453            0.290  0.706869  0.662441       129   \n\n       count_m2  \n8807       1154  \n22068       214  \n10773       350  \n20937       536  \n14380       340  \n23289       363  \n7267        526  \n1607        868  \n11522       529  \n20191       143  \n8087        359  \n7823       2129  \n15932       292  \n19778       396  \n5296        249  \n21238       742  \n7437       1495  \n8375         71  \n11066       139  \n20058       156  \n11786      1074  \n2682        125  \n10375       352  \n3776         25  \n26755       122  \n22660        82  \n15725       558  \n5460        673  \n5035       4548  \n14640       105  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>mean</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8807</th>\n      <td>ten</td>\n      <td>0.998332</td>\n      <td>0.335</td>\n      <td>0.834361</td>\n      <td>0.722564</td>\n      <td>1140</td>\n      <td>1154</td>\n    </tr>\n    <tr>\n      <th>22068</th>\n      <td>eleven</td>\n      <td>0.998247</td>\n      <td>0.339</td>\n      <td>0.813450</td>\n      <td>0.716899</td>\n      <td>128</td>\n      <td>214</td>\n    </tr>\n    <tr>\n      <th>10773</th>\n      <td>fifteen</td>\n      <td>0.997878</td>\n      <td>0.332</td>\n      <td>0.802010</td>\n      <td>0.710630</td>\n      <td>326</td>\n      <td>350</td>\n    </tr>\n    <tr>\n      <th>20937</th>\n      <td>twenty</td>\n      <td>0.997811</td>\n      <td>0.329</td>\n      <td>0.830547</td>\n      <td>0.719119</td>\n      <td>791</td>\n      <td>536</td>\n    </tr>\n    <tr>\n      <th>14380</th>\n      <td>twelve</td>\n      <td>0.997662</td>\n      <td>0.330</td>\n      <td>0.791642</td>\n      <td>0.706435</td>\n      <td>503</td>\n      <td>340</td>\n    </tr>\n    <tr>\n      <th>23289</th>\n      <td>13</td>\n      <td>0.997070</td>\n      <td>0.304</td>\n      <td>0.669598</td>\n      <td>0.656889</td>\n      <td>41</td>\n      <td>363</td>\n    </tr>\n    <tr>\n      <th>7267</th>\n      <td>11</td>\n      <td>0.996741</td>\n      <td>0.286</td>\n      <td>0.628893</td>\n      <td>0.637211</td>\n      <td>49</td>\n      <td>526</td>\n    </tr>\n    <tr>\n      <th>1607</th>\n      <td>eight</td>\n      <td>0.996465</td>\n      <td>0.326</td>\n      <td>0.848650</td>\n      <td>0.723705</td>\n      <td>524</td>\n      <td>868</td>\n    </tr>\n    <tr>\n      <th>11522</th>\n      <td>12</td>\n      <td>0.995956</td>\n      <td>0.347</td>\n      <td>0.760477</td>\n      <td>0.701144</td>\n      <td>73</td>\n      <td>529</td>\n    </tr>\n    <tr>\n      <th>20191</th>\n      <td>twenty-five</td>\n      <td>0.995953</td>\n      <td>0.352</td>\n      <td>0.815984</td>\n      <td>0.721312</td>\n      <td>162</td>\n      <td>143</td>\n    </tr>\n    <tr>\n      <th>8087</th>\n      <td>thirty</td>\n      <td>0.995821</td>\n      <td>0.321</td>\n      <td>0.823355</td>\n      <td>0.713392</td>\n      <td>466</td>\n      <td>359</td>\n    </tr>\n    <tr>\n      <th>7823</th>\n      <td>five</td>\n      <td>0.995344</td>\n      <td>0.335</td>\n      <td>0.842608</td>\n      <td>0.724317</td>\n      <td>1177</td>\n      <td>2129</td>\n    </tr>\n    <tr>\n      <th>15932</th>\n      <td>24</td>\n      <td>0.995152</td>\n      <td>0.306</td>\n      <td>0.756793</td>\n      <td>0.685981</td>\n      <td>31</td>\n      <td>292</td>\n    </tr>\n    <tr>\n      <th>19778</th>\n      <td>14</td>\n      <td>0.994896</td>\n      <td>0.304</td>\n      <td>0.676747</td>\n      <td>0.658548</td>\n      <td>51</td>\n      <td>396</td>\n    </tr>\n    <tr>\n      <th>5296</th>\n      <td>forty</td>\n      <td>0.994742</td>\n      <td>0.327</td>\n      <td>0.825067</td>\n      <td>0.715603</td>\n      <td>330</td>\n      <td>249</td>\n    </tr>\n    <tr>\n      <th>21238</th>\n      <td>20</td>\n      <td>0.994682</td>\n      <td>0.337</td>\n      <td>0.665067</td>\n      <td>0.665583</td>\n      <td>58</td>\n      <td>742</td>\n    </tr>\n    <tr>\n      <th>7437</th>\n      <td>six</td>\n      <td>0.994679</td>\n      <td>0.319</td>\n      <td>0.866468</td>\n      <td>0.726716</td>\n      <td>976</td>\n      <td>1495</td>\n    </tr>\n    <tr>\n      <th>8375</th>\n      <td>thirty-five</td>\n      <td>0.994590</td>\n      <td>0.328</td>\n      <td>0.708199</td>\n      <td>0.676929</td>\n      <td>40</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>11066</th>\n      <td>sixteen</td>\n      <td>0.994375</td>\n      <td>0.345</td>\n      <td>0.794630</td>\n      <td>0.711335</td>\n      <td>165</td>\n      <td>139</td>\n    </tr>\n    <tr>\n      <th>20058</th>\n      <td>fourteen</td>\n      <td>0.994299</td>\n      <td>0.334</td>\n      <td>0.780336</td>\n      <td>0.702879</td>\n      <td>123</td>\n      <td>156</td>\n    </tr>\n    <tr>\n      <th>11786</th>\n      <td>10</td>\n      <td>0.994153</td>\n      <td>0.322</td>\n      <td>0.670698</td>\n      <td>0.662284</td>\n      <td>104</td>\n      <td>1074</td>\n    </tr>\n    <tr>\n      <th>2682</th>\n      <td>eighteen</td>\n      <td>0.994082</td>\n      <td>0.332</td>\n      <td>0.813038</td>\n      <td>0.713040</td>\n      <td>201</td>\n      <td>125</td>\n    </tr>\n    <tr>\n      <th>10375</th>\n      <td>16</td>\n      <td>0.993810</td>\n      <td>0.309</td>\n      <td>0.696257</td>\n      <td>0.666356</td>\n      <td>42</td>\n      <td>352</td>\n    </tr>\n    <tr>\n      <th>3776</th>\n      <td>twenty-seven</td>\n      <td>0.993109</td>\n      <td>0.328</td>\n      <td>0.598287</td>\n      <td>0.639798</td>\n      <td>27</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>26755</th>\n      <td>sixty</td>\n      <td>0.992328</td>\n      <td>0.329</td>\n      <td>0.810002</td>\n      <td>0.710443</td>\n      <td>198</td>\n      <td>122</td>\n    </tr>\n    <tr>\n      <th>22660</th>\n      <td>seventeen</td>\n      <td>0.991974</td>\n      <td>0.312</td>\n      <td>0.782525</td>\n      <td>0.695500</td>\n      <td>111</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>15725</th>\n      <td>nine</td>\n      <td>0.991471</td>\n      <td>0.327</td>\n      <td>0.800287</td>\n      <td>0.706253</td>\n      <td>324</td>\n      <td>558</td>\n    </tr>\n    <tr>\n      <th>5460</th>\n      <td>30</td>\n      <td>0.991453</td>\n      <td>0.323</td>\n      <td>0.648498</td>\n      <td>0.654317</td>\n      <td>70</td>\n      <td>673</td>\n    </tr>\n    <tr>\n      <th>5035</th>\n      <td>three</td>\n      <td>0.990792</td>\n      <td>0.281</td>\n      <td>0.850726</td>\n      <td>0.707506</td>\n      <td>3089</td>\n      <td>4548</td>\n    </tr>\n    <tr>\n      <th>14640</th>\n      <td>thirteen</td>\n      <td>0.990453</td>\n      <td>0.290</td>\n      <td>0.706869</td>\n      <td>0.662441</td>\n      <td>129</td>\n      <td>105</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"lncs2\"], ascending=False)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by Intersection_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                word     lncs2  intersection_nn    cosine      mean  count_m1  \\\n11297          virus  0.323960            0.000 -0.149479  0.058160        13   \n4252            coke  0.272922            0.001  0.071755  0.115226        12   \n19626      rusticity  0.613301            0.001  0.544417  0.386239         3   \n7493      clumsiness  0.552998            0.001  0.532715  0.362238         1   \n23735        funding  0.344045            0.001  0.151782  0.165609         3   \n7662       inclusive  0.339527            0.002  0.072305  0.137944        13   \n11680             ml -0.029649            0.002 -0.079436 -0.035695         1   \n21938          major -0.016591            0.002  0.263108  0.082839       531   \n8815            rove  0.151496            0.002  0.073780  0.075758        48   \n25692          media  0.323505            0.002  0.092881  0.139462        49   \n805            twain  0.055887            0.002  0.011805  0.023231        29   \n17606  significantly  0.058707            0.002 -0.010645  0.016688        31   \n10982           aura  0.217044            0.002 -0.075655  0.047796         9   \n11944       uprising  0.307683            0.002  0.156027  0.155237        10   \n11150      projector  0.393249            0.002 -0.094298  0.100317        18   \n11323           lacy  0.361720            0.002  0.061578  0.141766         7   \n2646           teeny  0.563224            0.003  0.144121  0.236782         2   \n8205         roughly  0.084750            0.003  0.068283  0.052011        50   \n10425     cavalierly  0.700440            0.004  0.305752  0.336731         3   \n12716           vail  0.317838            0.004 -0.031890  0.096649        16   \n9552         clarify  0.141261            0.004  0.063697  0.069653         5   \n25829          trump  0.340584            0.004  0.167791  0.170791        53   \n17680           gage  0.496162            0.004  0.145964  0.215376        42   \n19623        spinoza  0.646836            0.004  0.591190  0.414009         2   \n24379         psyche  0.215099            0.004  0.096274  0.105124         7   \n18064          whiff  0.406077            0.004  0.234451  0.214843        12   \n18627          xx_jj  0.570590            0.005  0.499882  0.358491         3   \n26863        discard  0.078995            0.005  0.215280  0.099758        57   \n23372           vane  0.335328            0.005  0.106609  0.148979        26   \n19698       tiberius  0.681105            0.005  0.538918  0.408341         3   \n\n       count_m2  \n11297       166  \n4252         88  \n19626         2  \n7493          4  \n23735       119  \n7662         21  \n11680        18  \n21938      1554  \n8815         19  \n25692       394  \n805          31  \n17606       123  \n10982        52  \n11944        39  \n11150        33  \n11323        20  \n2646         10  \n8205        182  \n10425         3  \n12716        34  \n9552         50  \n25829        41  \n17680        21  \n19623         3  \n24379        35  \n18064        28  \n18627         3  \n26863       109  \n23372         9  \n19698         2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>mean</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11297</th>\n      <td>virus</td>\n      <td>0.323960</td>\n      <td>0.000</td>\n      <td>-0.149479</td>\n      <td>0.058160</td>\n      <td>13</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>4252</th>\n      <td>coke</td>\n      <td>0.272922</td>\n      <td>0.001</td>\n      <td>0.071755</td>\n      <td>0.115226</td>\n      <td>12</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>19626</th>\n      <td>rusticity</td>\n      <td>0.613301</td>\n      <td>0.001</td>\n      <td>0.544417</td>\n      <td>0.386239</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7493</th>\n      <td>clumsiness</td>\n      <td>0.552998</td>\n      <td>0.001</td>\n      <td>0.532715</td>\n      <td>0.362238</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>23735</th>\n      <td>funding</td>\n      <td>0.344045</td>\n      <td>0.001</td>\n      <td>0.151782</td>\n      <td>0.165609</td>\n      <td>3</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>7662</th>\n      <td>inclusive</td>\n      <td>0.339527</td>\n      <td>0.002</td>\n      <td>0.072305</td>\n      <td>0.137944</td>\n      <td>13</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>11680</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.002</td>\n      <td>-0.079436</td>\n      <td>-0.035695</td>\n      <td>1</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>21938</th>\n      <td>major</td>\n      <td>-0.016591</td>\n      <td>0.002</td>\n      <td>0.263108</td>\n      <td>0.082839</td>\n      <td>531</td>\n      <td>1554</td>\n    </tr>\n    <tr>\n      <th>8815</th>\n      <td>rove</td>\n      <td>0.151496</td>\n      <td>0.002</td>\n      <td>0.073780</td>\n      <td>0.075758</td>\n      <td>48</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>25692</th>\n      <td>media</td>\n      <td>0.323505</td>\n      <td>0.002</td>\n      <td>0.092881</td>\n      <td>0.139462</td>\n      <td>49</td>\n      <td>394</td>\n    </tr>\n    <tr>\n      <th>805</th>\n      <td>twain</td>\n      <td>0.055887</td>\n      <td>0.002</td>\n      <td>0.011805</td>\n      <td>0.023231</td>\n      <td>29</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>17606</th>\n      <td>significantly</td>\n      <td>0.058707</td>\n      <td>0.002</td>\n      <td>-0.010645</td>\n      <td>0.016688</td>\n      <td>31</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>10982</th>\n      <td>aura</td>\n      <td>0.217044</td>\n      <td>0.002</td>\n      <td>-0.075655</td>\n      <td>0.047796</td>\n      <td>9</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>11944</th>\n      <td>uprising</td>\n      <td>0.307683</td>\n      <td>0.002</td>\n      <td>0.156027</td>\n      <td>0.155237</td>\n      <td>10</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>11150</th>\n      <td>projector</td>\n      <td>0.393249</td>\n      <td>0.002</td>\n      <td>-0.094298</td>\n      <td>0.100317</td>\n      <td>18</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>11323</th>\n      <td>lacy</td>\n      <td>0.361720</td>\n      <td>0.002</td>\n      <td>0.061578</td>\n      <td>0.141766</td>\n      <td>7</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>2646</th>\n      <td>teeny</td>\n      <td>0.563224</td>\n      <td>0.003</td>\n      <td>0.144121</td>\n      <td>0.236782</td>\n      <td>2</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>8205</th>\n      <td>roughly</td>\n      <td>0.084750</td>\n      <td>0.003</td>\n      <td>0.068283</td>\n      <td>0.052011</td>\n      <td>50</td>\n      <td>182</td>\n    </tr>\n    <tr>\n      <th>10425</th>\n      <td>cavalierly</td>\n      <td>0.700440</td>\n      <td>0.004</td>\n      <td>0.305752</td>\n      <td>0.336731</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>12716</th>\n      <td>vail</td>\n      <td>0.317838</td>\n      <td>0.004</td>\n      <td>-0.031890</td>\n      <td>0.096649</td>\n      <td>16</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>9552</th>\n      <td>clarify</td>\n      <td>0.141261</td>\n      <td>0.004</td>\n      <td>0.063697</td>\n      <td>0.069653</td>\n      <td>5</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>25829</th>\n      <td>trump</td>\n      <td>0.340584</td>\n      <td>0.004</td>\n      <td>0.167791</td>\n      <td>0.170791</td>\n      <td>53</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>17680</th>\n      <td>gage</td>\n      <td>0.496162</td>\n      <td>0.004</td>\n      <td>0.145964</td>\n      <td>0.215376</td>\n      <td>42</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>19623</th>\n      <td>spinoza</td>\n      <td>0.646836</td>\n      <td>0.004</td>\n      <td>0.591190</td>\n      <td>0.414009</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>24379</th>\n      <td>psyche</td>\n      <td>0.215099</td>\n      <td>0.004</td>\n      <td>0.096274</td>\n      <td>0.105124</td>\n      <td>7</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>18064</th>\n      <td>whiff</td>\n      <td>0.406077</td>\n      <td>0.004</td>\n      <td>0.234451</td>\n      <td>0.214843</td>\n      <td>12</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>18627</th>\n      <td>xx_jj</td>\n      <td>0.570590</td>\n      <td>0.005</td>\n      <td>0.499882</td>\n      <td>0.358491</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>26863</th>\n      <td>discard</td>\n      <td>0.078995</td>\n      <td>0.005</td>\n      <td>0.215280</td>\n      <td>0.099758</td>\n      <td>57</td>\n      <td>109</td>\n    </tr>\n    <tr>\n      <th>23372</th>\n      <td>vane</td>\n      <td>0.335328</td>\n      <td>0.005</td>\n      <td>0.106609</td>\n      <td>0.148979</td>\n      <td>26</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>19698</th>\n      <td>tiberius</td>\n      <td>0.681105</td>\n      <td>0.005</td>\n      <td>0.538918</td>\n      <td>0.408341</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"intersection_nn\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words changed the less by Intersection_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             word     lncs2  intersection_nn    cosine      mean  count_m1  \\\n2920     convince  0.945782            0.486  0.781441  0.737741       476   \n248       believe  0.971778            0.478  0.758603  0.736127      2948   \n12402       swing  0.971353            0.462  0.728393  0.720582       166   \n10888        deny  0.964949            0.454  0.691601  0.703517       682   \n13761  understand  0.974703            0.442  0.727781  0.714828      1475   \n22884        find  0.928129            0.442  0.849056  0.739728      7549   \n22111       grass  0.945075            0.440  0.797416  0.727497       378   \n14853       slope  0.930147            0.434  0.855519  0.739888       137   \n6467        trust  0.957087            0.434  0.670041  0.687043      1050   \n2298        trunk  0.952310            0.432  0.776542  0.720284       322   \n13686     tree_nn  0.973066            0.431  0.871842  0.758636      2322   \n12878         leg  0.964123            0.431  0.840654  0.745259       376   \n8041         hair  0.912894            0.430  0.867503  0.736799       974   \n8534         hang  0.958704            0.430  0.809736  0.732813      1049   \n23570        leaf  0.932912            0.429  0.840396  0.734103       773   \n4355      satisfy  0.869777            0.427  0.681398  0.659392       521   \n10721    shoulder  0.972574            0.427  0.826431  0.742002       677   \n11188      collar  0.949337            0.425  0.783521  0.719286       112   \n20726      assure  0.933151            0.425  0.711387  0.689846       611   \n12940        rope  0.946561            0.425  0.830084  0.733882       175   \n15171       fence  0.921338            0.425  0.829482  0.725273       179   \n24535      carpet  0.924440            0.425  0.777948  0.709129       126   \n11954        pull  0.972609            0.424  0.750570  0.715726       379   \n15916       prove  0.958826            0.423  0.727998  0.703275      1436   \n19531         dry  0.924254            0.422  0.805396  0.717217       522   \n19495        thin  0.927979            0.421  0.801362  0.716780       407   \n9191         neck  0.945422            0.421  0.832751  0.733058       564   \n22493       bring  0.983159            0.421  0.865412  0.756524      3918   \n12894     boiling  0.913699            0.420  0.852232  0.728644        53   \n6016         hear  0.958789            0.420  0.851877  0.743555      4691   \n\n       count_m2  \n2920        450  \n248        2618  \n12402       626  \n10888       414  \n13761      1683  \n22884      6846  \n22111       483  \n14853       200  \n6467        562  \n2298        180  \n13686      1596  \n12878      1191  \n8041       1970  \n8534       1189  \n23570       477  \n4355        200  \n10721      1237  \n11188       213  \n20726       345  \n12940       276  \n15171       260  \n24535       174  \n11954      1989  \n15916       825  \n19531       691  \n19495       586  \n9191        714  \n22493      3332  \n12894        48  \n6016       4058  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>mean</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2920</th>\n      <td>convince</td>\n      <td>0.945782</td>\n      <td>0.486</td>\n      <td>0.781441</td>\n      <td>0.737741</td>\n      <td>476</td>\n      <td>450</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>believe</td>\n      <td>0.971778</td>\n      <td>0.478</td>\n      <td>0.758603</td>\n      <td>0.736127</td>\n      <td>2948</td>\n      <td>2618</td>\n    </tr>\n    <tr>\n      <th>12402</th>\n      <td>swing</td>\n      <td>0.971353</td>\n      <td>0.462</td>\n      <td>0.728393</td>\n      <td>0.720582</td>\n      <td>166</td>\n      <td>626</td>\n    </tr>\n    <tr>\n      <th>10888</th>\n      <td>deny</td>\n      <td>0.964949</td>\n      <td>0.454</td>\n      <td>0.691601</td>\n      <td>0.703517</td>\n      <td>682</td>\n      <td>414</td>\n    </tr>\n    <tr>\n      <th>13761</th>\n      <td>understand</td>\n      <td>0.974703</td>\n      <td>0.442</td>\n      <td>0.727781</td>\n      <td>0.714828</td>\n      <td>1475</td>\n      <td>1683</td>\n    </tr>\n    <tr>\n      <th>22884</th>\n      <td>find</td>\n      <td>0.928129</td>\n      <td>0.442</td>\n      <td>0.849056</td>\n      <td>0.739728</td>\n      <td>7549</td>\n      <td>6846</td>\n    </tr>\n    <tr>\n      <th>22111</th>\n      <td>grass</td>\n      <td>0.945075</td>\n      <td>0.440</td>\n      <td>0.797416</td>\n      <td>0.727497</td>\n      <td>378</td>\n      <td>483</td>\n    </tr>\n    <tr>\n      <th>14853</th>\n      <td>slope</td>\n      <td>0.930147</td>\n      <td>0.434</td>\n      <td>0.855519</td>\n      <td>0.739888</td>\n      <td>137</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>6467</th>\n      <td>trust</td>\n      <td>0.957087</td>\n      <td>0.434</td>\n      <td>0.670041</td>\n      <td>0.687043</td>\n      <td>1050</td>\n      <td>562</td>\n    </tr>\n    <tr>\n      <th>2298</th>\n      <td>trunk</td>\n      <td>0.952310</td>\n      <td>0.432</td>\n      <td>0.776542</td>\n      <td>0.720284</td>\n      <td>322</td>\n      <td>180</td>\n    </tr>\n    <tr>\n      <th>13686</th>\n      <td>tree_nn</td>\n      <td>0.973066</td>\n      <td>0.431</td>\n      <td>0.871842</td>\n      <td>0.758636</td>\n      <td>2322</td>\n      <td>1596</td>\n    </tr>\n    <tr>\n      <th>12878</th>\n      <td>leg</td>\n      <td>0.964123</td>\n      <td>0.431</td>\n      <td>0.840654</td>\n      <td>0.745259</td>\n      <td>376</td>\n      <td>1191</td>\n    </tr>\n    <tr>\n      <th>8041</th>\n      <td>hair</td>\n      <td>0.912894</td>\n      <td>0.430</td>\n      <td>0.867503</td>\n      <td>0.736799</td>\n      <td>974</td>\n      <td>1970</td>\n    </tr>\n    <tr>\n      <th>8534</th>\n      <td>hang</td>\n      <td>0.958704</td>\n      <td>0.430</td>\n      <td>0.809736</td>\n      <td>0.732813</td>\n      <td>1049</td>\n      <td>1189</td>\n    </tr>\n    <tr>\n      <th>23570</th>\n      <td>leaf</td>\n      <td>0.932912</td>\n      <td>0.429</td>\n      <td>0.840396</td>\n      <td>0.734103</td>\n      <td>773</td>\n      <td>477</td>\n    </tr>\n    <tr>\n      <th>4355</th>\n      <td>satisfy</td>\n      <td>0.869777</td>\n      <td>0.427</td>\n      <td>0.681398</td>\n      <td>0.659392</td>\n      <td>521</td>\n      <td>200</td>\n    </tr>\n    <tr>\n      <th>10721</th>\n      <td>shoulder</td>\n      <td>0.972574</td>\n      <td>0.427</td>\n      <td>0.826431</td>\n      <td>0.742002</td>\n      <td>677</td>\n      <td>1237</td>\n    </tr>\n    <tr>\n      <th>11188</th>\n      <td>collar</td>\n      <td>0.949337</td>\n      <td>0.425</td>\n      <td>0.783521</td>\n      <td>0.719286</td>\n      <td>112</td>\n      <td>213</td>\n    </tr>\n    <tr>\n      <th>20726</th>\n      <td>assure</td>\n      <td>0.933151</td>\n      <td>0.425</td>\n      <td>0.711387</td>\n      <td>0.689846</td>\n      <td>611</td>\n      <td>345</td>\n    </tr>\n    <tr>\n      <th>12940</th>\n      <td>rope</td>\n      <td>0.946561</td>\n      <td>0.425</td>\n      <td>0.830084</td>\n      <td>0.733882</td>\n      <td>175</td>\n      <td>276</td>\n    </tr>\n    <tr>\n      <th>15171</th>\n      <td>fence</td>\n      <td>0.921338</td>\n      <td>0.425</td>\n      <td>0.829482</td>\n      <td>0.725273</td>\n      <td>179</td>\n      <td>260</td>\n    </tr>\n    <tr>\n      <th>24535</th>\n      <td>carpet</td>\n      <td>0.924440</td>\n      <td>0.425</td>\n      <td>0.777948</td>\n      <td>0.709129</td>\n      <td>126</td>\n      <td>174</td>\n    </tr>\n    <tr>\n      <th>11954</th>\n      <td>pull</td>\n      <td>0.972609</td>\n      <td>0.424</td>\n      <td>0.750570</td>\n      <td>0.715726</td>\n      <td>379</td>\n      <td>1989</td>\n    </tr>\n    <tr>\n      <th>15916</th>\n      <td>prove</td>\n      <td>0.958826</td>\n      <td>0.423</td>\n      <td>0.727998</td>\n      <td>0.703275</td>\n      <td>1436</td>\n      <td>825</td>\n    </tr>\n    <tr>\n      <th>19531</th>\n      <td>dry</td>\n      <td>0.924254</td>\n      <td>0.422</td>\n      <td>0.805396</td>\n      <td>0.717217</td>\n      <td>522</td>\n      <td>691</td>\n    </tr>\n    <tr>\n      <th>19495</th>\n      <td>thin</td>\n      <td>0.927979</td>\n      <td>0.421</td>\n      <td>0.801362</td>\n      <td>0.716780</td>\n      <td>407</td>\n      <td>586</td>\n    </tr>\n    <tr>\n      <th>9191</th>\n      <td>neck</td>\n      <td>0.945422</td>\n      <td>0.421</td>\n      <td>0.832751</td>\n      <td>0.733058</td>\n      <td>564</td>\n      <td>714</td>\n    </tr>\n    <tr>\n      <th>22493</th>\n      <td>bring</td>\n      <td>0.983159</td>\n      <td>0.421</td>\n      <td>0.865412</td>\n      <td>0.756524</td>\n      <td>3918</td>\n      <td>3332</td>\n    </tr>\n    <tr>\n      <th>12894</th>\n      <td>boiling</td>\n      <td>0.913699</td>\n      <td>0.420</td>\n      <td>0.852232</td>\n      <td>0.728644</td>\n      <td>53</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>6016</th>\n      <td>hear</td>\n      <td>0.958789</td>\n      <td>0.420</td>\n      <td>0.851877</td>\n      <td>0.743555</td>\n      <td>4691</td>\n      <td>4058</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"intersection_nn\"], ascending=False)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               word     lncs2  intersection_nn    cosine      mean  count_m1  \\\n3112            ski  0.287075            0.017 -0.166819  0.045752         2   \n11297         virus  0.323960            0.000 -0.149479  0.058160        13   \n19687           fer  0.259189            0.034 -0.132545  0.053548         4   \n1718             op  0.695739            0.026 -0.127812  0.197976        21   \n18580        wilmer  0.500615            0.050 -0.111061  0.146518        11   \n18557        some--  0.068811            0.036 -0.110117 -0.001769         2   \n19448       setting  0.200999            0.009 -0.109949  0.033350         1   \n12377   frightening  0.095688            0.008 -0.102361  0.000442         3   \n20480          gist  0.272670            0.006 -0.098656  0.060005        15   \n12836           cal  0.479762            0.020 -0.098061  0.133900        13   \n2507     intriguing  0.198948            0.030 -0.095689  0.044419        13   \n11150     projector  0.393249            0.002 -0.094298  0.100317        18   \n440             int  0.534314            0.037 -0.084740  0.162191         5   \n11246           cir  0.372242            0.019 -0.082652  0.102863        10   \n5660         scurvy  0.292371            0.014 -0.081057  0.075105         5   \n11966         flume  0.600722            0.028 -0.080179  0.182848         6   \n11680            ml -0.029649            0.002 -0.079436 -0.035695         1   \n18254            uc  0.517708            0.015 -0.077761  0.151649         1   \n10982          aura  0.217044            0.002 -0.075655  0.047796         9   \n26144         ethel  0.413615            0.026 -0.075074  0.121514         1   \n12134        unused -0.059081            0.013 -0.073233 -0.039771        26   \n23049         lotus  0.677376            0.029 -0.069215  0.212387         2   \n17087       onwards  0.414718            0.032 -0.068938  0.125927        11   \n16592        walden  0.373120            0.010 -0.068412  0.104903        12   \n7594   ridiculously  0.558388            0.021 -0.065712  0.171225         7   \n12035       adeline  0.466286            0.035 -0.064648  0.145546        32   \n26306         humus  0.268067            0.031 -0.064053  0.078338         1   \n18973           doc  0.439268            0.025 -0.062904  0.133788         5   \n12746         elias  0.615312            0.040 -0.059430  0.198627        13   \n9170   fruitfulness  0.413633            0.035 -0.057541  0.130364        13   \n\n       count_m2  \n3112        218  \n11297       166  \n19687        47  \n1718         31  \n18580        16  \n18557         6  \n19448        62  \n12377        76  \n20480        15  \n12836        66  \n2507         43  \n11150        33  \n440          77  \n11246         1  \n5660          6  \n11966         1  \n11680        18  \n18254        15  \n10982        52  \n26144        40  \n12134        36  \n23049        22  \n17087         9  \n16592         8  \n7594         14  \n12035         1  \n26306        10  \n18973       107  \n12746        37  \n9170          2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>mean</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3112</th>\n      <td>ski</td>\n      <td>0.287075</td>\n      <td>0.017</td>\n      <td>-0.166819</td>\n      <td>0.045752</td>\n      <td>2</td>\n      <td>218</td>\n    </tr>\n    <tr>\n      <th>11297</th>\n      <td>virus</td>\n      <td>0.323960</td>\n      <td>0.000</td>\n      <td>-0.149479</td>\n      <td>0.058160</td>\n      <td>13</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>19687</th>\n      <td>fer</td>\n      <td>0.259189</td>\n      <td>0.034</td>\n      <td>-0.132545</td>\n      <td>0.053548</td>\n      <td>4</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>1718</th>\n      <td>op</td>\n      <td>0.695739</td>\n      <td>0.026</td>\n      <td>-0.127812</td>\n      <td>0.197976</td>\n      <td>21</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>18580</th>\n      <td>wilmer</td>\n      <td>0.500615</td>\n      <td>0.050</td>\n      <td>-0.111061</td>\n      <td>0.146518</td>\n      <td>11</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>18557</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.036</td>\n      <td>-0.110117</td>\n      <td>-0.001769</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>19448</th>\n      <td>setting</td>\n      <td>0.200999</td>\n      <td>0.009</td>\n      <td>-0.109949</td>\n      <td>0.033350</td>\n      <td>1</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>12377</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.008</td>\n      <td>-0.102361</td>\n      <td>0.000442</td>\n      <td>3</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>20480</th>\n      <td>gist</td>\n      <td>0.272670</td>\n      <td>0.006</td>\n      <td>-0.098656</td>\n      <td>0.060005</td>\n      <td>15</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>12836</th>\n      <td>cal</td>\n      <td>0.479762</td>\n      <td>0.020</td>\n      <td>-0.098061</td>\n      <td>0.133900</td>\n      <td>13</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>2507</th>\n      <td>intriguing</td>\n      <td>0.198948</td>\n      <td>0.030</td>\n      <td>-0.095689</td>\n      <td>0.044419</td>\n      <td>13</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>11150</th>\n      <td>projector</td>\n      <td>0.393249</td>\n      <td>0.002</td>\n      <td>-0.094298</td>\n      <td>0.100317</td>\n      <td>18</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>440</th>\n      <td>int</td>\n      <td>0.534314</td>\n      <td>0.037</td>\n      <td>-0.084740</td>\n      <td>0.162191</td>\n      <td>5</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>11246</th>\n      <td>cir</td>\n      <td>0.372242</td>\n      <td>0.019</td>\n      <td>-0.082652</td>\n      <td>0.102863</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5660</th>\n      <td>scurvy</td>\n      <td>0.292371</td>\n      <td>0.014</td>\n      <td>-0.081057</td>\n      <td>0.075105</td>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>11966</th>\n      <td>flume</td>\n      <td>0.600722</td>\n      <td>0.028</td>\n      <td>-0.080179</td>\n      <td>0.182848</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11680</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.002</td>\n      <td>-0.079436</td>\n      <td>-0.035695</td>\n      <td>1</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>18254</th>\n      <td>uc</td>\n      <td>0.517708</td>\n      <td>0.015</td>\n      <td>-0.077761</td>\n      <td>0.151649</td>\n      <td>1</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>10982</th>\n      <td>aura</td>\n      <td>0.217044</td>\n      <td>0.002</td>\n      <td>-0.075655</td>\n      <td>0.047796</td>\n      <td>9</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>26144</th>\n      <td>ethel</td>\n      <td>0.413615</td>\n      <td>0.026</td>\n      <td>-0.075074</td>\n      <td>0.121514</td>\n      <td>1</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>12134</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.013</td>\n      <td>-0.073233</td>\n      <td>-0.039771</td>\n      <td>26</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>23049</th>\n      <td>lotus</td>\n      <td>0.677376</td>\n      <td>0.029</td>\n      <td>-0.069215</td>\n      <td>0.212387</td>\n      <td>2</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>17087</th>\n      <td>onwards</td>\n      <td>0.414718</td>\n      <td>0.032</td>\n      <td>-0.068938</td>\n      <td>0.125927</td>\n      <td>11</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>16592</th>\n      <td>walden</td>\n      <td>0.373120</td>\n      <td>0.010</td>\n      <td>-0.068412</td>\n      <td>0.104903</td>\n      <td>12</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>7594</th>\n      <td>ridiculously</td>\n      <td>0.558388</td>\n      <td>0.021</td>\n      <td>-0.065712</td>\n      <td>0.171225</td>\n      <td>7</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>12035</th>\n      <td>adeline</td>\n      <td>0.466286</td>\n      <td>0.035</td>\n      <td>-0.064648</td>\n      <td>0.145546</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26306</th>\n      <td>humus</td>\n      <td>0.268067</td>\n      <td>0.031</td>\n      <td>-0.064053</td>\n      <td>0.078338</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>18973</th>\n      <td>doc</td>\n      <td>0.439268</td>\n      <td>0.025</td>\n      <td>-0.062904</td>\n      <td>0.133788</td>\n      <td>5</td>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>12746</th>\n      <td>elias</td>\n      <td>0.615312</td>\n      <td>0.040</td>\n      <td>-0.059430</td>\n      <td>0.198627</td>\n      <td>13</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>9170</th>\n      <td>fruitfulness</td>\n      <td>0.413633</td>\n      <td>0.035</td>\n      <td>-0.057541</td>\n      <td>0.130364</td>\n      <td>13</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"cosine\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words changed the less by cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          word     lncs2  intersection_nn    cosine      mean  count_m1  \\\n3063        an  0.458082            0.107  0.924624  0.496568     20796   \n7649       who  0.809027            0.128  0.919161  0.618729     17530   \n8330      than  0.405342            0.123  0.908293  0.478878     12372   \n2103       new  0.464222            0.132  0.903266  0.499829      5012   \n26782     more  0.790212            0.149  0.901612  0.613608     14909   \n11294      far  0.791386            0.188  0.900840  0.626742      5100   \n21851     whom  0.829928            0.218  0.900759  0.649562      3449   \n3912      from  0.577680            0.102  0.900104  0.526594     32523   \n21599      eye  0.942752            0.330  0.898691  0.723814      6165   \n22107    there  0.622997            0.166  0.898492  0.562496     16028   \n9401      foot  0.961278            0.382  0.895145  0.746141      2843   \n25801     mile  0.853808            0.301  0.892059  0.682289      1313   \n7289   evening  0.969631            0.241  0.891401  0.700677      1271   \n20930     side  0.945507            0.373  0.891220  0.736576      3347   \n18743  face_nn  0.934642            0.355  0.891037  0.726893      3394   \n9670       sun  0.948069            0.365  0.890343  0.734470      1438   \n6221   between  0.559891            0.130  0.889165  0.526352      3235   \n15879       as  0.695247            0.132  0.888889  0.572045     55498   \n11007     into  0.931223            0.169  0.887388  0.662537     11346   \n14377     door  0.922105            0.317  0.887247  0.708784      2153   \n16932     give  0.935255            0.393  0.885133  0.737796      9486   \n6783   through  0.887455            0.222  0.884283  0.664579      5494   \n5040      inch  0.839905            0.286  0.882523  0.669476       499   \n20307    water  0.971430            0.397  0.882270  0.750233      2720   \n8571        on  0.781084            0.157  0.880777  0.606287     33481   \n26014     long  0.750341            0.171  0.879172  0.600171      6873   \n7122   o'clock  0.814487            0.180  0.878836  0.624441       288   \n12281    along  0.910810            0.276  0.878565  0.688458      1738   \n17024    least  0.718551            0.209  0.878515  0.602022      1998   \n15980     book  0.948878            0.271  0.878083  0.699320      1866   \n\n       count_m2  \n3063      22248  \n7649      16983  \n8330      10985  \n2103      10031  \n26782     14446  \n11294      3903  \n21851       918  \n3912      29181  \n21599      4951  \n22107     17609  \n9401       2650  \n25801      1390  \n7289        957  \n20930      3271  \n18743      3932  \n9670       1125  \n6221       4114  \n15879     45094  \n11007     13789  \n14377      3406  \n16932      7412  \n6783       6906  \n5040        634  \n20307      2924  \n8571      47478  \n26014      6531  \n7122        242  \n12281      2484  \n17024      2249  \n15980      2530  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>mean</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3063</th>\n      <td>an</td>\n      <td>0.458082</td>\n      <td>0.107</td>\n      <td>0.924624</td>\n      <td>0.496568</td>\n      <td>20796</td>\n      <td>22248</td>\n    </tr>\n    <tr>\n      <th>7649</th>\n      <td>who</td>\n      <td>0.809027</td>\n      <td>0.128</td>\n      <td>0.919161</td>\n      <td>0.618729</td>\n      <td>17530</td>\n      <td>16983</td>\n    </tr>\n    <tr>\n      <th>8330</th>\n      <td>than</td>\n      <td>0.405342</td>\n      <td>0.123</td>\n      <td>0.908293</td>\n      <td>0.478878</td>\n      <td>12372</td>\n      <td>10985</td>\n    </tr>\n    <tr>\n      <th>2103</th>\n      <td>new</td>\n      <td>0.464222</td>\n      <td>0.132</td>\n      <td>0.903266</td>\n      <td>0.499829</td>\n      <td>5012</td>\n      <td>10031</td>\n    </tr>\n    <tr>\n      <th>26782</th>\n      <td>more</td>\n      <td>0.790212</td>\n      <td>0.149</td>\n      <td>0.901612</td>\n      <td>0.613608</td>\n      <td>14909</td>\n      <td>14446</td>\n    </tr>\n    <tr>\n      <th>11294</th>\n      <td>far</td>\n      <td>0.791386</td>\n      <td>0.188</td>\n      <td>0.900840</td>\n      <td>0.626742</td>\n      <td>5100</td>\n      <td>3903</td>\n    </tr>\n    <tr>\n      <th>21851</th>\n      <td>whom</td>\n      <td>0.829928</td>\n      <td>0.218</td>\n      <td>0.900759</td>\n      <td>0.649562</td>\n      <td>3449</td>\n      <td>918</td>\n    </tr>\n    <tr>\n      <th>3912</th>\n      <td>from</td>\n      <td>0.577680</td>\n      <td>0.102</td>\n      <td>0.900104</td>\n      <td>0.526594</td>\n      <td>32523</td>\n      <td>29181</td>\n    </tr>\n    <tr>\n      <th>21599</th>\n      <td>eye</td>\n      <td>0.942752</td>\n      <td>0.330</td>\n      <td>0.898691</td>\n      <td>0.723814</td>\n      <td>6165</td>\n      <td>4951</td>\n    </tr>\n    <tr>\n      <th>22107</th>\n      <td>there</td>\n      <td>0.622997</td>\n      <td>0.166</td>\n      <td>0.898492</td>\n      <td>0.562496</td>\n      <td>16028</td>\n      <td>17609</td>\n    </tr>\n    <tr>\n      <th>9401</th>\n      <td>foot</td>\n      <td>0.961278</td>\n      <td>0.382</td>\n      <td>0.895145</td>\n      <td>0.746141</td>\n      <td>2843</td>\n      <td>2650</td>\n    </tr>\n    <tr>\n      <th>25801</th>\n      <td>mile</td>\n      <td>0.853808</td>\n      <td>0.301</td>\n      <td>0.892059</td>\n      <td>0.682289</td>\n      <td>1313</td>\n      <td>1390</td>\n    </tr>\n    <tr>\n      <th>7289</th>\n      <td>evening</td>\n      <td>0.969631</td>\n      <td>0.241</td>\n      <td>0.891401</td>\n      <td>0.700677</td>\n      <td>1271</td>\n      <td>957</td>\n    </tr>\n    <tr>\n      <th>20930</th>\n      <td>side</td>\n      <td>0.945507</td>\n      <td>0.373</td>\n      <td>0.891220</td>\n      <td>0.736576</td>\n      <td>3347</td>\n      <td>3271</td>\n    </tr>\n    <tr>\n      <th>18743</th>\n      <td>face_nn</td>\n      <td>0.934642</td>\n      <td>0.355</td>\n      <td>0.891037</td>\n      <td>0.726893</td>\n      <td>3394</td>\n      <td>3932</td>\n    </tr>\n    <tr>\n      <th>9670</th>\n      <td>sun</td>\n      <td>0.948069</td>\n      <td>0.365</td>\n      <td>0.890343</td>\n      <td>0.734470</td>\n      <td>1438</td>\n      <td>1125</td>\n    </tr>\n    <tr>\n      <th>6221</th>\n      <td>between</td>\n      <td>0.559891</td>\n      <td>0.130</td>\n      <td>0.889165</td>\n      <td>0.526352</td>\n      <td>3235</td>\n      <td>4114</td>\n    </tr>\n    <tr>\n      <th>15879</th>\n      <td>as</td>\n      <td>0.695247</td>\n      <td>0.132</td>\n      <td>0.888889</td>\n      <td>0.572045</td>\n      <td>55498</td>\n      <td>45094</td>\n    </tr>\n    <tr>\n      <th>11007</th>\n      <td>into</td>\n      <td>0.931223</td>\n      <td>0.169</td>\n      <td>0.887388</td>\n      <td>0.662537</td>\n      <td>11346</td>\n      <td>13789</td>\n    </tr>\n    <tr>\n      <th>14377</th>\n      <td>door</td>\n      <td>0.922105</td>\n      <td>0.317</td>\n      <td>0.887247</td>\n      <td>0.708784</td>\n      <td>2153</td>\n      <td>3406</td>\n    </tr>\n    <tr>\n      <th>16932</th>\n      <td>give</td>\n      <td>0.935255</td>\n      <td>0.393</td>\n      <td>0.885133</td>\n      <td>0.737796</td>\n      <td>9486</td>\n      <td>7412</td>\n    </tr>\n    <tr>\n      <th>6783</th>\n      <td>through</td>\n      <td>0.887455</td>\n      <td>0.222</td>\n      <td>0.884283</td>\n      <td>0.664579</td>\n      <td>5494</td>\n      <td>6906</td>\n    </tr>\n    <tr>\n      <th>5040</th>\n      <td>inch</td>\n      <td>0.839905</td>\n      <td>0.286</td>\n      <td>0.882523</td>\n      <td>0.669476</td>\n      <td>499</td>\n      <td>634</td>\n    </tr>\n    <tr>\n      <th>20307</th>\n      <td>water</td>\n      <td>0.971430</td>\n      <td>0.397</td>\n      <td>0.882270</td>\n      <td>0.750233</td>\n      <td>2720</td>\n      <td>2924</td>\n    </tr>\n    <tr>\n      <th>8571</th>\n      <td>on</td>\n      <td>0.781084</td>\n      <td>0.157</td>\n      <td>0.880777</td>\n      <td>0.606287</td>\n      <td>33481</td>\n      <td>47478</td>\n    </tr>\n    <tr>\n      <th>26014</th>\n      <td>long</td>\n      <td>0.750341</td>\n      <td>0.171</td>\n      <td>0.879172</td>\n      <td>0.600171</td>\n      <td>6873</td>\n      <td>6531</td>\n    </tr>\n    <tr>\n      <th>7122</th>\n      <td>o'clock</td>\n      <td>0.814487</td>\n      <td>0.180</td>\n      <td>0.878836</td>\n      <td>0.624441</td>\n      <td>288</td>\n      <td>242</td>\n    </tr>\n    <tr>\n      <th>12281</th>\n      <td>along</td>\n      <td>0.910810</td>\n      <td>0.276</td>\n      <td>0.878565</td>\n      <td>0.688458</td>\n      <td>1738</td>\n      <td>2484</td>\n    </tr>\n    <tr>\n      <th>17024</th>\n      <td>least</td>\n      <td>0.718551</td>\n      <td>0.209</td>\n      <td>0.878515</td>\n      <td>0.602022</td>\n      <td>1998</td>\n      <td>2249</td>\n    </tr>\n    <tr>\n      <th>15980</th>\n      <td>book</td>\n      <td>0.948878</td>\n      <td>0.271</td>\n      <td>0.878083</td>\n      <td>0.699320</td>\n      <td>1866</td>\n      <td>2530</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"cosine\"], ascending=False)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                   word     lncs2  intersection_nn    cosine      mean  \\\n18743           face_nn  0.934642            0.355  0.891037  0.726893   \n8987        chairman_nn  0.889095            0.181  0.664473  0.578189   \n2399             pin_vb  0.915103            0.407  0.667831  0.663311   \n18327           lane_nn  0.892045            0.287  0.693115  0.624053   \n3698            ball_nn  0.897295            0.308  0.696024  0.633773   \n23330          quilt_nn  0.907541            0.329  0.700923  0.645821   \n6690         fiction_nn  0.887914            0.290  0.713829  0.630581   \n23902  contemplation_nn  0.931850            0.325  0.717873  0.658241   \n11939           risk_nn  0.792147            0.216  0.729821  0.579323   \n12712          twist_nn  0.820299            0.148  0.564152  0.510817   \n7010          donkey_nn  0.945303            0.260  0.664242  0.623182   \n12070   relationship_nn  0.895997            0.302  0.735452  0.644483   \n26268            gas_nn  0.864054            0.229  0.743162  0.612072   \n8066          savage_nn  0.841278            0.208  0.562237  0.537172   \n2573          stroke_vb  0.907412            0.382  0.758184  0.682532   \n15700      multitude_nn  0.770127            0.135  0.549758  0.484962   \n20866            bag_nn  0.881313            0.349  0.791530  0.673948   \n15723          ounce_nn  0.867488            0.306  0.827537  0.667008   \n23226           word_nn  0.939014            0.348  0.840899  0.709304   \n13686           tree_nn  0.973066            0.431  0.871842  0.758636   \n8163            part_nn  0.819416            0.224  0.739295  0.594237   \n8736            lass_nn  0.822736            0.162  0.567552  0.517429   \n17424           stab_nn  0.846357            0.229  0.571190  0.548849   \n9463          record_nn  0.510405            0.117  0.489969  0.372458   \n5491           graft_nn  0.557973            0.077  0.425276  0.353416   \n18053            tip_vb  0.956252            0.367  0.609342  0.644198   \n19810          thump_nn  0.883925            0.317  0.705398  0.635441   \n19143         player_nn  0.689653            0.110  0.650410  0.483354   \n6793            prop_nn  0.714958            0.102  0.424872  0.413943   \n21987         circle_vb  0.887449            0.292  0.733982  0.637810   \n8880             rag_nn  0.958997            0.370  0.734575  0.687857   \n2633          attack_nn  0.866716            0.248  0.750544  0.621754   \n4053            edge_nn  0.965849            0.413  0.783036  0.720628   \n5371            head_nn  0.967564            0.391  0.839189  0.732584   \n6812            land_nn  0.885108            0.339  0.864635  0.696247   \n25591            bit_nn  0.895419            0.295  0.617770  0.602730   \n7563           plane_nn  0.474181            0.097  0.293152  0.288111   \n\n       count_m1  count_m2 truth  \n18743      3394      3932     0  \n8987        147       683     0  \n2399        114       217     0  \n18327       211       289     0  \n3698        440       878     0  \n23330       106       189     0  \n6690        202       326     0  \n23902       240       111     0  \n11939       286       643     0  \n12712       103       186     0  \n7010        118       148     0  \n12070       130       841     0  \n26268       155       680     0  \n8066        504       133     0  \n2573        110       227     0  \n15700       475       131     0  \n20866       214       899     0  \n15723       208       189     0  \n23226      4387      3166     0  \n13686      2322      1596     0  \n8163       4410      3213     0  \n8736        111       106     1  \n17424        92       117     1  \n9463        420      1188     1  \n5491        119       109     1  \n18053       119       241     1  \n19810        89       127     1  \n19143       132       939     1  \n6793        121       147     1  \n21987       131       245     1  \n8880        158       208     1  \n2633        454       833     1  \n4053        457      1072     1  \n5371       3599      4127     1  \n6812       2321      1624     1  \n25591       296       622     1  \n7563        278       792     1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>mean</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>truth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18743</th>\n      <td>face_nn</td>\n      <td>0.934642</td>\n      <td>0.355</td>\n      <td>0.891037</td>\n      <td>0.726893</td>\n      <td>3394</td>\n      <td>3932</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8987</th>\n      <td>chairman_nn</td>\n      <td>0.889095</td>\n      <td>0.181</td>\n      <td>0.664473</td>\n      <td>0.578189</td>\n      <td>147</td>\n      <td>683</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2399</th>\n      <td>pin_vb</td>\n      <td>0.915103</td>\n      <td>0.407</td>\n      <td>0.667831</td>\n      <td>0.663311</td>\n      <td>114</td>\n      <td>217</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18327</th>\n      <td>lane_nn</td>\n      <td>0.892045</td>\n      <td>0.287</td>\n      <td>0.693115</td>\n      <td>0.624053</td>\n      <td>211</td>\n      <td>289</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3698</th>\n      <td>ball_nn</td>\n      <td>0.897295</td>\n      <td>0.308</td>\n      <td>0.696024</td>\n      <td>0.633773</td>\n      <td>440</td>\n      <td>878</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23330</th>\n      <td>quilt_nn</td>\n      <td>0.907541</td>\n      <td>0.329</td>\n      <td>0.700923</td>\n      <td>0.645821</td>\n      <td>106</td>\n      <td>189</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6690</th>\n      <td>fiction_nn</td>\n      <td>0.887914</td>\n      <td>0.290</td>\n      <td>0.713829</td>\n      <td>0.630581</td>\n      <td>202</td>\n      <td>326</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23902</th>\n      <td>contemplation_nn</td>\n      <td>0.931850</td>\n      <td>0.325</td>\n      <td>0.717873</td>\n      <td>0.658241</td>\n      <td>240</td>\n      <td>111</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11939</th>\n      <td>risk_nn</td>\n      <td>0.792147</td>\n      <td>0.216</td>\n      <td>0.729821</td>\n      <td>0.579323</td>\n      <td>286</td>\n      <td>643</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12712</th>\n      <td>twist_nn</td>\n      <td>0.820299</td>\n      <td>0.148</td>\n      <td>0.564152</td>\n      <td>0.510817</td>\n      <td>103</td>\n      <td>186</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7010</th>\n      <td>donkey_nn</td>\n      <td>0.945303</td>\n      <td>0.260</td>\n      <td>0.664242</td>\n      <td>0.623182</td>\n      <td>118</td>\n      <td>148</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12070</th>\n      <td>relationship_nn</td>\n      <td>0.895997</td>\n      <td>0.302</td>\n      <td>0.735452</td>\n      <td>0.644483</td>\n      <td>130</td>\n      <td>841</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26268</th>\n      <td>gas_nn</td>\n      <td>0.864054</td>\n      <td>0.229</td>\n      <td>0.743162</td>\n      <td>0.612072</td>\n      <td>155</td>\n      <td>680</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8066</th>\n      <td>savage_nn</td>\n      <td>0.841278</td>\n      <td>0.208</td>\n      <td>0.562237</td>\n      <td>0.537172</td>\n      <td>504</td>\n      <td>133</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2573</th>\n      <td>stroke_vb</td>\n      <td>0.907412</td>\n      <td>0.382</td>\n      <td>0.758184</td>\n      <td>0.682532</td>\n      <td>110</td>\n      <td>227</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15700</th>\n      <td>multitude_nn</td>\n      <td>0.770127</td>\n      <td>0.135</td>\n      <td>0.549758</td>\n      <td>0.484962</td>\n      <td>475</td>\n      <td>131</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20866</th>\n      <td>bag_nn</td>\n      <td>0.881313</td>\n      <td>0.349</td>\n      <td>0.791530</td>\n      <td>0.673948</td>\n      <td>214</td>\n      <td>899</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15723</th>\n      <td>ounce_nn</td>\n      <td>0.867488</td>\n      <td>0.306</td>\n      <td>0.827537</td>\n      <td>0.667008</td>\n      <td>208</td>\n      <td>189</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23226</th>\n      <td>word_nn</td>\n      <td>0.939014</td>\n      <td>0.348</td>\n      <td>0.840899</td>\n      <td>0.709304</td>\n      <td>4387</td>\n      <td>3166</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13686</th>\n      <td>tree_nn</td>\n      <td>0.973066</td>\n      <td>0.431</td>\n      <td>0.871842</td>\n      <td>0.758636</td>\n      <td>2322</td>\n      <td>1596</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8163</th>\n      <td>part_nn</td>\n      <td>0.819416</td>\n      <td>0.224</td>\n      <td>0.739295</td>\n      <td>0.594237</td>\n      <td>4410</td>\n      <td>3213</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8736</th>\n      <td>lass_nn</td>\n      <td>0.822736</td>\n      <td>0.162</td>\n      <td>0.567552</td>\n      <td>0.517429</td>\n      <td>111</td>\n      <td>106</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17424</th>\n      <td>stab_nn</td>\n      <td>0.846357</td>\n      <td>0.229</td>\n      <td>0.571190</td>\n      <td>0.548849</td>\n      <td>92</td>\n      <td>117</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9463</th>\n      <td>record_nn</td>\n      <td>0.510405</td>\n      <td>0.117</td>\n      <td>0.489969</td>\n      <td>0.372458</td>\n      <td>420</td>\n      <td>1188</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5491</th>\n      <td>graft_nn</td>\n      <td>0.557973</td>\n      <td>0.077</td>\n      <td>0.425276</td>\n      <td>0.353416</td>\n      <td>119</td>\n      <td>109</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18053</th>\n      <td>tip_vb</td>\n      <td>0.956252</td>\n      <td>0.367</td>\n      <td>0.609342</td>\n      <td>0.644198</td>\n      <td>119</td>\n      <td>241</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19810</th>\n      <td>thump_nn</td>\n      <td>0.883925</td>\n      <td>0.317</td>\n      <td>0.705398</td>\n      <td>0.635441</td>\n      <td>89</td>\n      <td>127</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19143</th>\n      <td>player_nn</td>\n      <td>0.689653</td>\n      <td>0.110</td>\n      <td>0.650410</td>\n      <td>0.483354</td>\n      <td>132</td>\n      <td>939</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6793</th>\n      <td>prop_nn</td>\n      <td>0.714958</td>\n      <td>0.102</td>\n      <td>0.424872</td>\n      <td>0.413943</td>\n      <td>121</td>\n      <td>147</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21987</th>\n      <td>circle_vb</td>\n      <td>0.887449</td>\n      <td>0.292</td>\n      <td>0.733982</td>\n      <td>0.637810</td>\n      <td>131</td>\n      <td>245</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8880</th>\n      <td>rag_nn</td>\n      <td>0.958997</td>\n      <td>0.370</td>\n      <td>0.734575</td>\n      <td>0.687857</td>\n      <td>158</td>\n      <td>208</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2633</th>\n      <td>attack_nn</td>\n      <td>0.866716</td>\n      <td>0.248</td>\n      <td>0.750544</td>\n      <td>0.621754</td>\n      <td>454</td>\n      <td>833</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4053</th>\n      <td>edge_nn</td>\n      <td>0.965849</td>\n      <td>0.413</td>\n      <td>0.783036</td>\n      <td>0.720628</td>\n      <td>457</td>\n      <td>1072</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5371</th>\n      <td>head_nn</td>\n      <td>0.967564</td>\n      <td>0.391</td>\n      <td>0.839189</td>\n      <td>0.732584</td>\n      <td>3599</td>\n      <td>4127</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6812</th>\n      <td>land_nn</td>\n      <td>0.885108</td>\n      <td>0.339</td>\n      <td>0.864635</td>\n      <td>0.696247</td>\n      <td>2321</td>\n      <td>1624</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25591</th>\n      <td>bit_nn</td>\n      <td>0.895419</td>\n      <td>0.295</td>\n      <td>0.617770</td>\n      <td>0.602730</td>\n      <td>296</td>\n      <td>622</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7563</th>\n      <td>plane_nn</td>\n      <td>0.474181</td>\n      <td>0.097</td>\n      <td>0.293152</td>\n      <td>0.288111</td>\n      <td>278</td>\n      <td>792</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "gt = get_gt(\"english\")\n",
    "gt_vocabulary_df = shared_vocabulary_df[shared_vocabulary_df[\"word\"].isin(gt[:, 0])]\n",
    "gt_vocabulary_df[\"truth\"] = gt_vocabulary_df[\"word\"].apply(\n",
    "    lambda x: gt[numpy.where(gt[:, 0] == x), 1][0,0]\n",
    ")\n",
    "gt_vocabulary_df = gt_vocabulary_df.sort_values(by=[\"truth\"])\n",
    "gt_vocabulary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594681239754",
   "display_name": "Python 3.8.3 64-bit ('thesis': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}