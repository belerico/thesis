{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from cade.metrics.comparative import lncs2, intersection_nn, initialize_avgs, get_neighbors_set\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from pandas import pandas\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "from tabulate import tabulate\n",
    "from config import CURRENT_EXP_DIR, config, get_logger, log_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load language models and groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(lang: str):\n",
    "    model1 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus1.model\"\n",
    "    )\n",
    "    model2 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus2.model\"\n",
    "    )\n",
    "    return model1, model2\n",
    "\n",
    "def get_gt(lang: str, binary=True):\n",
    "    binary_truth = numpy.loadtxt(\n",
    "        \"./data/\"\n",
    "        + lang\n",
    "        + \"/semeval2020_ulscd_\"\n",
    "        + lang[:3]\n",
    "        + \"/truth/\" + (\"binary\" if binary else \"graded\") + \".txt\",\n",
    "        dtype=str,\n",
    "        delimiter=\"\\t\",\n",
    "    )\n",
    "    return binary_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English (get LNCS2, Intersection_NN and Cosine scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang = \"english\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Initialize models avgs\n",
    "initialize_avgs(model1, model2)\n",
    "shared_vocabulary = set(model1.wv.vocab.keys()).intersection(set(model2.wv.vocab.keys()))\n",
    "shared_vocabulary_df = pandas.DataFrame(shared_vocabulary, columns=[\"word\"])\n",
    "shared_vocabulary_df[\"lncs2\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: lncs2(word, model1, model2, 25)\n",
    ")\n",
    "shared_vocabulary_df[\"intersection_nn\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: intersection_nn(word, model1, model2)\n",
    ")\n",
    "shared_vocabulary_df[\"cosine\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: 1 - cosine(model1.wv[word], model2.wv[word])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add mean of the three metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shared_vocabulary_df[\"mean\"] = shared_vocabulary_df[[\"lncs2\", \"cosine\", \"intersection_nn\"]].apply(\n",
    "    lambda x: (x.lncs2 + x.cosine + x.intersection_nn) / 3, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocabulary_df[\"count_m1\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: model1.wv.vocab[word].count\n",
    ")\n",
    "shared_vocabulary_df[\"count_m2\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: model2.wv.vocab[word].count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocabulary_df.to_pickle(\"./shared_vocabulary_metrics.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words that changed the most (by LNCS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n10661       pregnant -0.059092            0.988 -0.001763        31       158   \n24935         unused -0.059081            0.987 -0.073233        26        36   \n20337         scarce -0.044323            0.990  0.072006       329        61   \n20078        incline -0.034136            0.988  0.176630       114        25   \n6585              ml -0.029649            0.998 -0.079436         1        18   \n4671           major -0.016591            0.998  0.263108       531      1554   \n5073         someday  0.002855            0.988  0.115850         1       125   \n16236       mentally  0.016916            0.980  0.328319        31        90   \n147            tense  0.032603            0.984  0.138092        11       139   \n19984            err  0.036362            0.974  0.169528        70        11   \n15005        someone  0.045325            0.985  0.050252         3      1589   \n16315        backing  0.045896            0.987 -0.038172         5        42   \n18442          twain  0.055887            0.998  0.011805        29        31   \n26316       juvenile  0.056204            0.974  0.037996        32        56   \n23333             hy  0.056843            0.992  0.008580       109        11   \n7053   significantly  0.058707            0.998 -0.010645        31       123   \n20197          today  0.063035            0.983  0.364674       402      1973   \n10305        hearing  0.064546            0.990 -0.008649         1       149   \n17195         some--  0.068811            0.964 -0.110117         2         6   \n20128        drastic  0.069603            0.993  0.058760         3        48   \n17361     detachment  0.070652            0.993  0.089520        77        44   \n11084     deflection  0.071443            0.983  0.267019        15         3   \n21890        discard  0.078995            0.995  0.215280        57       109   \n16451      cheapness  0.079505            0.984  0.070801        17         4   \n16344        roughly  0.084750            0.997  0.068283        50       182   \n23643     conveyance  0.085479            0.985  0.159299        44         4   \n10956         boring  0.089093            0.991 -0.009177         8        77   \n12508          check  0.091327            0.979  0.178886       367      1028   \n11922    frightening  0.095688            0.992 -0.102361         3        76   \n18608        burthen  0.101387            0.981  0.162739        49         1   \n\n           mean  \n10661  0.309048  \n24935  0.284895  \n20337  0.339228  \n20078  0.376831  \n6585   0.296305  \n4671   0.414839  \n5073   0.368902  \n16236  0.441745  \n147    0.384898  \n19984  0.393297  \n15005  0.360192  \n16315  0.331575  \n18442  0.355231  \n26316  0.356067  \n23333  0.352474  \n7053   0.348688  \n20197  0.470237  \n10305  0.348632  \n17195  0.307565  \n20128  0.373788  \n17361  0.384391  \n11084  0.440487  \n21890  0.429758  \n16451  0.378102  \n16344  0.383344  \n23643  0.409926  \n10956  0.356972  \n12508  0.416404  \n11922  0.328442  \n18608  0.415042  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10661</th>\n      <td>pregnant</td>\n      <td>-0.059092</td>\n      <td>0.988</td>\n      <td>-0.001763</td>\n      <td>31</td>\n      <td>158</td>\n      <td>0.309048</td>\n    </tr>\n    <tr>\n      <th>24935</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.987</td>\n      <td>-0.073233</td>\n      <td>26</td>\n      <td>36</td>\n      <td>0.284895</td>\n    </tr>\n    <tr>\n      <th>20337</th>\n      <td>scarce</td>\n      <td>-0.044323</td>\n      <td>0.990</td>\n      <td>0.072006</td>\n      <td>329</td>\n      <td>61</td>\n      <td>0.339228</td>\n    </tr>\n    <tr>\n      <th>20078</th>\n      <td>incline</td>\n      <td>-0.034136</td>\n      <td>0.988</td>\n      <td>0.176630</td>\n      <td>114</td>\n      <td>25</td>\n      <td>0.376831</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>4671</th>\n      <td>major</td>\n      <td>-0.016591</td>\n      <td>0.998</td>\n      <td>0.263108</td>\n      <td>531</td>\n      <td>1554</td>\n      <td>0.414839</td>\n    </tr>\n    <tr>\n      <th>5073</th>\n      <td>someday</td>\n      <td>0.002855</td>\n      <td>0.988</td>\n      <td>0.115850</td>\n      <td>1</td>\n      <td>125</td>\n      <td>0.368902</td>\n    </tr>\n    <tr>\n      <th>16236</th>\n      <td>mentally</td>\n      <td>0.016916</td>\n      <td>0.980</td>\n      <td>0.328319</td>\n      <td>31</td>\n      <td>90</td>\n      <td>0.441745</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>tense</td>\n      <td>0.032603</td>\n      <td>0.984</td>\n      <td>0.138092</td>\n      <td>11</td>\n      <td>139</td>\n      <td>0.384898</td>\n    </tr>\n    <tr>\n      <th>19984</th>\n      <td>err</td>\n      <td>0.036362</td>\n      <td>0.974</td>\n      <td>0.169528</td>\n      <td>70</td>\n      <td>11</td>\n      <td>0.393297</td>\n    </tr>\n    <tr>\n      <th>15005</th>\n      <td>someone</td>\n      <td>0.045325</td>\n      <td>0.985</td>\n      <td>0.050252</td>\n      <td>3</td>\n      <td>1589</td>\n      <td>0.360192</td>\n    </tr>\n    <tr>\n      <th>16315</th>\n      <td>backing</td>\n      <td>0.045896</td>\n      <td>0.987</td>\n      <td>-0.038172</td>\n      <td>5</td>\n      <td>42</td>\n      <td>0.331575</td>\n    </tr>\n    <tr>\n      <th>18442</th>\n      <td>twain</td>\n      <td>0.055887</td>\n      <td>0.998</td>\n      <td>0.011805</td>\n      <td>29</td>\n      <td>31</td>\n      <td>0.355231</td>\n    </tr>\n    <tr>\n      <th>26316</th>\n      <td>juvenile</td>\n      <td>0.056204</td>\n      <td>0.974</td>\n      <td>0.037996</td>\n      <td>32</td>\n      <td>56</td>\n      <td>0.356067</td>\n    </tr>\n    <tr>\n      <th>23333</th>\n      <td>hy</td>\n      <td>0.056843</td>\n      <td>0.992</td>\n      <td>0.008580</td>\n      <td>109</td>\n      <td>11</td>\n      <td>0.352474</td>\n    </tr>\n    <tr>\n      <th>7053</th>\n      <td>significantly</td>\n      <td>0.058707</td>\n      <td>0.998</td>\n      <td>-0.010645</td>\n      <td>31</td>\n      <td>123</td>\n      <td>0.348688</td>\n    </tr>\n    <tr>\n      <th>20197</th>\n      <td>today</td>\n      <td>0.063035</td>\n      <td>0.983</td>\n      <td>0.364674</td>\n      <td>402</td>\n      <td>1973</td>\n      <td>0.470237</td>\n    </tr>\n    <tr>\n      <th>10305</th>\n      <td>hearing</td>\n      <td>0.064546</td>\n      <td>0.990</td>\n      <td>-0.008649</td>\n      <td>1</td>\n      <td>149</td>\n      <td>0.348632</td>\n    </tr>\n    <tr>\n      <th>17195</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.964</td>\n      <td>-0.110117</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.307565</td>\n    </tr>\n    <tr>\n      <th>20128</th>\n      <td>drastic</td>\n      <td>0.069603</td>\n      <td>0.993</td>\n      <td>0.058760</td>\n      <td>3</td>\n      <td>48</td>\n      <td>0.373788</td>\n    </tr>\n    <tr>\n      <th>17361</th>\n      <td>detachment</td>\n      <td>0.070652</td>\n      <td>0.993</td>\n      <td>0.089520</td>\n      <td>77</td>\n      <td>44</td>\n      <td>0.384391</td>\n    </tr>\n    <tr>\n      <th>11084</th>\n      <td>deflection</td>\n      <td>0.071443</td>\n      <td>0.983</td>\n      <td>0.267019</td>\n      <td>15</td>\n      <td>3</td>\n      <td>0.440487</td>\n    </tr>\n    <tr>\n      <th>21890</th>\n      <td>discard</td>\n      <td>0.078995</td>\n      <td>0.995</td>\n      <td>0.215280</td>\n      <td>57</td>\n      <td>109</td>\n      <td>0.429758</td>\n    </tr>\n    <tr>\n      <th>16451</th>\n      <td>cheapness</td>\n      <td>0.079505</td>\n      <td>0.984</td>\n      <td>0.070801</td>\n      <td>17</td>\n      <td>4</td>\n      <td>0.378102</td>\n    </tr>\n    <tr>\n      <th>16344</th>\n      <td>roughly</td>\n      <td>0.084750</td>\n      <td>0.997</td>\n      <td>0.068283</td>\n      <td>50</td>\n      <td>182</td>\n      <td>0.383344</td>\n    </tr>\n    <tr>\n      <th>23643</th>\n      <td>conveyance</td>\n      <td>0.085479</td>\n      <td>0.985</td>\n      <td>0.159299</td>\n      <td>44</td>\n      <td>4</td>\n      <td>0.409926</td>\n    </tr>\n    <tr>\n      <th>10956</th>\n      <td>boring</td>\n      <td>0.089093</td>\n      <td>0.991</td>\n      <td>-0.009177</td>\n      <td>8</td>\n      <td>77</td>\n      <td>0.356972</td>\n    </tr>\n    <tr>\n      <th>12508</th>\n      <td>check</td>\n      <td>0.091327</td>\n      <td>0.979</td>\n      <td>0.178886</td>\n      <td>367</td>\n      <td>1028</td>\n      <td>0.416404</td>\n    </tr>\n    <tr>\n      <th>11922</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.992</td>\n      <td>-0.102361</td>\n      <td>3</td>\n      <td>76</td>\n      <td>0.328442</td>\n    </tr>\n    <tr>\n      <th>18608</th>\n      <td>burthen</td>\n      <td>0.101387</td>\n      <td>0.981</td>\n      <td>0.162739</td>\n      <td>49</td>\n      <td>1</td>\n      <td>0.415042</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"lncs2\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by Intersection_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n4043           virus  0.323960            1.000 -0.149479        13       166   \n21585     clumsiness  0.552998            0.999  0.532715         1         4   \n17746      rusticity  0.613301            0.999  0.544417         3         2   \n23853        funding  0.344045            0.999  0.151782         3       119   \n7452            coke  0.272922            0.999  0.071755        12        88   \n22296       uprising  0.307683            0.998  0.156027        10        39   \n17630           aura  0.217044            0.998 -0.075655         9        52   \n4266           media  0.323505            0.998  0.092881        49       394   \n25496           rove  0.151496            0.998  0.073780        48        19   \n18442          twain  0.055887            0.998  0.011805        29        31   \n6585              ml -0.029649            0.998 -0.079436         1        18   \n4671           major -0.016591            0.998  0.263108       531      1554   \n25637      projector  0.393249            0.998 -0.094298        18        33   \n9065       inclusive  0.339527            0.998  0.072305        13        21   \n25410           lacy  0.361720            0.998  0.061578         7        20   \n7053   significantly  0.058707            0.998 -0.010645        31       123   \n16344        roughly  0.084750            0.997  0.068283        50       182   \n381            teeny  0.563224            0.997  0.144121         2        10   \n20932         psyche  0.215099            0.996  0.096274         7        35   \n18791           gage  0.496162            0.996  0.145964        42        21   \n8811         spinoza  0.646836            0.996  0.591190         2         3   \n2624            vail  0.317838            0.996 -0.031890        16        34   \n19380          whiff  0.406077            0.996  0.234451        12        28   \n13000          trump  0.340584            0.996  0.167791        53        41   \n26630        clarify  0.141261            0.996  0.063697         5        50   \n21436     cavalierly  0.700440            0.996  0.305752         3         3   \n14062          xx_jj  0.570590            0.995  0.499882         3         3   \n21890        discard  0.078995            0.995  0.215280        57       109   \n12087   recuperation  0.590249            0.995  0.372132         2         5   \n6130            barb  0.414557            0.995  0.044461         9        33   \n\n           mean  \n4043   0.391494  \n21585  0.694905  \n17746  0.718906  \n23853  0.498276  \n7452   0.447892  \n22296  0.487237  \n17630  0.379796  \n4266   0.471462  \n25496  0.407758  \n18442  0.355231  \n6585   0.296305  \n4671   0.414839  \n25637  0.432317  \n9065   0.469944  \n25410  0.473766  \n7053   0.348688  \n16344  0.383344  \n381    0.568115  \n20932  0.435791  \n18791  0.546042  \n8811   0.744675  \n2624   0.427316  \n19380  0.545509  \n13000  0.501458  \n26630  0.400319  \n21436  0.667397  \n14062  0.688491  \n21890  0.429758  \n12087  0.652460  \n6130   0.484673  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4043</th>\n      <td>virus</td>\n      <td>0.323960</td>\n      <td>1.000</td>\n      <td>-0.149479</td>\n      <td>13</td>\n      <td>166</td>\n      <td>0.391494</td>\n    </tr>\n    <tr>\n      <th>21585</th>\n      <td>clumsiness</td>\n      <td>0.552998</td>\n      <td>0.999</td>\n      <td>0.532715</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0.694905</td>\n    </tr>\n    <tr>\n      <th>17746</th>\n      <td>rusticity</td>\n      <td>0.613301</td>\n      <td>0.999</td>\n      <td>0.544417</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0.718906</td>\n    </tr>\n    <tr>\n      <th>23853</th>\n      <td>funding</td>\n      <td>0.344045</td>\n      <td>0.999</td>\n      <td>0.151782</td>\n      <td>3</td>\n      <td>119</td>\n      <td>0.498276</td>\n    </tr>\n    <tr>\n      <th>7452</th>\n      <td>coke</td>\n      <td>0.272922</td>\n      <td>0.999</td>\n      <td>0.071755</td>\n      <td>12</td>\n      <td>88</td>\n      <td>0.447892</td>\n    </tr>\n    <tr>\n      <th>22296</th>\n      <td>uprising</td>\n      <td>0.307683</td>\n      <td>0.998</td>\n      <td>0.156027</td>\n      <td>10</td>\n      <td>39</td>\n      <td>0.487237</td>\n    </tr>\n    <tr>\n      <th>17630</th>\n      <td>aura</td>\n      <td>0.217044</td>\n      <td>0.998</td>\n      <td>-0.075655</td>\n      <td>9</td>\n      <td>52</td>\n      <td>0.379796</td>\n    </tr>\n    <tr>\n      <th>4266</th>\n      <td>media</td>\n      <td>0.323505</td>\n      <td>0.998</td>\n      <td>0.092881</td>\n      <td>49</td>\n      <td>394</td>\n      <td>0.471462</td>\n    </tr>\n    <tr>\n      <th>25496</th>\n      <td>rove</td>\n      <td>0.151496</td>\n      <td>0.998</td>\n      <td>0.073780</td>\n      <td>48</td>\n      <td>19</td>\n      <td>0.407758</td>\n    </tr>\n    <tr>\n      <th>18442</th>\n      <td>twain</td>\n      <td>0.055887</td>\n      <td>0.998</td>\n      <td>0.011805</td>\n      <td>29</td>\n      <td>31</td>\n      <td>0.355231</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>4671</th>\n      <td>major</td>\n      <td>-0.016591</td>\n      <td>0.998</td>\n      <td>0.263108</td>\n      <td>531</td>\n      <td>1554</td>\n      <td>0.414839</td>\n    </tr>\n    <tr>\n      <th>25637</th>\n      <td>projector</td>\n      <td>0.393249</td>\n      <td>0.998</td>\n      <td>-0.094298</td>\n      <td>18</td>\n      <td>33</td>\n      <td>0.432317</td>\n    </tr>\n    <tr>\n      <th>9065</th>\n      <td>inclusive</td>\n      <td>0.339527</td>\n      <td>0.998</td>\n      <td>0.072305</td>\n      <td>13</td>\n      <td>21</td>\n      <td>0.469944</td>\n    </tr>\n    <tr>\n      <th>25410</th>\n      <td>lacy</td>\n      <td>0.361720</td>\n      <td>0.998</td>\n      <td>0.061578</td>\n      <td>7</td>\n      <td>20</td>\n      <td>0.473766</td>\n    </tr>\n    <tr>\n      <th>7053</th>\n      <td>significantly</td>\n      <td>0.058707</td>\n      <td>0.998</td>\n      <td>-0.010645</td>\n      <td>31</td>\n      <td>123</td>\n      <td>0.348688</td>\n    </tr>\n    <tr>\n      <th>16344</th>\n      <td>roughly</td>\n      <td>0.084750</td>\n      <td>0.997</td>\n      <td>0.068283</td>\n      <td>50</td>\n      <td>182</td>\n      <td>0.383344</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>teeny</td>\n      <td>0.563224</td>\n      <td>0.997</td>\n      <td>0.144121</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0.568115</td>\n    </tr>\n    <tr>\n      <th>20932</th>\n      <td>psyche</td>\n      <td>0.215099</td>\n      <td>0.996</td>\n      <td>0.096274</td>\n      <td>7</td>\n      <td>35</td>\n      <td>0.435791</td>\n    </tr>\n    <tr>\n      <th>18791</th>\n      <td>gage</td>\n      <td>0.496162</td>\n      <td>0.996</td>\n      <td>0.145964</td>\n      <td>42</td>\n      <td>21</td>\n      <td>0.546042</td>\n    </tr>\n    <tr>\n      <th>8811</th>\n      <td>spinoza</td>\n      <td>0.646836</td>\n      <td>0.996</td>\n      <td>0.591190</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0.744675</td>\n    </tr>\n    <tr>\n      <th>2624</th>\n      <td>vail</td>\n      <td>0.317838</td>\n      <td>0.996</td>\n      <td>-0.031890</td>\n      <td>16</td>\n      <td>34</td>\n      <td>0.427316</td>\n    </tr>\n    <tr>\n      <th>19380</th>\n      <td>whiff</td>\n      <td>0.406077</td>\n      <td>0.996</td>\n      <td>0.234451</td>\n      <td>12</td>\n      <td>28</td>\n      <td>0.545509</td>\n    </tr>\n    <tr>\n      <th>13000</th>\n      <td>trump</td>\n      <td>0.340584</td>\n      <td>0.996</td>\n      <td>0.167791</td>\n      <td>53</td>\n      <td>41</td>\n      <td>0.501458</td>\n    </tr>\n    <tr>\n      <th>26630</th>\n      <td>clarify</td>\n      <td>0.141261</td>\n      <td>0.996</td>\n      <td>0.063697</td>\n      <td>5</td>\n      <td>50</td>\n      <td>0.400319</td>\n    </tr>\n    <tr>\n      <th>21436</th>\n      <td>cavalierly</td>\n      <td>0.700440</td>\n      <td>0.996</td>\n      <td>0.305752</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.667397</td>\n    </tr>\n    <tr>\n      <th>14062</th>\n      <td>xx_jj</td>\n      <td>0.570590</td>\n      <td>0.995</td>\n      <td>0.499882</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.688491</td>\n    </tr>\n    <tr>\n      <th>21890</th>\n      <td>discard</td>\n      <td>0.078995</td>\n      <td>0.995</td>\n      <td>0.215280</td>\n      <td>57</td>\n      <td>109</td>\n      <td>0.429758</td>\n    </tr>\n    <tr>\n      <th>12087</th>\n      <td>recuperation</td>\n      <td>0.590249</td>\n      <td>0.995</td>\n      <td>0.372132</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0.652460</td>\n    </tr>\n    <tr>\n      <th>6130</th>\n      <td>barb</td>\n      <td>0.414557</td>\n      <td>0.995</td>\n      <td>0.044461</td>\n      <td>9</td>\n      <td>33</td>\n      <td>0.484673</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"intersection_nn\"], ascending=False)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n16205           ski  0.287075            0.983 -0.166819         2       218   \n4043          virus  0.323960            1.000 -0.149479        13       166   \n7259            fer  0.259189            0.966 -0.132545         4        47   \n18503            op  0.695739            0.974 -0.127812        21        31   \n17938        wilmer  0.500615            0.950 -0.111061        11        16   \n17195        some--  0.068811            0.964 -0.110117         2         6   \n1684        setting  0.200999            0.991 -0.109949         1        62   \n11922   frightening  0.095688            0.992 -0.102361         3        76   \n15049          gist  0.272670            0.994 -0.098656        15        15   \n17813           cal  0.479762            0.980 -0.098061        13        66   \n5256     intriguing  0.198948            0.970 -0.095689        13        43   \n25637     projector  0.393249            0.998 -0.094298        18        33   \n23774           int  0.534314            0.963 -0.084740         5        77   \n449             cir  0.372242            0.981 -0.082652        10         1   \n5775         scurvy  0.292371            0.986 -0.081057         5         6   \n8812          flume  0.600722            0.972 -0.080179         6         1   \n6585             ml -0.029649            0.998 -0.079436         1        18   \n2193             uc  0.517708            0.985 -0.077761         1        15   \n17630          aura  0.217044            0.998 -0.075655         9        52   \n17288         ethel  0.413615            0.974 -0.075074         1        40   \n24935        unused -0.059081            0.987 -0.073233        26        36   \n9359          lotus  0.677376            0.971 -0.069215         2        22   \n4905        onwards  0.414718            0.968 -0.068938        11         9   \n20737        walden  0.373120            0.990 -0.068412        12         8   \n8234   ridiculously  0.558388            0.979 -0.065712         7        14   \n14593       adeline  0.466286            0.965 -0.064648        32         1   \n2159          humus  0.268067            0.969 -0.064053         1        10   \n2587            doc  0.439268            0.975 -0.062904         5       107   \n21108         elias  0.615312            0.960 -0.059430        13        37   \n24062  fruitfulness  0.413633            0.965 -0.057541        13         2   \n\n           mean  \n16205  0.367752  \n4043   0.391494  \n7259   0.364215  \n18503  0.513976  \n17938  0.446518  \n17195  0.307565  \n1684   0.360683  \n11922  0.328442  \n15049  0.389338  \n17813  0.453900  \n5256   0.357753  \n25637  0.432317  \n23774  0.470858  \n449    0.423530  \n5775   0.399105  \n8812   0.497514  \n6585   0.296305  \n2193   0.474982  \n17630  0.379796  \n17288  0.437514  \n24935  0.284895  \n9359   0.526387  \n4905   0.437927  \n20737  0.431570  \n8234   0.490559  \n14593  0.455546  \n2159   0.391005  \n2587   0.450455  \n21108  0.505294  \n24062  0.440364  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16205</th>\n      <td>ski</td>\n      <td>0.287075</td>\n      <td>0.983</td>\n      <td>-0.166819</td>\n      <td>2</td>\n      <td>218</td>\n      <td>0.367752</td>\n    </tr>\n    <tr>\n      <th>4043</th>\n      <td>virus</td>\n      <td>0.323960</td>\n      <td>1.000</td>\n      <td>-0.149479</td>\n      <td>13</td>\n      <td>166</td>\n      <td>0.391494</td>\n    </tr>\n    <tr>\n      <th>7259</th>\n      <td>fer</td>\n      <td>0.259189</td>\n      <td>0.966</td>\n      <td>-0.132545</td>\n      <td>4</td>\n      <td>47</td>\n      <td>0.364215</td>\n    </tr>\n    <tr>\n      <th>18503</th>\n      <td>op</td>\n      <td>0.695739</td>\n      <td>0.974</td>\n      <td>-0.127812</td>\n      <td>21</td>\n      <td>31</td>\n      <td>0.513976</td>\n    </tr>\n    <tr>\n      <th>17938</th>\n      <td>wilmer</td>\n      <td>0.500615</td>\n      <td>0.950</td>\n      <td>-0.111061</td>\n      <td>11</td>\n      <td>16</td>\n      <td>0.446518</td>\n    </tr>\n    <tr>\n      <th>17195</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.964</td>\n      <td>-0.110117</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.307565</td>\n    </tr>\n    <tr>\n      <th>1684</th>\n      <td>setting</td>\n      <td>0.200999</td>\n      <td>0.991</td>\n      <td>-0.109949</td>\n      <td>1</td>\n      <td>62</td>\n      <td>0.360683</td>\n    </tr>\n    <tr>\n      <th>11922</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.992</td>\n      <td>-0.102361</td>\n      <td>3</td>\n      <td>76</td>\n      <td>0.328442</td>\n    </tr>\n    <tr>\n      <th>15049</th>\n      <td>gist</td>\n      <td>0.272670</td>\n      <td>0.994</td>\n      <td>-0.098656</td>\n      <td>15</td>\n      <td>15</td>\n      <td>0.389338</td>\n    </tr>\n    <tr>\n      <th>17813</th>\n      <td>cal</td>\n      <td>0.479762</td>\n      <td>0.980</td>\n      <td>-0.098061</td>\n      <td>13</td>\n      <td>66</td>\n      <td>0.453900</td>\n    </tr>\n    <tr>\n      <th>5256</th>\n      <td>intriguing</td>\n      <td>0.198948</td>\n      <td>0.970</td>\n      <td>-0.095689</td>\n      <td>13</td>\n      <td>43</td>\n      <td>0.357753</td>\n    </tr>\n    <tr>\n      <th>25637</th>\n      <td>projector</td>\n      <td>0.393249</td>\n      <td>0.998</td>\n      <td>-0.094298</td>\n      <td>18</td>\n      <td>33</td>\n      <td>0.432317</td>\n    </tr>\n    <tr>\n      <th>23774</th>\n      <td>int</td>\n      <td>0.534314</td>\n      <td>0.963</td>\n      <td>-0.084740</td>\n      <td>5</td>\n      <td>77</td>\n      <td>0.470858</td>\n    </tr>\n    <tr>\n      <th>449</th>\n      <td>cir</td>\n      <td>0.372242</td>\n      <td>0.981</td>\n      <td>-0.082652</td>\n      <td>10</td>\n      <td>1</td>\n      <td>0.423530</td>\n    </tr>\n    <tr>\n      <th>5775</th>\n      <td>scurvy</td>\n      <td>0.292371</td>\n      <td>0.986</td>\n      <td>-0.081057</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0.399105</td>\n    </tr>\n    <tr>\n      <th>8812</th>\n      <td>flume</td>\n      <td>0.600722</td>\n      <td>0.972</td>\n      <td>-0.080179</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.497514</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>2193</th>\n      <td>uc</td>\n      <td>0.517708</td>\n      <td>0.985</td>\n      <td>-0.077761</td>\n      <td>1</td>\n      <td>15</td>\n      <td>0.474982</td>\n    </tr>\n    <tr>\n      <th>17630</th>\n      <td>aura</td>\n      <td>0.217044</td>\n      <td>0.998</td>\n      <td>-0.075655</td>\n      <td>9</td>\n      <td>52</td>\n      <td>0.379796</td>\n    </tr>\n    <tr>\n      <th>17288</th>\n      <td>ethel</td>\n      <td>0.413615</td>\n      <td>0.974</td>\n      <td>-0.075074</td>\n      <td>1</td>\n      <td>40</td>\n      <td>0.437514</td>\n    </tr>\n    <tr>\n      <th>24935</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.987</td>\n      <td>-0.073233</td>\n      <td>26</td>\n      <td>36</td>\n      <td>0.284895</td>\n    </tr>\n    <tr>\n      <th>9359</th>\n      <td>lotus</td>\n      <td>0.677376</td>\n      <td>0.971</td>\n      <td>-0.069215</td>\n      <td>2</td>\n      <td>22</td>\n      <td>0.526387</td>\n    </tr>\n    <tr>\n      <th>4905</th>\n      <td>onwards</td>\n      <td>0.414718</td>\n      <td>0.968</td>\n      <td>-0.068938</td>\n      <td>11</td>\n      <td>9</td>\n      <td>0.437927</td>\n    </tr>\n    <tr>\n      <th>20737</th>\n      <td>walden</td>\n      <td>0.373120</td>\n      <td>0.990</td>\n      <td>-0.068412</td>\n      <td>12</td>\n      <td>8</td>\n      <td>0.431570</td>\n    </tr>\n    <tr>\n      <th>8234</th>\n      <td>ridiculously</td>\n      <td>0.558388</td>\n      <td>0.979</td>\n      <td>-0.065712</td>\n      <td>7</td>\n      <td>14</td>\n      <td>0.490559</td>\n    </tr>\n    <tr>\n      <th>14593</th>\n      <td>adeline</td>\n      <td>0.466286</td>\n      <td>0.965</td>\n      <td>-0.064648</td>\n      <td>32</td>\n      <td>1</td>\n      <td>0.455546</td>\n    </tr>\n    <tr>\n      <th>2159</th>\n      <td>humus</td>\n      <td>0.268067</td>\n      <td>0.969</td>\n      <td>-0.064053</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.391005</td>\n    </tr>\n    <tr>\n      <th>2587</th>\n      <td>doc</td>\n      <td>0.439268</td>\n      <td>0.975</td>\n      <td>-0.062904</td>\n      <td>5</td>\n      <td>107</td>\n      <td>0.450455</td>\n    </tr>\n    <tr>\n      <th>21108</th>\n      <td>elias</td>\n      <td>0.615312</td>\n      <td>0.960</td>\n      <td>-0.059430</td>\n      <td>13</td>\n      <td>37</td>\n      <td>0.505294</td>\n    </tr>\n    <tr>\n      <th>24062</th>\n      <td>fruitfulness</td>\n      <td>0.413633</td>\n      <td>0.965</td>\n      <td>-0.057541</td>\n      <td>13</td>\n      <td>2</td>\n      <td>0.440364</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"cosine\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 word     lncs2  intersection_nn    cosine  count_m1  \\\n24935          unused -0.059081            0.987 -0.073233        26   \n6585               ml -0.029649            0.998 -0.079436         1   \n17195          some--  0.068811            0.964 -0.110117         2   \n10661        pregnant -0.059092            0.988 -0.001763        31   \n11922     frightening  0.095688            0.992 -0.102361         3   \n16315         backing  0.045896            0.987 -0.038172         5   \n20337          scarce -0.044323            0.990  0.072006       329   \n10305         hearing  0.064546            0.990 -0.008649         1   \n7053    significantly  0.058707            0.998 -0.010645        31   \n23333              hy  0.056843            0.992  0.008580       109   \n18442           twain  0.055887            0.998  0.011805        29   \n26316        juvenile  0.056204            0.974  0.037996        32   \n10956          boring  0.089093            0.991 -0.009177         8   \n5256       intriguing  0.198948            0.970 -0.095689        13   \n15005         someone  0.045325            0.985  0.050252         3   \n1684          setting  0.200999            0.991 -0.109949         1   \n7259              fer  0.259189            0.966 -0.132545         4   \n16205             ski  0.287075            0.983 -0.166819         2   \n5073          someday  0.002855            0.988  0.115850         1   \n13092        futurity  0.177934            0.974 -0.035909        37   \n20128         drastic  0.069603            0.993  0.058760         3   \n20078         incline -0.034136            0.988  0.176630       114   \n20662       lineament  0.178748            0.979 -0.026509        51   \n8384         lifelong  0.115619            0.995  0.023565         1   \n16451       cheapness  0.079505            0.984  0.070801        17   \n9831       habitually  0.109373            0.978  0.047016        63   \n24592             vet  0.166211            0.980 -0.011214         3   \n17630            aura  0.217044            0.998 -0.075655         9   \n15164             hew  0.171765            0.993 -0.019913        30   \n2558   overwhelmingly  0.146081            0.985  0.017748         1   \n\n       count_m2      mean  \n24935        36  0.284895  \n6585         18  0.296305  \n17195         6  0.307565  \n10661       158  0.309048  \n11922        76  0.328442  \n16315        42  0.331575  \n20337        61  0.339228  \n10305       149  0.348632  \n7053        123  0.348688  \n23333        11  0.352474  \n18442        31  0.355231  \n26316        56  0.356067  \n10956        77  0.356972  \n5256         43  0.357753  \n15005      1589  0.360192  \n1684         62  0.360683  \n7259         47  0.364215  \n16205       218  0.367752  \n5073        125  0.368902  \n13092         2  0.372008  \n20128        48  0.373788  \n20078        25  0.376831  \n20662         1  0.377080  \n8384         57  0.378061  \n16451         4  0.378102  \n9831          9  0.378130  \n24592        43  0.378332  \n17630        52  0.379796  \n15164        18  0.381617  \n2558         33  0.382943  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24935</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.987</td>\n      <td>-0.073233</td>\n      <td>26</td>\n      <td>36</td>\n      <td>0.284895</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>17195</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.964</td>\n      <td>-0.110117</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.307565</td>\n    </tr>\n    <tr>\n      <th>10661</th>\n      <td>pregnant</td>\n      <td>-0.059092</td>\n      <td>0.988</td>\n      <td>-0.001763</td>\n      <td>31</td>\n      <td>158</td>\n      <td>0.309048</td>\n    </tr>\n    <tr>\n      <th>11922</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.992</td>\n      <td>-0.102361</td>\n      <td>3</td>\n      <td>76</td>\n      <td>0.328442</td>\n    </tr>\n    <tr>\n      <th>16315</th>\n      <td>backing</td>\n      <td>0.045896</td>\n      <td>0.987</td>\n      <td>-0.038172</td>\n      <td>5</td>\n      <td>42</td>\n      <td>0.331575</td>\n    </tr>\n    <tr>\n      <th>20337</th>\n      <td>scarce</td>\n      <td>-0.044323</td>\n      <td>0.990</td>\n      <td>0.072006</td>\n      <td>329</td>\n      <td>61</td>\n      <td>0.339228</td>\n    </tr>\n    <tr>\n      <th>10305</th>\n      <td>hearing</td>\n      <td>0.064546</td>\n      <td>0.990</td>\n      <td>-0.008649</td>\n      <td>1</td>\n      <td>149</td>\n      <td>0.348632</td>\n    </tr>\n    <tr>\n      <th>7053</th>\n      <td>significantly</td>\n      <td>0.058707</td>\n      <td>0.998</td>\n      <td>-0.010645</td>\n      <td>31</td>\n      <td>123</td>\n      <td>0.348688</td>\n    </tr>\n    <tr>\n      <th>23333</th>\n      <td>hy</td>\n      <td>0.056843</td>\n      <td>0.992</td>\n      <td>0.008580</td>\n      <td>109</td>\n      <td>11</td>\n      <td>0.352474</td>\n    </tr>\n    <tr>\n      <th>18442</th>\n      <td>twain</td>\n      <td>0.055887</td>\n      <td>0.998</td>\n      <td>0.011805</td>\n      <td>29</td>\n      <td>31</td>\n      <td>0.355231</td>\n    </tr>\n    <tr>\n      <th>26316</th>\n      <td>juvenile</td>\n      <td>0.056204</td>\n      <td>0.974</td>\n      <td>0.037996</td>\n      <td>32</td>\n      <td>56</td>\n      <td>0.356067</td>\n    </tr>\n    <tr>\n      <th>10956</th>\n      <td>boring</td>\n      <td>0.089093</td>\n      <td>0.991</td>\n      <td>-0.009177</td>\n      <td>8</td>\n      <td>77</td>\n      <td>0.356972</td>\n    </tr>\n    <tr>\n      <th>5256</th>\n      <td>intriguing</td>\n      <td>0.198948</td>\n      <td>0.970</td>\n      <td>-0.095689</td>\n      <td>13</td>\n      <td>43</td>\n      <td>0.357753</td>\n    </tr>\n    <tr>\n      <th>15005</th>\n      <td>someone</td>\n      <td>0.045325</td>\n      <td>0.985</td>\n      <td>0.050252</td>\n      <td>3</td>\n      <td>1589</td>\n      <td>0.360192</td>\n    </tr>\n    <tr>\n      <th>1684</th>\n      <td>setting</td>\n      <td>0.200999</td>\n      <td>0.991</td>\n      <td>-0.109949</td>\n      <td>1</td>\n      <td>62</td>\n      <td>0.360683</td>\n    </tr>\n    <tr>\n      <th>7259</th>\n      <td>fer</td>\n      <td>0.259189</td>\n      <td>0.966</td>\n      <td>-0.132545</td>\n      <td>4</td>\n      <td>47</td>\n      <td>0.364215</td>\n    </tr>\n    <tr>\n      <th>16205</th>\n      <td>ski</td>\n      <td>0.287075</td>\n      <td>0.983</td>\n      <td>-0.166819</td>\n      <td>2</td>\n      <td>218</td>\n      <td>0.367752</td>\n    </tr>\n    <tr>\n      <th>5073</th>\n      <td>someday</td>\n      <td>0.002855</td>\n      <td>0.988</td>\n      <td>0.115850</td>\n      <td>1</td>\n      <td>125</td>\n      <td>0.368902</td>\n    </tr>\n    <tr>\n      <th>13092</th>\n      <td>futurity</td>\n      <td>0.177934</td>\n      <td>0.974</td>\n      <td>-0.035909</td>\n      <td>37</td>\n      <td>2</td>\n      <td>0.372008</td>\n    </tr>\n    <tr>\n      <th>20128</th>\n      <td>drastic</td>\n      <td>0.069603</td>\n      <td>0.993</td>\n      <td>0.058760</td>\n      <td>3</td>\n      <td>48</td>\n      <td>0.373788</td>\n    </tr>\n    <tr>\n      <th>20078</th>\n      <td>incline</td>\n      <td>-0.034136</td>\n      <td>0.988</td>\n      <td>0.176630</td>\n      <td>114</td>\n      <td>25</td>\n      <td>0.376831</td>\n    </tr>\n    <tr>\n      <th>20662</th>\n      <td>lineament</td>\n      <td>0.178748</td>\n      <td>0.979</td>\n      <td>-0.026509</td>\n      <td>51</td>\n      <td>1</td>\n      <td>0.377080</td>\n    </tr>\n    <tr>\n      <th>8384</th>\n      <td>lifelong</td>\n      <td>0.115619</td>\n      <td>0.995</td>\n      <td>0.023565</td>\n      <td>1</td>\n      <td>57</td>\n      <td>0.378061</td>\n    </tr>\n    <tr>\n      <th>16451</th>\n      <td>cheapness</td>\n      <td>0.079505</td>\n      <td>0.984</td>\n      <td>0.070801</td>\n      <td>17</td>\n      <td>4</td>\n      <td>0.378102</td>\n    </tr>\n    <tr>\n      <th>9831</th>\n      <td>habitually</td>\n      <td>0.109373</td>\n      <td>0.978</td>\n      <td>0.047016</td>\n      <td>63</td>\n      <td>9</td>\n      <td>0.378130</td>\n    </tr>\n    <tr>\n      <th>24592</th>\n      <td>vet</td>\n      <td>0.166211</td>\n      <td>0.980</td>\n      <td>-0.011214</td>\n      <td>3</td>\n      <td>43</td>\n      <td>0.378332</td>\n    </tr>\n    <tr>\n      <th>17630</th>\n      <td>aura</td>\n      <td>0.217044</td>\n      <td>0.998</td>\n      <td>-0.075655</td>\n      <td>9</td>\n      <td>52</td>\n      <td>0.379796</td>\n    </tr>\n    <tr>\n      <th>15164</th>\n      <td>hew</td>\n      <td>0.171765</td>\n      <td>0.993</td>\n      <td>-0.019913</td>\n      <td>30</td>\n      <td>18</td>\n      <td>0.381617</td>\n    </tr>\n    <tr>\n      <th>2558</th>\n      <td>overwhelmingly</td>\n      <td>0.146081</td>\n      <td>0.985</td>\n      <td>0.017748</td>\n      <td>1</td>\n      <td>33</td>\n      <td>0.382943</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"mean\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594667553763",
   "display_name": "Python 3.7.7 64-bit ('thesis': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}