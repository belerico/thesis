{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from cade.metrics.comparative import lncs2, intersection_nn, initialize_avgs\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from pandas import pandas\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "from tabulate import tabulate\n",
    "from config import CURRENT_EXP_DIR, config, get_logger, log_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load language models and groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(lang: str):\n",
    "    model1 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus1.model\"\n",
    "    )\n",
    "    model2 = Word2Vec.load(\n",
    "        CURRENT_EXP_DIR.split(\"_\")[0]\n",
    "        + \"_0\"\n",
    "        + \"/model/\"\n",
    "        + lang\n",
    "        + \"/corpus2.model\"\n",
    "    )\n",
    "    return model1, model2\n",
    "\n",
    "def get_gt(lang: str, binary=True):\n",
    "    binary_truth = numpy.loadtxt(\n",
    "        \"./data/\"\n",
    "        + lang\n",
    "        + \"/semeval2020_ulscd_\"\n",
    "        + lang[:3]\n",
    "        + \"/truth/\" + (\"binary\" if binary else \"graded\") + \".txt\",\n",
    "        dtype=str,\n",
    "        delimiter=\"\\t\",\n",
    "    )\n",
    "    return binary_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English (get LNCS2, Intersection_NN and Cosine scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang = \"english\"\n",
    "# Load models\n",
    "model1, model2 = get_models(lang)\n",
    "# Initialize models avgs\n",
    "initialize_avgs(model1, model2)\n",
    "shared_vocabulary = set(model1.wv.vocab.keys()).intersection(set(model2.wv.vocab.keys()))\n",
    "shared_vocabulary_df = pandas.DataFrame(shared_vocabulary, columns=[\"word\"])\n",
    "shared_vocabulary_df[\"lncs2\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: lncs2(word, model1, model2, 25)\n",
    ")\n",
    "shared_vocabulary_df[\"intersection_nn\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: intersection_nn(word, model1, model2)\n",
    ")\n",
    "shared_vocabulary_df[\"cosine\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: 1 - cosine(model1.wv[word], model2.wv[word])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add mean of the three metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shared_vocabulary_df[\"mean\"] = shared_vocabulary_df[[\"lncs2\", \"cosine\", \"intersection_nn\"]].apply(\n",
    "    lambda x: (x.lncs2 + x.cosine + x.intersection_nn) / 3, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocabulary_df[\"count_m1\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: model1.wv.vocab[word].count\n",
    ")\n",
    "shared_vocabulary_df[\"count_m2\"] = shared_vocabulary_df[\"word\"].apply(\n",
    "    lambda word: model2.wv.vocab[word].count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocabulary_df.to_pickle(\"./shared_vocabulary_metrics.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words that changed the most (by LNCS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n10661       pregnant -0.059092            0.988 -0.001763        31       158   \n24935         unused -0.059081            0.987 -0.073233        26        36   \n20337         scarce -0.044323            0.990  0.072006       329        61   \n20078        incline -0.034136            0.988  0.176630       114        25   \n6585              ml -0.029649            0.998 -0.079436         1        18   \n4671           major -0.016591            0.998  0.263108       531      1554   \n5073         someday  0.002855            0.988  0.115850         1       125   \n16236       mentally  0.016916            0.980  0.328319        31        90   \n147            tense  0.032603            0.984  0.138092        11       139   \n19984            err  0.036362            0.974  0.169528        70        11   \n15005        someone  0.045325            0.985  0.050252         3      1589   \n16315        backing  0.045896            0.987 -0.038172         5        42   \n18442          twain  0.055887            0.998  0.011805        29        31   \n26316       juvenile  0.056204            0.974  0.037996        32        56   \n23333             hy  0.056843            0.992  0.008580       109        11   \n7053   significantly  0.058707            0.998 -0.010645        31       123   \n20197          today  0.063035            0.983  0.364674       402      1973   \n10305        hearing  0.064546            0.990 -0.008649         1       149   \n17195         some--  0.068811            0.964 -0.110117         2         6   \n20128        drastic  0.069603            0.993  0.058760         3        48   \n17361     detachment  0.070652            0.993  0.089520        77        44   \n11084     deflection  0.071443            0.983  0.267019        15         3   \n21890        discard  0.078995            0.995  0.215280        57       109   \n16451      cheapness  0.079505            0.984  0.070801        17         4   \n16344        roughly  0.084750            0.997  0.068283        50       182   \n23643     conveyance  0.085479            0.985  0.159299        44         4   \n10956         boring  0.089093            0.991 -0.009177         8        77   \n12508          check  0.091327            0.979  0.178886       367      1028   \n11922    frightening  0.095688            0.992 -0.102361         3        76   \n18608        burthen  0.101387            0.981  0.162739        49         1   \n\n           mean  \n10661  0.309048  \n24935  0.284895  \n20337  0.339228  \n20078  0.376831  \n6585   0.296305  \n4671   0.414839  \n5073   0.368902  \n16236  0.441745  \n147    0.384898  \n19984  0.393297  \n15005  0.360192  \n16315  0.331575  \n18442  0.355231  \n26316  0.356067  \n23333  0.352474  \n7053   0.348688  \n20197  0.470237  \n10305  0.348632  \n17195  0.307565  \n20128  0.373788  \n17361  0.384391  \n11084  0.440487  \n21890  0.429758  \n16451  0.378102  \n16344  0.383344  \n23643  0.409926  \n10956  0.356972  \n12508  0.416404  \n11922  0.328442  \n18608  0.415042  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10661</th>\n      <td>pregnant</td>\n      <td>-0.059092</td>\n      <td>0.988</td>\n      <td>-0.001763</td>\n      <td>31</td>\n      <td>158</td>\n      <td>0.309048</td>\n    </tr>\n    <tr>\n      <th>24935</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.987</td>\n      <td>-0.073233</td>\n      <td>26</td>\n      <td>36</td>\n      <td>0.284895</td>\n    </tr>\n    <tr>\n      <th>20337</th>\n      <td>scarce</td>\n      <td>-0.044323</td>\n      <td>0.990</td>\n      <td>0.072006</td>\n      <td>329</td>\n      <td>61</td>\n      <td>0.339228</td>\n    </tr>\n    <tr>\n      <th>20078</th>\n      <td>incline</td>\n      <td>-0.034136</td>\n      <td>0.988</td>\n      <td>0.176630</td>\n      <td>114</td>\n      <td>25</td>\n      <td>0.376831</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>4671</th>\n      <td>major</td>\n      <td>-0.016591</td>\n      <td>0.998</td>\n      <td>0.263108</td>\n      <td>531</td>\n      <td>1554</td>\n      <td>0.414839</td>\n    </tr>\n    <tr>\n      <th>5073</th>\n      <td>someday</td>\n      <td>0.002855</td>\n      <td>0.988</td>\n      <td>0.115850</td>\n      <td>1</td>\n      <td>125</td>\n      <td>0.368902</td>\n    </tr>\n    <tr>\n      <th>16236</th>\n      <td>mentally</td>\n      <td>0.016916</td>\n      <td>0.980</td>\n      <td>0.328319</td>\n      <td>31</td>\n      <td>90</td>\n      <td>0.441745</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>tense</td>\n      <td>0.032603</td>\n      <td>0.984</td>\n      <td>0.138092</td>\n      <td>11</td>\n      <td>139</td>\n      <td>0.384898</td>\n    </tr>\n    <tr>\n      <th>19984</th>\n      <td>err</td>\n      <td>0.036362</td>\n      <td>0.974</td>\n      <td>0.169528</td>\n      <td>70</td>\n      <td>11</td>\n      <td>0.393297</td>\n    </tr>\n    <tr>\n      <th>15005</th>\n      <td>someone</td>\n      <td>0.045325</td>\n      <td>0.985</td>\n      <td>0.050252</td>\n      <td>3</td>\n      <td>1589</td>\n      <td>0.360192</td>\n    </tr>\n    <tr>\n      <th>16315</th>\n      <td>backing</td>\n      <td>0.045896</td>\n      <td>0.987</td>\n      <td>-0.038172</td>\n      <td>5</td>\n      <td>42</td>\n      <td>0.331575</td>\n    </tr>\n    <tr>\n      <th>18442</th>\n      <td>twain</td>\n      <td>0.055887</td>\n      <td>0.998</td>\n      <td>0.011805</td>\n      <td>29</td>\n      <td>31</td>\n      <td>0.355231</td>\n    </tr>\n    <tr>\n      <th>26316</th>\n      <td>juvenile</td>\n      <td>0.056204</td>\n      <td>0.974</td>\n      <td>0.037996</td>\n      <td>32</td>\n      <td>56</td>\n      <td>0.356067</td>\n    </tr>\n    <tr>\n      <th>23333</th>\n      <td>hy</td>\n      <td>0.056843</td>\n      <td>0.992</td>\n      <td>0.008580</td>\n      <td>109</td>\n      <td>11</td>\n      <td>0.352474</td>\n    </tr>\n    <tr>\n      <th>7053</th>\n      <td>significantly</td>\n      <td>0.058707</td>\n      <td>0.998</td>\n      <td>-0.010645</td>\n      <td>31</td>\n      <td>123</td>\n      <td>0.348688</td>\n    </tr>\n    <tr>\n      <th>20197</th>\n      <td>today</td>\n      <td>0.063035</td>\n      <td>0.983</td>\n      <td>0.364674</td>\n      <td>402</td>\n      <td>1973</td>\n      <td>0.470237</td>\n    </tr>\n    <tr>\n      <th>10305</th>\n      <td>hearing</td>\n      <td>0.064546</td>\n      <td>0.990</td>\n      <td>-0.008649</td>\n      <td>1</td>\n      <td>149</td>\n      <td>0.348632</td>\n    </tr>\n    <tr>\n      <th>17195</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.964</td>\n      <td>-0.110117</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.307565</td>\n    </tr>\n    <tr>\n      <th>20128</th>\n      <td>drastic</td>\n      <td>0.069603</td>\n      <td>0.993</td>\n      <td>0.058760</td>\n      <td>3</td>\n      <td>48</td>\n      <td>0.373788</td>\n    </tr>\n    <tr>\n      <th>17361</th>\n      <td>detachment</td>\n      <td>0.070652</td>\n      <td>0.993</td>\n      <td>0.089520</td>\n      <td>77</td>\n      <td>44</td>\n      <td>0.384391</td>\n    </tr>\n    <tr>\n      <th>11084</th>\n      <td>deflection</td>\n      <td>0.071443</td>\n      <td>0.983</td>\n      <td>0.267019</td>\n      <td>15</td>\n      <td>3</td>\n      <td>0.440487</td>\n    </tr>\n    <tr>\n      <th>21890</th>\n      <td>discard</td>\n      <td>0.078995</td>\n      <td>0.995</td>\n      <td>0.215280</td>\n      <td>57</td>\n      <td>109</td>\n      <td>0.429758</td>\n    </tr>\n    <tr>\n      <th>16451</th>\n      <td>cheapness</td>\n      <td>0.079505</td>\n      <td>0.984</td>\n      <td>0.070801</td>\n      <td>17</td>\n      <td>4</td>\n      <td>0.378102</td>\n    </tr>\n    <tr>\n      <th>16344</th>\n      <td>roughly</td>\n      <td>0.084750</td>\n      <td>0.997</td>\n      <td>0.068283</td>\n      <td>50</td>\n      <td>182</td>\n      <td>0.383344</td>\n    </tr>\n    <tr>\n      <th>23643</th>\n      <td>conveyance</td>\n      <td>0.085479</td>\n      <td>0.985</td>\n      <td>0.159299</td>\n      <td>44</td>\n      <td>4</td>\n      <td>0.409926</td>\n    </tr>\n    <tr>\n      <th>10956</th>\n      <td>boring</td>\n      <td>0.089093</td>\n      <td>0.991</td>\n      <td>-0.009177</td>\n      <td>8</td>\n      <td>77</td>\n      <td>0.356972</td>\n    </tr>\n    <tr>\n      <th>12508</th>\n      <td>check</td>\n      <td>0.091327</td>\n      <td>0.979</td>\n      <td>0.178886</td>\n      <td>367</td>\n      <td>1028</td>\n      <td>0.416404</td>\n    </tr>\n    <tr>\n      <th>11922</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.992</td>\n      <td>-0.102361</td>\n      <td>3</td>\n      <td>76</td>\n      <td>0.328442</td>\n    </tr>\n    <tr>\n      <th>18608</th>\n      <td>burthen</td>\n      <td>0.101387</td>\n      <td>0.981</td>\n      <td>0.162739</td>\n      <td>49</td>\n      <td>1</td>\n      <td>0.415042</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"lncs2\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by Intersection_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n11083    convince  0.945782            0.514  0.781441       476       450   \n6945      believe  0.971778            0.522  0.758603      2948      2618   \n1709        swing  0.971353            0.538  0.728393       166       626   \n148          deny  0.964949            0.546  0.691601       682       414   \n7501   understand  0.974703            0.558  0.727781      1475      1683   \n9286         find  0.928129            0.558  0.849056      7549      6846   \n617         grass  0.945075            0.560  0.797416       378       483   \n2186        trust  0.957087            0.566  0.670041      1050       562   \n4771        slope  0.930147            0.566  0.855519       137       200   \n15394       trunk  0.952310            0.568  0.776542       322       180   \n429       tree_nn  0.973066            0.569  0.871842      2322      1596   \n16268         leg  0.964123            0.569  0.840654       376      1191   \n26917        hair  0.912894            0.570  0.867503       974      1970   \n16234        hang  0.958704            0.570  0.809736      1049      1189   \n21156        leaf  0.932912            0.571  0.840396       773       477   \n22643     satisfy  0.869777            0.573  0.681398       521       200   \n21794    shoulder  0.972574            0.573  0.826431       677      1237   \n1423        fence  0.921338            0.575  0.829482       179       260   \n18862      assure  0.933151            0.575  0.711387       611       345   \n10701        rope  0.946561            0.575  0.830084       175       276   \n126        carpet  0.924440            0.575  0.777948       126       174   \n13307      collar  0.949337            0.575  0.783521       112       213   \n25176        pull  0.972609            0.576  0.750570       379      1989   \n10274       prove  0.958826            0.577  0.727998      1436       825   \n13489         dry  0.924254            0.578  0.805396       522       691   \n26842        neck  0.945422            0.579  0.832751       564       714   \n23704        thin  0.927979            0.579  0.801362       407       586   \n5782        bring  0.983159            0.579  0.865412      3918      3332   \n5043         hear  0.958789            0.580  0.851877      4691      4058   \n14119     boiling  0.913699            0.580  0.852232        53        48   \n\n           mean  \n11083  0.747074  \n6945   0.750794  \n1709   0.745915  \n148    0.734183  \n7501   0.753495  \n9286   0.778395  \n617    0.767497  \n2186   0.731043  \n4771   0.783888  \n15394  0.765617  \n429    0.804636  \n16268  0.791259  \n26917  0.783466  \n16234  0.779480  \n21156  0.781436  \n22643  0.708058  \n21794  0.790668  \n1423   0.775273  \n18862  0.739846  \n10701  0.783882  \n126    0.759129  \n13307  0.769286  \n25176  0.766393  \n10274  0.754608  \n13489  0.769217  \n26842  0.785724  \n23704  0.769447  \n5782   0.809190  \n5043   0.796889  \n14119  0.781977  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11083</th>\n      <td>convince</td>\n      <td>0.945782</td>\n      <td>0.514</td>\n      <td>0.781441</td>\n      <td>476</td>\n      <td>450</td>\n      <td>0.747074</td>\n    </tr>\n    <tr>\n      <th>6945</th>\n      <td>believe</td>\n      <td>0.971778</td>\n      <td>0.522</td>\n      <td>0.758603</td>\n      <td>2948</td>\n      <td>2618</td>\n      <td>0.750794</td>\n    </tr>\n    <tr>\n      <th>1709</th>\n      <td>swing</td>\n      <td>0.971353</td>\n      <td>0.538</td>\n      <td>0.728393</td>\n      <td>166</td>\n      <td>626</td>\n      <td>0.745915</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>deny</td>\n      <td>0.964949</td>\n      <td>0.546</td>\n      <td>0.691601</td>\n      <td>682</td>\n      <td>414</td>\n      <td>0.734183</td>\n    </tr>\n    <tr>\n      <th>7501</th>\n      <td>understand</td>\n      <td>0.974703</td>\n      <td>0.558</td>\n      <td>0.727781</td>\n      <td>1475</td>\n      <td>1683</td>\n      <td>0.753495</td>\n    </tr>\n    <tr>\n      <th>9286</th>\n      <td>find</td>\n      <td>0.928129</td>\n      <td>0.558</td>\n      <td>0.849056</td>\n      <td>7549</td>\n      <td>6846</td>\n      <td>0.778395</td>\n    </tr>\n    <tr>\n      <th>617</th>\n      <td>grass</td>\n      <td>0.945075</td>\n      <td>0.560</td>\n      <td>0.797416</td>\n      <td>378</td>\n      <td>483</td>\n      <td>0.767497</td>\n    </tr>\n    <tr>\n      <th>2186</th>\n      <td>trust</td>\n      <td>0.957087</td>\n      <td>0.566</td>\n      <td>0.670041</td>\n      <td>1050</td>\n      <td>562</td>\n      <td>0.731043</td>\n    </tr>\n    <tr>\n      <th>4771</th>\n      <td>slope</td>\n      <td>0.930147</td>\n      <td>0.566</td>\n      <td>0.855519</td>\n      <td>137</td>\n      <td>200</td>\n      <td>0.783888</td>\n    </tr>\n    <tr>\n      <th>15394</th>\n      <td>trunk</td>\n      <td>0.952310</td>\n      <td>0.568</td>\n      <td>0.776542</td>\n      <td>322</td>\n      <td>180</td>\n      <td>0.765617</td>\n    </tr>\n    <tr>\n      <th>429</th>\n      <td>tree_nn</td>\n      <td>0.973066</td>\n      <td>0.569</td>\n      <td>0.871842</td>\n      <td>2322</td>\n      <td>1596</td>\n      <td>0.804636</td>\n    </tr>\n    <tr>\n      <th>16268</th>\n      <td>leg</td>\n      <td>0.964123</td>\n      <td>0.569</td>\n      <td>0.840654</td>\n      <td>376</td>\n      <td>1191</td>\n      <td>0.791259</td>\n    </tr>\n    <tr>\n      <th>26917</th>\n      <td>hair</td>\n      <td>0.912894</td>\n      <td>0.570</td>\n      <td>0.867503</td>\n      <td>974</td>\n      <td>1970</td>\n      <td>0.783466</td>\n    </tr>\n    <tr>\n      <th>16234</th>\n      <td>hang</td>\n      <td>0.958704</td>\n      <td>0.570</td>\n      <td>0.809736</td>\n      <td>1049</td>\n      <td>1189</td>\n      <td>0.779480</td>\n    </tr>\n    <tr>\n      <th>21156</th>\n      <td>leaf</td>\n      <td>0.932912</td>\n      <td>0.571</td>\n      <td>0.840396</td>\n      <td>773</td>\n      <td>477</td>\n      <td>0.781436</td>\n    </tr>\n    <tr>\n      <th>22643</th>\n      <td>satisfy</td>\n      <td>0.869777</td>\n      <td>0.573</td>\n      <td>0.681398</td>\n      <td>521</td>\n      <td>200</td>\n      <td>0.708058</td>\n    </tr>\n    <tr>\n      <th>21794</th>\n      <td>shoulder</td>\n      <td>0.972574</td>\n      <td>0.573</td>\n      <td>0.826431</td>\n      <td>677</td>\n      <td>1237</td>\n      <td>0.790668</td>\n    </tr>\n    <tr>\n      <th>1423</th>\n      <td>fence</td>\n      <td>0.921338</td>\n      <td>0.575</td>\n      <td>0.829482</td>\n      <td>179</td>\n      <td>260</td>\n      <td>0.775273</td>\n    </tr>\n    <tr>\n      <th>18862</th>\n      <td>assure</td>\n      <td>0.933151</td>\n      <td>0.575</td>\n      <td>0.711387</td>\n      <td>611</td>\n      <td>345</td>\n      <td>0.739846</td>\n    </tr>\n    <tr>\n      <th>10701</th>\n      <td>rope</td>\n      <td>0.946561</td>\n      <td>0.575</td>\n      <td>0.830084</td>\n      <td>175</td>\n      <td>276</td>\n      <td>0.783882</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>carpet</td>\n      <td>0.924440</td>\n      <td>0.575</td>\n      <td>0.777948</td>\n      <td>126</td>\n      <td>174</td>\n      <td>0.759129</td>\n    </tr>\n    <tr>\n      <th>13307</th>\n      <td>collar</td>\n      <td>0.949337</td>\n      <td>0.575</td>\n      <td>0.783521</td>\n      <td>112</td>\n      <td>213</td>\n      <td>0.769286</td>\n    </tr>\n    <tr>\n      <th>25176</th>\n      <td>pull</td>\n      <td>0.972609</td>\n      <td>0.576</td>\n      <td>0.750570</td>\n      <td>379</td>\n      <td>1989</td>\n      <td>0.766393</td>\n    </tr>\n    <tr>\n      <th>10274</th>\n      <td>prove</td>\n      <td>0.958826</td>\n      <td>0.577</td>\n      <td>0.727998</td>\n      <td>1436</td>\n      <td>825</td>\n      <td>0.754608</td>\n    </tr>\n    <tr>\n      <th>13489</th>\n      <td>dry</td>\n      <td>0.924254</td>\n      <td>0.578</td>\n      <td>0.805396</td>\n      <td>522</td>\n      <td>691</td>\n      <td>0.769217</td>\n    </tr>\n    <tr>\n      <th>26842</th>\n      <td>neck</td>\n      <td>0.945422</td>\n      <td>0.579</td>\n      <td>0.832751</td>\n      <td>564</td>\n      <td>714</td>\n      <td>0.785724</td>\n    </tr>\n    <tr>\n      <th>23704</th>\n      <td>thin</td>\n      <td>0.927979</td>\n      <td>0.579</td>\n      <td>0.801362</td>\n      <td>407</td>\n      <td>586</td>\n      <td>0.769447</td>\n    </tr>\n    <tr>\n      <th>5782</th>\n      <td>bring</td>\n      <td>0.983159</td>\n      <td>0.579</td>\n      <td>0.865412</td>\n      <td>3918</td>\n      <td>3332</td>\n      <td>0.809190</td>\n    </tr>\n    <tr>\n      <th>5043</th>\n      <td>hear</td>\n      <td>0.958789</td>\n      <td>0.580</td>\n      <td>0.851877</td>\n      <td>4691</td>\n      <td>4058</td>\n      <td>0.796889</td>\n    </tr>\n    <tr>\n      <th>14119</th>\n      <td>boiling</td>\n      <td>0.913699</td>\n      <td>0.580</td>\n      <td>0.852232</td>\n      <td>53</td>\n      <td>48</td>\n      <td>0.781977</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"intersection_nn\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               word     lncs2  intersection_nn    cosine  count_m1  count_m2  \\\n16205           ski  0.287075            0.983 -0.166819         2       218   \n4043          virus  0.323960            1.000 -0.149479        13       166   \n7259            fer  0.259189            0.966 -0.132545         4        47   \n18503            op  0.695739            0.974 -0.127812        21        31   \n17938        wilmer  0.500615            0.950 -0.111061        11        16   \n17195        some--  0.068811            0.964 -0.110117         2         6   \n1684        setting  0.200999            0.991 -0.109949         1        62   \n11922   frightening  0.095688            0.992 -0.102361         3        76   \n15049          gist  0.272670            0.994 -0.098656        15        15   \n17813           cal  0.479762            0.980 -0.098061        13        66   \n5256     intriguing  0.198948            0.970 -0.095689        13        43   \n25637     projector  0.393249            0.998 -0.094298        18        33   \n23774           int  0.534314            0.963 -0.084740         5        77   \n449             cir  0.372242            0.981 -0.082652        10         1   \n5775         scurvy  0.292371            0.986 -0.081057         5         6   \n8812          flume  0.600722            0.972 -0.080179         6         1   \n6585             ml -0.029649            0.998 -0.079436         1        18   \n2193             uc  0.517708            0.985 -0.077761         1        15   \n17630          aura  0.217044            0.998 -0.075655         9        52   \n17288         ethel  0.413615            0.974 -0.075074         1        40   \n24935        unused -0.059081            0.987 -0.073233        26        36   \n9359          lotus  0.677376            0.971 -0.069215         2        22   \n4905        onwards  0.414718            0.968 -0.068938        11         9   \n20737        walden  0.373120            0.990 -0.068412        12         8   \n8234   ridiculously  0.558388            0.979 -0.065712         7        14   \n14593       adeline  0.466286            0.965 -0.064648        32         1   \n2159          humus  0.268067            0.969 -0.064053         1        10   \n2587            doc  0.439268            0.975 -0.062904         5       107   \n21108         elias  0.615312            0.960 -0.059430        13        37   \n24062  fruitfulness  0.413633            0.965 -0.057541        13         2   \n\n           mean  \n16205  0.367752  \n4043   0.391494  \n7259   0.364215  \n18503  0.513976  \n17938  0.446518  \n17195  0.307565  \n1684   0.360683  \n11922  0.328442  \n15049  0.389338  \n17813  0.453900  \n5256   0.357753  \n25637  0.432317  \n23774  0.470858  \n449    0.423530  \n5775   0.399105  \n8812   0.497514  \n6585   0.296305  \n2193   0.474982  \n17630  0.379796  \n17288  0.437514  \n24935  0.284895  \n9359   0.526387  \n4905   0.437927  \n20737  0.431570  \n8234   0.490559  \n14593  0.455546  \n2159   0.391005  \n2587   0.450455  \n21108  0.505294  \n24062  0.440364  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16205</th>\n      <td>ski</td>\n      <td>0.287075</td>\n      <td>0.983</td>\n      <td>-0.166819</td>\n      <td>2</td>\n      <td>218</td>\n      <td>0.367752</td>\n    </tr>\n    <tr>\n      <th>4043</th>\n      <td>virus</td>\n      <td>0.323960</td>\n      <td>1.000</td>\n      <td>-0.149479</td>\n      <td>13</td>\n      <td>166</td>\n      <td>0.391494</td>\n    </tr>\n    <tr>\n      <th>7259</th>\n      <td>fer</td>\n      <td>0.259189</td>\n      <td>0.966</td>\n      <td>-0.132545</td>\n      <td>4</td>\n      <td>47</td>\n      <td>0.364215</td>\n    </tr>\n    <tr>\n      <th>18503</th>\n      <td>op</td>\n      <td>0.695739</td>\n      <td>0.974</td>\n      <td>-0.127812</td>\n      <td>21</td>\n      <td>31</td>\n      <td>0.513976</td>\n    </tr>\n    <tr>\n      <th>17938</th>\n      <td>wilmer</td>\n      <td>0.500615</td>\n      <td>0.950</td>\n      <td>-0.111061</td>\n      <td>11</td>\n      <td>16</td>\n      <td>0.446518</td>\n    </tr>\n    <tr>\n      <th>17195</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.964</td>\n      <td>-0.110117</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.307565</td>\n    </tr>\n    <tr>\n      <th>1684</th>\n      <td>setting</td>\n      <td>0.200999</td>\n      <td>0.991</td>\n      <td>-0.109949</td>\n      <td>1</td>\n      <td>62</td>\n      <td>0.360683</td>\n    </tr>\n    <tr>\n      <th>11922</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.992</td>\n      <td>-0.102361</td>\n      <td>3</td>\n      <td>76</td>\n      <td>0.328442</td>\n    </tr>\n    <tr>\n      <th>15049</th>\n      <td>gist</td>\n      <td>0.272670</td>\n      <td>0.994</td>\n      <td>-0.098656</td>\n      <td>15</td>\n      <td>15</td>\n      <td>0.389338</td>\n    </tr>\n    <tr>\n      <th>17813</th>\n      <td>cal</td>\n      <td>0.479762</td>\n      <td>0.980</td>\n      <td>-0.098061</td>\n      <td>13</td>\n      <td>66</td>\n      <td>0.453900</td>\n    </tr>\n    <tr>\n      <th>5256</th>\n      <td>intriguing</td>\n      <td>0.198948</td>\n      <td>0.970</td>\n      <td>-0.095689</td>\n      <td>13</td>\n      <td>43</td>\n      <td>0.357753</td>\n    </tr>\n    <tr>\n      <th>25637</th>\n      <td>projector</td>\n      <td>0.393249</td>\n      <td>0.998</td>\n      <td>-0.094298</td>\n      <td>18</td>\n      <td>33</td>\n      <td>0.432317</td>\n    </tr>\n    <tr>\n      <th>23774</th>\n      <td>int</td>\n      <td>0.534314</td>\n      <td>0.963</td>\n      <td>-0.084740</td>\n      <td>5</td>\n      <td>77</td>\n      <td>0.470858</td>\n    </tr>\n    <tr>\n      <th>449</th>\n      <td>cir</td>\n      <td>0.372242</td>\n      <td>0.981</td>\n      <td>-0.082652</td>\n      <td>10</td>\n      <td>1</td>\n      <td>0.423530</td>\n    </tr>\n    <tr>\n      <th>5775</th>\n      <td>scurvy</td>\n      <td>0.292371</td>\n      <td>0.986</td>\n      <td>-0.081057</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0.399105</td>\n    </tr>\n    <tr>\n      <th>8812</th>\n      <td>flume</td>\n      <td>0.600722</td>\n      <td>0.972</td>\n      <td>-0.080179</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.497514</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>2193</th>\n      <td>uc</td>\n      <td>0.517708</td>\n      <td>0.985</td>\n      <td>-0.077761</td>\n      <td>1</td>\n      <td>15</td>\n      <td>0.474982</td>\n    </tr>\n    <tr>\n      <th>17630</th>\n      <td>aura</td>\n      <td>0.217044</td>\n      <td>0.998</td>\n      <td>-0.075655</td>\n      <td>9</td>\n      <td>52</td>\n      <td>0.379796</td>\n    </tr>\n    <tr>\n      <th>17288</th>\n      <td>ethel</td>\n      <td>0.413615</td>\n      <td>0.974</td>\n      <td>-0.075074</td>\n      <td>1</td>\n      <td>40</td>\n      <td>0.437514</td>\n    </tr>\n    <tr>\n      <th>24935</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.987</td>\n      <td>-0.073233</td>\n      <td>26</td>\n      <td>36</td>\n      <td>0.284895</td>\n    </tr>\n    <tr>\n      <th>9359</th>\n      <td>lotus</td>\n      <td>0.677376</td>\n      <td>0.971</td>\n      <td>-0.069215</td>\n      <td>2</td>\n      <td>22</td>\n      <td>0.526387</td>\n    </tr>\n    <tr>\n      <th>4905</th>\n      <td>onwards</td>\n      <td>0.414718</td>\n      <td>0.968</td>\n      <td>-0.068938</td>\n      <td>11</td>\n      <td>9</td>\n      <td>0.437927</td>\n    </tr>\n    <tr>\n      <th>20737</th>\n      <td>walden</td>\n      <td>0.373120</td>\n      <td>0.990</td>\n      <td>-0.068412</td>\n      <td>12</td>\n      <td>8</td>\n      <td>0.431570</td>\n    </tr>\n    <tr>\n      <th>8234</th>\n      <td>ridiculously</td>\n      <td>0.558388</td>\n      <td>0.979</td>\n      <td>-0.065712</td>\n      <td>7</td>\n      <td>14</td>\n      <td>0.490559</td>\n    </tr>\n    <tr>\n      <th>14593</th>\n      <td>adeline</td>\n      <td>0.466286</td>\n      <td>0.965</td>\n      <td>-0.064648</td>\n      <td>32</td>\n      <td>1</td>\n      <td>0.455546</td>\n    </tr>\n    <tr>\n      <th>2159</th>\n      <td>humus</td>\n      <td>0.268067</td>\n      <td>0.969</td>\n      <td>-0.064053</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.391005</td>\n    </tr>\n    <tr>\n      <th>2587</th>\n      <td>doc</td>\n      <td>0.439268</td>\n      <td>0.975</td>\n      <td>-0.062904</td>\n      <td>5</td>\n      <td>107</td>\n      <td>0.450455</td>\n    </tr>\n    <tr>\n      <th>21108</th>\n      <td>elias</td>\n      <td>0.615312</td>\n      <td>0.960</td>\n      <td>-0.059430</td>\n      <td>13</td>\n      <td>37</td>\n      <td>0.505294</td>\n    </tr>\n    <tr>\n      <th>24062</th>\n      <td>fruitfulness</td>\n      <td>0.413633</td>\n      <td>0.965</td>\n      <td>-0.057541</td>\n      <td>13</td>\n      <td>2</td>\n      <td>0.440364</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"cosine\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words change the most (by mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 word     lncs2  intersection_nn    cosine  count_m1  \\\n24935          unused -0.059081            0.987 -0.073233        26   \n6585               ml -0.029649            0.998 -0.079436         1   \n17195          some--  0.068811            0.964 -0.110117         2   \n10661        pregnant -0.059092            0.988 -0.001763        31   \n11922     frightening  0.095688            0.992 -0.102361         3   \n16315         backing  0.045896            0.987 -0.038172         5   \n20337          scarce -0.044323            0.990  0.072006       329   \n10305         hearing  0.064546            0.990 -0.008649         1   \n7053    significantly  0.058707            0.998 -0.010645        31   \n23333              hy  0.056843            0.992  0.008580       109   \n18442           twain  0.055887            0.998  0.011805        29   \n26316        juvenile  0.056204            0.974  0.037996        32   \n10956          boring  0.089093            0.991 -0.009177         8   \n5256       intriguing  0.198948            0.970 -0.095689        13   \n15005         someone  0.045325            0.985  0.050252         3   \n1684          setting  0.200999            0.991 -0.109949         1   \n7259              fer  0.259189            0.966 -0.132545         4   \n16205             ski  0.287075            0.983 -0.166819         2   \n5073          someday  0.002855            0.988  0.115850         1   \n13092        futurity  0.177934            0.974 -0.035909        37   \n20128         drastic  0.069603            0.993  0.058760         3   \n20078         incline -0.034136            0.988  0.176630       114   \n20662       lineament  0.178748            0.979 -0.026509        51   \n8384         lifelong  0.115619            0.995  0.023565         1   \n16451       cheapness  0.079505            0.984  0.070801        17   \n9831       habitually  0.109373            0.978  0.047016        63   \n24592             vet  0.166211            0.980 -0.011214         3   \n17630            aura  0.217044            0.998 -0.075655         9   \n15164             hew  0.171765            0.993 -0.019913        30   \n2558   overwhelmingly  0.146081            0.985  0.017748         1   \n\n       count_m2      mean  \n24935        36  0.284895  \n6585         18  0.296305  \n17195         6  0.307565  \n10661       158  0.309048  \n11922        76  0.328442  \n16315        42  0.331575  \n20337        61  0.339228  \n10305       149  0.348632  \n7053        123  0.348688  \n23333        11  0.352474  \n18442        31  0.355231  \n26316        56  0.356067  \n10956        77  0.356972  \n5256         43  0.357753  \n15005      1589  0.360192  \n1684         62  0.360683  \n7259         47  0.364215  \n16205       218  0.367752  \n5073        125  0.368902  \n13092         2  0.372008  \n20128        48  0.373788  \n20078        25  0.376831  \n20662         1  0.377080  \n8384         57  0.378061  \n16451         4  0.378102  \n9831          9  0.378130  \n24592        43  0.378332  \n17630        52  0.379796  \n15164        18  0.381617  \n2558         33  0.382943  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>lncs2</th>\n      <th>intersection_nn</th>\n      <th>cosine</th>\n      <th>count_m1</th>\n      <th>count_m2</th>\n      <th>mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24935</th>\n      <td>unused</td>\n      <td>-0.059081</td>\n      <td>0.987</td>\n      <td>-0.073233</td>\n      <td>26</td>\n      <td>36</td>\n      <td>0.284895</td>\n    </tr>\n    <tr>\n      <th>6585</th>\n      <td>ml</td>\n      <td>-0.029649</td>\n      <td>0.998</td>\n      <td>-0.079436</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0.296305</td>\n    </tr>\n    <tr>\n      <th>17195</th>\n      <td>some--</td>\n      <td>0.068811</td>\n      <td>0.964</td>\n      <td>-0.110117</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.307565</td>\n    </tr>\n    <tr>\n      <th>10661</th>\n      <td>pregnant</td>\n      <td>-0.059092</td>\n      <td>0.988</td>\n      <td>-0.001763</td>\n      <td>31</td>\n      <td>158</td>\n      <td>0.309048</td>\n    </tr>\n    <tr>\n      <th>11922</th>\n      <td>frightening</td>\n      <td>0.095688</td>\n      <td>0.992</td>\n      <td>-0.102361</td>\n      <td>3</td>\n      <td>76</td>\n      <td>0.328442</td>\n    </tr>\n    <tr>\n      <th>16315</th>\n      <td>backing</td>\n      <td>0.045896</td>\n      <td>0.987</td>\n      <td>-0.038172</td>\n      <td>5</td>\n      <td>42</td>\n      <td>0.331575</td>\n    </tr>\n    <tr>\n      <th>20337</th>\n      <td>scarce</td>\n      <td>-0.044323</td>\n      <td>0.990</td>\n      <td>0.072006</td>\n      <td>329</td>\n      <td>61</td>\n      <td>0.339228</td>\n    </tr>\n    <tr>\n      <th>10305</th>\n      <td>hearing</td>\n      <td>0.064546</td>\n      <td>0.990</td>\n      <td>-0.008649</td>\n      <td>1</td>\n      <td>149</td>\n      <td>0.348632</td>\n    </tr>\n    <tr>\n      <th>7053</th>\n      <td>significantly</td>\n      <td>0.058707</td>\n      <td>0.998</td>\n      <td>-0.010645</td>\n      <td>31</td>\n      <td>123</td>\n      <td>0.348688</td>\n    </tr>\n    <tr>\n      <th>23333</th>\n      <td>hy</td>\n      <td>0.056843</td>\n      <td>0.992</td>\n      <td>0.008580</td>\n      <td>109</td>\n      <td>11</td>\n      <td>0.352474</td>\n    </tr>\n    <tr>\n      <th>18442</th>\n      <td>twain</td>\n      <td>0.055887</td>\n      <td>0.998</td>\n      <td>0.011805</td>\n      <td>29</td>\n      <td>31</td>\n      <td>0.355231</td>\n    </tr>\n    <tr>\n      <th>26316</th>\n      <td>juvenile</td>\n      <td>0.056204</td>\n      <td>0.974</td>\n      <td>0.037996</td>\n      <td>32</td>\n      <td>56</td>\n      <td>0.356067</td>\n    </tr>\n    <tr>\n      <th>10956</th>\n      <td>boring</td>\n      <td>0.089093</td>\n      <td>0.991</td>\n      <td>-0.009177</td>\n      <td>8</td>\n      <td>77</td>\n      <td>0.356972</td>\n    </tr>\n    <tr>\n      <th>5256</th>\n      <td>intriguing</td>\n      <td>0.198948</td>\n      <td>0.970</td>\n      <td>-0.095689</td>\n      <td>13</td>\n      <td>43</td>\n      <td>0.357753</td>\n    </tr>\n    <tr>\n      <th>15005</th>\n      <td>someone</td>\n      <td>0.045325</td>\n      <td>0.985</td>\n      <td>0.050252</td>\n      <td>3</td>\n      <td>1589</td>\n      <td>0.360192</td>\n    </tr>\n    <tr>\n      <th>1684</th>\n      <td>setting</td>\n      <td>0.200999</td>\n      <td>0.991</td>\n      <td>-0.109949</td>\n      <td>1</td>\n      <td>62</td>\n      <td>0.360683</td>\n    </tr>\n    <tr>\n      <th>7259</th>\n      <td>fer</td>\n      <td>0.259189</td>\n      <td>0.966</td>\n      <td>-0.132545</td>\n      <td>4</td>\n      <td>47</td>\n      <td>0.364215</td>\n    </tr>\n    <tr>\n      <th>16205</th>\n      <td>ski</td>\n      <td>0.287075</td>\n      <td>0.983</td>\n      <td>-0.166819</td>\n      <td>2</td>\n      <td>218</td>\n      <td>0.367752</td>\n    </tr>\n    <tr>\n      <th>5073</th>\n      <td>someday</td>\n      <td>0.002855</td>\n      <td>0.988</td>\n      <td>0.115850</td>\n      <td>1</td>\n      <td>125</td>\n      <td>0.368902</td>\n    </tr>\n    <tr>\n      <th>13092</th>\n      <td>futurity</td>\n      <td>0.177934</td>\n      <td>0.974</td>\n      <td>-0.035909</td>\n      <td>37</td>\n      <td>2</td>\n      <td>0.372008</td>\n    </tr>\n    <tr>\n      <th>20128</th>\n      <td>drastic</td>\n      <td>0.069603</td>\n      <td>0.993</td>\n      <td>0.058760</td>\n      <td>3</td>\n      <td>48</td>\n      <td>0.373788</td>\n    </tr>\n    <tr>\n      <th>20078</th>\n      <td>incline</td>\n      <td>-0.034136</td>\n      <td>0.988</td>\n      <td>0.176630</td>\n      <td>114</td>\n      <td>25</td>\n      <td>0.376831</td>\n    </tr>\n    <tr>\n      <th>20662</th>\n      <td>lineament</td>\n      <td>0.178748</td>\n      <td>0.979</td>\n      <td>-0.026509</td>\n      <td>51</td>\n      <td>1</td>\n      <td>0.377080</td>\n    </tr>\n    <tr>\n      <th>8384</th>\n      <td>lifelong</td>\n      <td>0.115619</td>\n      <td>0.995</td>\n      <td>0.023565</td>\n      <td>1</td>\n      <td>57</td>\n      <td>0.378061</td>\n    </tr>\n    <tr>\n      <th>16451</th>\n      <td>cheapness</td>\n      <td>0.079505</td>\n      <td>0.984</td>\n      <td>0.070801</td>\n      <td>17</td>\n      <td>4</td>\n      <td>0.378102</td>\n    </tr>\n    <tr>\n      <th>9831</th>\n      <td>habitually</td>\n      <td>0.109373</td>\n      <td>0.978</td>\n      <td>0.047016</td>\n      <td>63</td>\n      <td>9</td>\n      <td>0.378130</td>\n    </tr>\n    <tr>\n      <th>24592</th>\n      <td>vet</td>\n      <td>0.166211</td>\n      <td>0.980</td>\n      <td>-0.011214</td>\n      <td>3</td>\n      <td>43</td>\n      <td>0.378332</td>\n    </tr>\n    <tr>\n      <th>17630</th>\n      <td>aura</td>\n      <td>0.217044</td>\n      <td>0.998</td>\n      <td>-0.075655</td>\n      <td>9</td>\n      <td>52</td>\n      <td>0.379796</td>\n    </tr>\n    <tr>\n      <th>15164</th>\n      <td>hew</td>\n      <td>0.171765</td>\n      <td>0.993</td>\n      <td>-0.019913</td>\n      <td>30</td>\n      <td>18</td>\n      <td>0.381617</td>\n    </tr>\n    <tr>\n      <th>2558</th>\n      <td>overwhelmingly</td>\n      <td>0.146081</td>\n      <td>0.985</td>\n      <td>0.017748</td>\n      <td>1</td>\n      <td>33</td>\n      <td>0.382943</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "shared_vocabulary_df = shared_vocabulary_df.sort_values(by=[\"mean\"], ascending=True)\n",
    "shared_vocabulary_df.head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594667553763",
   "display_name": "Python 3.7.7 64-bit ('thesis': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}